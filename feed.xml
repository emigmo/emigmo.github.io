<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://emigmo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emigmo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-11T03:27:31+00:00</updated><id>https://emigmo.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://emigmo.github.io/blog/2025/2025-01-15-xiaohanding-paper-suggestion/" rel="alternate" type="text/html" title=""/><published>2025-11-11T03:27:31+00:00</published><updated>2025-11-11T03:27:31+00:00</updated><id>https://emigmo.github.io/blog/2025/2025-01-15-xiaohanding-paper-suggestion</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/2025-01-15-xiaohanding-paper-suggestion/"><![CDATA[<h1 id="丁霄汉学术论文写作指导从hard-to-follow到高可读性">丁霄汉：学术论文写作指导–从”Hard to Follow”到高可读性</h1> <p><em>作者：丁霄汉</em><br/> <em>时间：2025年1月15日</em><br/> <em>来源：学术写作经验分享</em></p> <hr/> <h2 id="前言">前言</h2> <p>几年间我主笔或大改了20篇左右的顶会论文,一开始是在清华的实验室里苦思冥想,被拒了好几次才有点长进,基本不会因为写作被秒拒了;后来实习和工作时,在帮实习生和学弟改文章的过程中也发现了一些常见问题。我想总结一些给顶会论文写作新手的建议。如果能帮助同学们少走弯路的话,我将不胜荣幸。</p> <p>这几条建议将分成上下两篇,每篇聚焦于如何避免收到一条致命的”恶评”,分别是:</p> <ul> <li><strong>(上)This paper is hard to follow</strong>(宏观层面,谋篇布局,优化论文结构,卖出核心贡献)</li> <li><strong>(下)The readability can be greatly improved</strong>(微观层面,精耕细作,少犯常见的小错误)</li> </ul> <hr/> <h2 id="目录">目录</h2> <h3 id="第一部分宏观布局避免hard-to-follow">第一部分：宏观布局，避免”hard to follow”</h3> <ul> <li><a href="#审稿人视角">审稿人视角</a></li> <li><a href="#例子1不要完整还原研究的心路历程">例子1：不要完整还原研究的心路历程</a></li> <li><a href="#例子2不要轻言based-on和ab">例子2：不要轻言”based on”和”A+B”</a></li> <li><a href="#总结和其他建议">总结和其他建议</a></li> </ul> <h3 id="第二部分精耕细作提高readability">第二部分：精耕细作，提高”readability”</h3> <ul> <li><a href="#逻辑强度">逻辑强度</a></li> <li><a href="#防御弹性">防御弹性</a></li> <li><a href="#迷惑时间">迷惑时间</a></li> <li><a href="#信息密度">信息密度</a></li> </ul> <hr/> <h2 id="前言-1">前言</h2> <p>几年间我主笔或大改了20篇左右的顶会论文，一开始是在清华的实验室里苦思冥想，被拒了好几次才有点长进，基本不会因为写作被秒拒了；后来实习和工作时，在帮实习生和学弟改文章的过程中也发现了一些常见问题。我想总结一些给顶会论文写作新手的建议。如果能帮助同学们少走弯路的话，我将不胜荣幸。</p> <p>这几条建议将分成上下两篇，每篇聚焦于如何避免收到一条致命的”恶评”，分别是：</p> <ul> <li><strong>（上）This paper is hard to follow</strong>（宏观层面，谋篇布局，优化论文结构，卖出核心贡献）</li> <li><strong>（下）The readability can be greatly improved</strong>（微观层面，精耕细作，少犯常见的小错误）</li> </ul> <hr/> <h2 id="第一部分宏观布局避免hard-to-follow-1">第一部分：宏观布局，避免”hard to follow”</h2> <p>本篇先从”hard to follow”开始。缺乏经验的同学收到这样的恶评外加一个strong reject之后可能会感到迷惑和委屈，所以本文首先从审稿人的角度来分析他们为什么会对你的文章作出这样的评价，然后通过两个例子给出相应的写作建议。</p> <h3 id="审稿人视角">审稿人视角</h3> <p>这个评价可能表示论文的宏观组织形式和内容分布与审稿人所期望的有较大的差距，所以他在耐心耗尽前没能搞懂你做了什么。对同一篇文章，审稿人想的跟作者想的到底差在哪呢？ 一位同学可能会对自己刚完成的论文很满意，因为：</p> <ul> <li>A. 在对一个重要问题的研究过程中，我遇到了几个复杂的子问题，我一环扣一环地解决了这几个问题，最终得到了一个巧妙的解决方案。我记录了这个精彩的过程，审稿人应当为这一方案的系统性和自洽性折服。</li> <li>B. 我讨论的问题有点难以理解，但只要脑子里转过那个弯儿来，经过一系列挑战性十足的思想游戏，就可以把这个问题转化为另一个简单的问题，那么解决方案就自然呼之欲出了。我用两页纸的篇幅进行了这一思辨，逐步推出了最终的结论，审稿人肯定会受到灵魂的启迪。</li> <li>C. 我在别人的工作上加入了本质的创新，变成了全新的东西，最终的效果很好，如假包换的SOTA。审稿人只要能看懂实验结果，就应该给我accept。 那审稿人是怎么看的呢？ 审稿人根本就不在乎。 审稿人点开了你的文章，边看边想：</li> <li>A. 他到底要解决什么问题？怎么一个问题又一个问题？到底哪个是他要解决的？哪个方法是common practice，哪个是他提出来的？</li> <li>B. 这两页纸在写什么东西？前面没总起，后面没总结，中间又是假设又是推论的在干什么？这个结论是怎么回事，这不是很显然的吗？磨叽了两页纸，就这？</li> <li>C. 哦，这个懂了，yet another AAA + BBB，我跳过中间四页直接看看效果吧。这张表里面的这个指标是啥意思，就比别人高了0.2，这个差别很大吗？ 审稿人期望的是什么呢？</li> <li>审稿人想要的是迅速搞明白你提出了什么新东西，而不是听你娓娓道来、抽丝剥茧，跟着你一起由浅入深地感受那四方求索、最终豁然开朗的成就感。写论文的目的在于向读者介绍工作的最终形态，而不是记录研究过程。有的同学会在文章里描述自己从最初的idea艰难跋涉直到最终结果的心路历程，这多半是没用的。</li> <li>审稿人不会逐字逐句地看论文，他希望他直接跳到任何一页上之后都能在所有人都习惯的地方迅速找到他想要看到的内容。所以，如果你的论文组织形式不太常规，审稿人可能根本就懒得看。</li> <li>很多人是为了学到新东西才来审稿的。如果你把一些反直觉、有趣或很实用的结论摆在显眼的地方给他看，他可能会感到有收获。但不要期望每个审稿人都愿意从你的大段文字和意义不明的图表中自己经过思考得到这些结论。</li> </ul> <h3 id="例子1不要完整还原研究的心路历程">例子1：不要完整还原研究的心路历程</h3> <p>我为了解决一个重要的已知问题（问题甲）提出了方案A（跟其他人的方法相比变化很大），发现方案A不够好，研究一下为什么方案A不够好呢，发现这是因为方案A引入了另一个问题（问题乙）；我研究了一下如何解决问题乙，最终提出了改进版方案B（在A的基础上改动很小，加一个trick），所有问题都解决了。而且这个trick用在其他方法中也有提升。我提出了两个方案，还发现了一个别人没发现的问题，我觉得这篇文章很稳。 但审稿人是以不同的视角来看这篇文章的，他并没有亲身经历我的研究过程，不可能像我一样清楚重点在哪里。他可能会问：</p> <ul> <li>如何证明方案A不好使的本质原因是问题乙？</li> <li>方案A也有一定效果，如何证明这是因为解决了问题甲？</li> <li>方案B相对于方案A只有很小的差别，怎么就这么点novelty？</li> <li>你说B的改进用在其他方法上也有提升，那我为什么还需要A？A不也没比其他方法好到哪去吗？ 我觉得很冤：这篇文章的最终结果明明是方案B，B是A的巧妙升级版，差别当然很小了，但是A也是我提出的呀！还有，为什么要纠结A和问题甲的关系，这很重要吗？ 如果审稿人能听到我的抱怨，他会说：</li> <li>你全篇都在鼓吹方案B，你没有强调A，我怎么知道它也是这篇文章主要内容的一部分？</li> <li>就算A是你提出的，它也没有解决你强调的问题乙啊，难道它也算主要贡献？</li> <li>既然A不算主要贡献，那你所强调的B可不就是只有很小的改动吗？ 辩经是没有意义的，我们只能在写作时注意避免引发这种无意义的争论。如果我们稍微改变一下写作的方式，审稿人就更容易看懂了：</li> <li>我提出了一个全新的方法来解决一个重要问题。</li> <li>这个全新的方法包括一个全新的框架和一个通用的trick。</li> <li>分别证明二者的有效性：用这一框架而不用这个trick会导致效果相对变差；这是因为另一个有趣的问题（做一些分析）；这个trick用在其他方法里，也可以有一定的提升；二者结合，效果才是最好的。 为什么审稿人喜欢这样的组织形式呢？因为他一眼就看出你提出了什么东西，以及它们各自有什么作用。他只需要判断你提出的东西技术上是否正确和效果是否显著就可以了。</li> </ul> <h3 id="例子2不要轻言based-on和ab">例子2：不要轻言”based on”和”A+B”</h3> <p>“提出了基于AAA和BBB的XXX”是本科毕业论文里常用的说法，适用于需要稳妥安全而不怎么需要创新性的场合。在顶会论文里这么写，可能会把本来还不错的创新性给写没了。在大多数情况下，不要轻言自己的工作“based on”，你完全可以说自己注意到了某个重要问题，琢磨明白了背后的原理，想到了一个解决方案，提出的是一种新的东西，这种东西自然地用到了AAA和BBB，而不是简单地改了改人家的成果。 设想一下，把ResNet的写作改成“based on”和“A+B”的形式，是不是就把创新性写没了？ 《ResNet: Yet Another Simplified GoogLeNet》 ：我们基于GoogLeNet和VGGNet设计了一种大量用3x3卷积（借鉴VGGNet）和并行shortcut（简化自GoogleNet）的模型，超越了GoogLeNet和VGGNet。为什么效果这么好呢？因为VGGNet和GoogLeNet珠玉在前，Batch Norm也很好用。反正这个工作就是这么简单，我们没有想到理论上有什么创新性，但我们给后续的工作搭建了一个好的baseline。 这样的文章能让读者学到多少东西呢？ ResNet实际上是怎么写的呢？</p> <ul> <li>各位想必都知道当今有个重要的问题：模型越深，居然会越掉点。</li> <li>我们提出的解决方法惊人地简单：将模型中的映射形式从y=f(x)改成y=f(x)+x，也就是所提出的“残差学习”，就能解决这个问题。这是因为模型容易拟合f(x)=0，不容易学出f(x)=x。</li> <li>残差学习非常容易实现，效果非常好。 提出问题——抽象出背后的原理——提出自己的解决方案和具体的实现——实验验证，这才是更符合人类认知规律的论文写法。 有的同学觉得，一篇贡献没那么大的论文硬要写成这样是在”讲故事”，但只要实事求是，只要实验证据能够支撑你的故事，这样的故事既有利于审稿人给你打出高分，也有利于读者学到新知识，更有利于你的观点的传播，属于是赢麻了。</li> </ul> <h3 id="总结和其他建议">总结和其他建议</h3> <ul> <li>上面说的第一件事其实也跟ResNet有关：真实历史上的ResNet来自于对GoogLeNet的拆解研究，并不是突然发现了“残差学习”的原理才有了ResNet，而是孙剑老师带领的团队先通过拆解GoogLeNet发现shortcut结构好用后思考出来的解释。我们上面设想的“丐版”写法虽然是反映了真实的研究过程的，却并不利于背后原理的深挖和核心思想的传播。这个实例正好能够支持本文的观点：研究怎么做和论文怎么写，是两码事。</li> <li>论文是传播知识的工具，是方便别人省时省力地学到新东西的标准化交流方式，不是个人展示个性的舞台。审稿人（读者）习惯于看什么样的论文，我们就写什么样的论文。这本质上跟流行歌曲和通俗小说是一样的，费尽心机来迎合受众而已，不寒碜。</li> <li>没必要在文章结构上寻求创新。大多数文章都可以写成Introduction + Related Work + Method + Experiment + Conclusion的形式。如果你提出的不是单一创新点而是一揽子小改动的组合，可以学习ShuffleNet v2是怎样写的。</li> <li>Be explicit. 你不强调的东西，不要指望别人自己悟出来。</li> <li>要在一个地方集中地完整描述你提出的东西，不要散落得到处都是。不要假设一个读者会有耐心看完50%的篇幅才理解你做了什么。</li> </ul> <hr/> <h2 id="第二部分精耕细作提高readability-1">第二部分：精耕细作，提高”readability”</h2> <p>在《顶会论文写作建议（上）：宏观布局，避免“hard to follow”》中，我们讨论了如何在宏观层面设计论文的结构，以大多数人乐于接受的方式卖出自己的核心贡献。 在本篇中，我们将会聚焦于语言组织的层面，通过十几个我近几年从清华的学弟或实习生同学的论文中收集到的例子，讨论如何提高文章可读性，让读者能心平气和、行云流水地读完，让审稿人能更客观且无痛地评判文章的价值。 需要注意的是，我们讨论的前提是假设论文至少在语法层面是正确的，不再讨论那些可以用Grammarly或ChatGPT自动解决的基础问题。 正如上篇所说，大部分论文都可以写成提出问题——抽象出背后的原理——解决问题的格式（俗称讲故事）。本篇的宏观结构也将遵循这一套路，从而作为对上篇所述宏观写作原则的一次具体实践。 假设本文真的是一篇论文，那么在省略了一大堆关于写作如何重要、写作如何成为了一个重要的研究领域、引用了一大堆关于写作的重要prior work（作者自己也不一定看过，但是看到别人都在引所以就引了）的铺垫语句之后（这些气氛句子本来也没人会看，所以我就不写了），我们的故事现在开始。 本文提出用以下概念来度量文章的可读性：逻辑强度、防御弹性、迷惑时间、信息密度。在此之上，本文提出一些实用的建议和技巧来提高文章的可读性。</p> <h3 id="逻辑强度">逻辑强度</h3> <p>在任何语言的学术写作中，逻辑的连贯都远比用词的华丽更重要。上篇已经介绍了一些宏观逻辑设计上的技巧。在微观层面需要注意的是，逻辑的连贯在于逻辑本身，而不在于衔接词（to this end, in contrast, specifically等等）。 换句话说，我们应该把衔接词当成使语言更加流畅的点缀，而不是通过衔接词来为本没有逻辑的句子强行构造逻辑。例如从总括到具体描述时，用“Specifically, …”；前后两句存在对比关系时，用“In contrast, …”。 而不是反过来！并不是我们写下“In contrast”，前后两句就真的因此而有了对比关系。衔接词与真实逻辑的不匹配会让人疑惑，显著降低文章的可读性。 我们的高中英语写作将衔接词的存在视作逻辑本身，甚至当成作文中的加分项。实际上，用一大堆衔接词不一定能提高文章的流畅程度，反而可能有负面作用。 我们写下每一个衔接词之后都要三省吾身：它前后两句的逻辑关系真实存在吗？它自身放在这个语境下正确吗？不用它行不行？ 例1：衔接词必须自身正确，经得起推敲。 We argue that problem A is critical. To this end, we propose method B. 作者写下这句的时候想的可能只是挑个词来通过problem A引出method B，逻辑是显然的，但写出来的句子却是经不起推敲的：”this end”中的this指的是哪个end？上文说了一个“end“（要做什么/要实现什么目的）吗？上文实际上只是提出了一个观点，并没有说要做什么，所以这个衔接词自身的存在就是错误的。 例2：衔接词不能强加逻辑关系。 The system comprises three modules. First of all, Module A is …. Second, Module B is …. Last but not least, Module C is …. 作者的本意是描述三个并列的事物，这几个衔接词却给这三个本来没有次序关系的东西强加了一定的次序。这时候就不如去掉这些词，分别介绍三个事物即可，根本不用加任何衔接，语言依然流畅：The system comprises three modules. Module A… Module B… Module C.. 例3：衔接词要准确反应真实逻辑关系。 The baseline model is … On top of that, model A employs an extra attention module. In contrast, we propose a novel objective function. 这句话让人迷惑：我们的模型除了这个novel objective function以外是否也用了这个extra attention module呢？读者在寻找“In contrast”对比的对象的时候可以有两种理解。 读者可以理解成model A和我们的模型是用两种方式来解决同一问题，model A用了这一结构而没用这一目标函数，我们是用了这一目标函数而没用这一结构。读者也可以理解成我们试图拿model A和我们自己的方法对比以凸显这个目标函数的效果，所以我们的模型等于model A + 这个新提出的目标函数。 如何修改呢？如果一定要对比的话（比如你的那位不怎么懂但是confidence分数拉满的审稿人一定要让你对比），应该在结构和loss两方面分别讨论从而形成对比，消除歧义，顺便突出我们的优势： From a structural perspective, model A introduces an extra attention module while we use the same model structure as the baseline. In terms of the objective function, method A adopts the vanilla XXX loss, which suffers from …, while we … 这也就是我们所说的，先有逻辑（”我们要从两个方面进行对比！”）后有衔接词（当然不用也行），而不是用衔接词构造逻辑（简单塞进去一个In contrast并期望对比关系就因此而产生）。</p> <h3 id="防御弹性">防御弹性</h3> <p>在写作的时候，我们每写完一句话都要考虑审稿人可能从哪个角度攻击这句话。“防御弹性”指的是我们的语言引起审稿人的质疑的频率和面对审稿人的挑剔时的抵抗能力。 正如写代码时要有“防御式编程”的概念，写作时也要有“防御式写作”的意识，随时考虑笔下的语句是不是无懈可击的。 例4：言出有据。 当我们说“Problem A是本领域的关键问题且尚未得到解决”，这时就要考虑到审稿人可能会问：“为什么说这是关键问题？它造成的后果有多严重？这种后果对最终性能影响大吗？”这就需要我们完善引用： It is reported that problem A results in … [1,2,3] and … [4,5], which are critical to … because … [6, 7, 8]. 丰富且适当的引用会让人觉得作者学识渊博，对本领域理解深刻（哪怕你只看过那些文章的摘要，只是在制造这样的幻象也行）。 例5：轻重适当。 在展示了实验结果之后，我们往往需要解释为什么我们的方法会work。不解释一般是不行的，解释的太绝对了又可能被审稿人challenge（“怎么证明？有什么理论？”），这时就要把握轻重。</p> <ul> <li>我们有直接的证据时：The performance improves, which is attributed to that XXX can … （后面要高调地把证据展示出来）</li> <li>没有直接的证据，但有一些可视化等间接证据：The performance improves, which may be explained that XXX can …</li> <li>几乎没有证据，只是感觉应该是这样的，反正跟我们的motivation是相符的：The performance improves, suggesting that XXX can … 例6：不要用自讨苦吃的主观词汇。 In Table 1, it is obviously exhibited that … 在学术写作这种场合使用诸如obviously之类的带有强烈主观色彩的词汇没有任何好处。如果审稿人能看出你的结果好，不需要你自己强调这个obviously；如果审稿人看不懂，你越强调他就会越迷惑：到底好在哪了，跟我仔细唠唠，我先判你一个overclaim，阁下又该如何应对呢？</li> </ul> <h3 id="迷惑时间">迷惑时间</h3> <p>“迷惑时间”是读者在阅读过程中每一次“咦，这是啥”到“哦，原来是这样”之间的时间的总和。当代人类的耐心是有限的，一篇文章的总的迷惑时间越短，可读性就越高，审稿人就会越心平气和。 例7：提出一个概念后应就近解释。 在深度学习领域中，一个复杂的模型中的一个简单结构根据其功能（和讲故事的需要）可能被命名为XXX Perceiver, XXX Perceptron, XXX Recognizer, XXX Gating Function等等。 建议在给出其名字以后直接解释其实质（we propose XXX, which is implemented with a two-layer MLP）。不然如果在Introduction里只给出了它的名字而在Method部分再给出其实现的话，读者会在很长的一段时间里带着“这么玄乎的东西到底是个啥”的疑惑，最后发现“原来就这啊”，阅读体验很不好。 例8：指代对象应显然且毫无歧义。 We update structure A to solve the problem caused by model B, which is widely used in the literature. 这里“widely used”到底是A还是B？如果我们的写作水平有限，无法让长句完全不带歧义的话，就应该将其拆为短句。没有人会因为你缺少展示高超英语水平的长难句而拒你的文章！ 例9：不要在需要读者集中注意力时多次引用数页后的内容。 我们在introduction里可以提几句我们的结果有多么多么好，带上几个对后面的图表的reference，但不要频繁这么干，尤其是不要在需要读者集中注意力（比如你正在激情四射地展开自己的核心故事，需要读者来判断这个故事是否合理的时候），不然读者翻过去再翻回来的这段时间里思维就断了。读者思维的连续性也是可读性的一部分，毕竟论文是线性叙事。这也是孙剑老师教我的最后一点。</p> <h3 id="信息密度">信息密度</h3> <p>信息密度就不是什么新的概念了，指的是读者能从单位长度的文字中提取到的有效信息量。信息密度过低的文章会让人走神并怀疑作者的专业性。 例10：气氛组语句不应过于冗长。 我们都知道每篇论文中总有一些不会有人认真看的句子，这些句子一般位于各章开头，且其中“recent years”含量极高。这些句子一般还是要写几句的，但建议注意以下几点。</p> <ul> <li>不要太长，写长了没人会看的。我曾经审过一篇做模型压缩的文章，开头从LeNet唠到ViT，直到第二页右半栏才提出“model compression”的定义。</li> <li>不相关的不要写。如果你这篇文章是做ViT的且不涉及底层算子，就不要提卷积网络是如何滑动窗口的了。</li> <li>不要写成历史书。审了这么多稿，我现在对从人类发现动物视皮质原理到LeNet再到AlexNet再到ResNet这段历史已经完全PTSD了。</li> <li>写都写了，尽量跟主旨有点关系，尽快引出正题。例如，如果你提出的是一种新的hierarchical ViT，那么就可以在正文第一句里就把ViT按是否hierarchical分成两类（最近ViT们繁荣发展，包括两类……），第二句就进入主题。 例11：精炼语言。 非英语母语者写出来的英语一般不如母语者精炼，初学者更是经常因为害怕语言不够准确而越写越长。例如： The image classification performance on ImageNet is 79.99% 改为 The ImageNet top-1 accuracy is 79.99%.</li> </ul> <p>As can be observed in Table 1, A outperforms B. 改为 Table 1 shows that A outperforms B.</p> <p>Table 1 shows that the accuracy of model A is 81.0% and the accuracy of model B is 80.0%, so we conclude that model A outperforms model B. 改为 Table 1 shows that model A outperforms model B by 1.0% in the accuracy (81.0% v.s. 80.0%). 用简练的语言准确表达意思是一门技术活，具有本质的困难性。建议多看英语母语者写的论文和其他文字内容，特别是那些有丰富教学经验的年轻选手，因为写教案是最锻炼语言组织能力的（对的，我说的就是当年斯坦福CS231N的主讲Andrej Karpathy）。 例12：图表附近应是全文中信息密度最高的地方，重要的解释和阐述距离图表越近越好。</p> <ul> <li>如果图表中有缩写，那么标题里最好就有解释。</li> <li>如果希望强调Table 5中的某个结果，那么分析这一结果的语句最好跟Table 5在同一页上，而且那句话前后最好就有“Table 5”这几个字。这是因为读者可能根本不会仔细看你写的文字，而是先看图表再去找跟图表中的内容关联的文字。一眼看到Table 5中某个亮眼的结果并感到好奇时，他很可能用pdf阅读器的搜索功能来搜“Table 5”。</li> <li>不要指望读者自己从复杂的表格中自己想明白应该拿谁去跟谁对比以得出结论，我们应该把希望形成对比的内容放在一起。如果这样的表格很难设计的话，哪怕为此必须得把某个结果（一般是需要跟若干组结果全对比一遍的baseline）重复几行也在所不惜。没有人会因为表格设计不够优雅而拒你的论文，但看不明白表格进而血压升高的审稿人是真的会。</li> </ul>]]></content><author><name></name></author></entry><entry><title type="html">Anthropic 研究员详解：构建高效 Claude 智能体的完整方法论</title><link href="https://emigmo.github.io/blog/2025/anthropic-claude-agent-methodology/" rel="alternate" type="text/html" title="Anthropic 研究员详解：构建高效 Claude 智能体的完整方法论"/><published>2025-11-06T00:00:00+00:00</published><updated>2025-11-06T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/anthropic-claude-agent-methodology</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/anthropic-claude-agent-methodology/"><![CDATA[<h1 id="anthropic-研究员详解构建高效-claude-智能体的完整方法论">Anthropic 研究员详解：构建高效 Claude 智能体的完整方法论</h1> <p><em>访谈对象：Alex Albert (Claude 关系负责人)、Erik Schluntz (多智能体研究员)</em><br/> <em>时间：2025年11月</em><br/> <em>来源：Anthropic 官方播客</em></p> <hr/> <h2 id="前言">前言</h2> <blockquote> <p><strong>“编码是智能体最基本、最核心的技能。一旦拥有了一个出色的编码智能体，这个智能体几乎可以完成任何其他类型的工作。”</strong></p> <p><strong>“工具应该映射 UI，而非 API——这是构建智能体工具最常见也最严重的错误观念。”</strong></p> </blockquote> <p>最近，来自 Anthropic 的两位核心成员——Claude 关系负责人 Alex Albert 与多智能体研究员 Erik Schluntz，深入探讨了 AI 智能体在过去数月中的快速演进。他们分享了从简单的”工作流”过渡到复杂的”多智能体系统”的实践经验，并详细阐述了如何通过代码、Claude Skills、MCP 和工具的最佳实践来构建更高效、更自主的 Claude 智能体。</p> <p><strong>三个核心洞察：</strong></p> <ol> <li><strong>编码能力是一切的基础</strong>：强大的编码智能体可以泛化到任何领域，这种”溢出效应”是 Claude 在所有任务上表现出色的关键</li> <li><strong>架构演进路径清晰</strong>：从静态工作流 → 单一智能体循环 → 智能体工作流 → 多智能体系统，复杂度逐级递增</li> <li><strong>UI映射原则至关重要</strong>：工具设计应模拟用户界面而非后端API，这能显著提升智能体效率</li> </ol> <hr/> <h2 id="目录">目录</h2> <ul> <li><a href="#一claude-作为智能体的基础编码能力的溢出效应">一、Claude 作为智能体的基础：编码能力的”溢出效应”</a></li> <li><a href="#二开发者工具的演进从-sdk-到-skills">二、开发者工具的演进：从 SDK 到 Skills</a></li> <li><a href="#三智能体系统的架构演进">三、智能体系统的架构演进</a></li> <li><a href="#四智能体开发者的核心最佳实践">四、智能体开发者的核心最佳实践</a></li> <li><a href="#五未来展望长程任务的自动交付">五、未来展望：长程任务的自动交付</a></li> <li><a href="#结语从工具到伙伴的跃迁">结语：从工具到伙伴的跃迁</a></li> </ul> <hr/> <h2 id="一claude-作为智能体的基础编码能力的溢出效应">一、Claude 作为智能体的基础：编码能力的”溢出效应”</h2> <h3 id="11-智能体能力的训练根源">1.1 智能体能力的训练根源</h3> <p>要理解如何构建高效的智能体，首先要明白 Claude 为何擅长执行智能体任务。Erik 指出，核心在于<strong>大量的刻意练习</strong>。</p> <p><strong>训练策略：</strong></p> <ul> <li>在训练过程中，Claude 被要求处理许多开放式问题</li> <li>这些任务要求采取多个步骤、使用工具、探索环境</li> <li>通过强化学习对编码、搜索等不同场景进行大量练习</li> </ul> <p>这种训练方式让 Claude 积累了丰富的”作为智能体”的经验，因此在智能体任务上表现出色。</p> <h3 id="12-为什么编码是最重要的技能">1.2 为什么编码是最重要的技能</h3> <p>外界普遍认为 Claude 在编码方面异常强大，但常误以为这种能力仅限于技术领域。Erik 提出了不同的看法：</p> <blockquote> <p><strong>编码是智能体最基本、最核心的技能。</strong></p> </blockquote> <p>Anthropic 的理念是<strong>“先训练最难的东西”</strong>——即编码，那么其他一切都会变得更容易。</p> <p><strong>编码能力的泛化场景：</strong></p> <ul> <li><strong>搜索任务</strong>：编写代码调用 Web 搜索 API</li> <li><strong>行程规划</strong>：编写代码创建日程表或数据结构</li> <li><strong>数据分析</strong>：编写脚本处理和可视化数据</li> </ul> <p>编码能力的”溢出效应”极其显著，它是使 Claude 在所有领域都表现出色的基石。</p> <h3 id="13-从直接生成到代码生成的效率革命">1.3 从直接生成到代码生成的效率革命</h3> <p>这种以编码为核心的理念，已经体现在 Claude.ai 网页版的功能中：Claude 能通过编写代码来创建实际的文件。</p> <p><strong>典型案例：</strong></p> <p>Erik 分享了一个亲身经历。他让 Claude 帮他为演示文稿制作图表：</p> <ol> <li><strong>简单图表</strong>：Claude 直接编写 SVG 代码生成</li> <li><strong>复杂图表</strong>：当需要大量重复性细节时，Claude 改变策略——编写一段脚本来生成 SVG 文件</li> </ol> <blockquote> <p><strong>效率对比：脚本运行速度远远快于 Claude 逐字生成图像文件的速度。</strong></p> </blockquote> <p><strong>核心原则：</strong> 对于许多复杂或重复性的任务，让智能体编写代码来生产某个”人工产物”，比让它直接创建这个产物要高效得多。</p> <hr/> <h2 id="二开发者工具的演进从-sdk-到-skills">二、开发者工具的演进：从 SDK 到 Skills</h2> <h3 id="21-claude-code-sdk通用智能体框架">2.1 Claude Code SDK：通用智能体框架</h3> <p>当开发者真正开始构建自己的智能体时，<strong>Claude Code SDK</strong> 正变得越来越受欢迎。</p> <p><strong>SDK 的核心价值：</strong></p> <ul> <li>解决了”重复造轮子”问题</li> <li>内置了所有基础工作：循环、工具构建、工具执行、文件系统交互、MCP 处理</li> <li>虽然名字里有”Code”，但本质上是一个<strong>通用智能体框架</strong></li> </ul> <p><strong>使用建议：</strong></p> <p>Erik 强烈建议开发者将这个 SDK 作为智能体循环的核心。这样开发者可以把时间花在真正有价值的地方：</p> <ul> <li>通过 MCP 添加独特的工具</li> <li>定制业务逻辑</li> <li>实现特定功能</li> </ul> <p><strong>高度可定制性：</strong></p> <p>开发者可以移除编码相关部分，然后填入自己需要的任何提示或工具。Erik 甚至用它规划过约会——通过集成网络搜索工具，这个”编码 SDK”帮他搜索了地区活动和餐馆，推荐了长木花园和附近的中餐馆。</p> <h3 id="22-claude-skills从指令到资源的跃迁">2.2 Claude Skills：从指令到资源的跃迁</h3> <p><strong>技能的起源：Claude.md 文件</strong></p> <p>开发者可以在项目根目录放置 <code class="language-plaintext highlighter-rouge">Claude.md</code> 文件，向 Claude 提供项目背景信息：</p> <ul> <li>编程风格偏好</li> <li>项目目录结构</li> <li>技术栈说明</li> </ul> <p><strong>Skills 的革命性扩展：</strong></p> <p>Skills 不再局限于提供纯文本”指令”，而是允许开发者为 Claude 提供<strong>任何类型的文件作为”资源”</strong>。</p> <p><strong>资源类型示例：</strong></p> <ol> <li><strong>模板文件</strong>：公司官方 PowerPoint 模板</li> <li><strong>辅助脚本</strong>：Claude 可调用的现成代码</li> <li><strong>资产文件</strong>：图像、Logo、高管照片等</li> </ol> <blockquote> <p><strong>从”给指令”到”给资源”</strong>——这标志着智能体工具的重大转变。</p> </blockquote> <p><strong>“黑客帝国”比喻：</strong></p> <p>Alex 用《黑客帝国》中 Neo 学习功夫的场景作比喻：当”功夫”程序被注入大脑后，他瞬间掌握了技能。给 Claude 一项”技能”的感觉非常相似——比如给它一个”如何创建电子表格”的技能包，Claude 就像变成了专业的”银行家”，能够构建复杂的财务模型。</p> <hr/> <h2 id="三智能体系统的架构演进">三、智能体系统的架构演进</h2> <h3 id="31-从工作流到智能体循环">3.1 从工作流到智能体循环</h3> <p>几个月前，智能体领域正处于过渡期：从”工作流”向”单一智能体系统”转变。</p> <p><strong>工作流 vs 智能体循环：</strong></p> <table> <thead> <tr> <th>类型</th> <th>特点</th> <th>适用场景</th> </tr> </thead> <tbody> <tr> <td>工作流</td> <td>链接的提示词序列，每步单次执行</td> <td>需要极低延迟的简单任务</td> </tr> <tr> <td>智能体循环</td> <td>模型在循环中运行，可反馈和纠正</td> <td>追求绝对质量的复杂任务</td> </tr> </tbody> </table> <p><strong>为什么智能体循环胜出：</strong></p> <p>Claude 在”响应反馈”和”纠正自身工作”方面的能力已经非常出色，因此智能体循环在追求质量的任务上表现远超工作流。</p> <h3 id="32-智能体工作流串行的智能体链">3.2 智能体工作流：串行的智能体链</h3> <p>Erik 观察到的最新趋势：<strong>“智能体工作流”(Workflows of Agents)</strong>。</p> <p><strong>案例对比：数据查询与图表绘制</strong></p> <p><strong>旧的工作流（Workflow）：</strong></p> <ol> <li>步骤一（单次尝试）：Claude 编写 SQL 命令加载数据</li> <li>步骤二（单次尝试）：基于数据绘制图表</li> <li><strong>失败点</strong>：如果步骤一的 SQL 失败，步骤二对此一无所知，基于错误数据继续执行</li> </ol> <p><strong>新的智能体工作流（Workflow of Agents）：</strong></p> <ol> <li>步骤一（完整智能体）： <ul> <li>尝试编写 SQL 查询</li> <li>运行并查看输出</li> <li>如果失败，迭代重试直到获得正确数据</li> </ul> </li> <li>步骤二：当且仅当步骤一确认成功后，才移交给下一个智能体</li> </ol> <blockquote> <p><strong>从”链接提示”演进到”链接智能体”</strong>——这是智能体架构的重要里程碑。</p> </blockquote> <h3 id="33-可观测性挑战与简单性原则">3.3 可观测性挑战与简单性原则</h3> <p><strong>复杂性带来的难题：</strong></p> <p>随着系统变得越来越复杂，<strong>可观测性</strong>会变得”非常困难”。</p> <p><strong>Erik 的核心建议：</strong></p> <blockquote> <p><strong>永远从最简单的方法开始，只有在绝对必要时才增加复杂性。</strong></p> </blockquote> <p><strong>推荐的渐进路径：</strong></p> <ol> <li><strong>第一步</strong>：尝试单次调用能否解决问题</li> <li><strong>第二步</strong>：使用 Claude Code SDK 等简单智能体循环</li> <li><strong>第三步</strong>：只有在简单方法无法满足需求时，才构建复杂的多层系统</li> </ol> <p>每增加一层复杂性，系统的可观测性就会变得更难。</p> <h3 id="34-多智能体系统并行的协作架构">3.4 多智能体系统：并行的协作架构</h3> <p><strong>智能体工作流 vs 多智能体：</strong></p> <table> <thead> <tr> <th>类型</th> <th>执行方式</th> <th>特点</th> </tr> </thead> <tbody> <tr> <td>智能体工作流</td> <td>串行（Sequential）</td> <td>一个智能体完成后传递给下一个</td> </tr> <tr> <td>多智能体</td> <td>并行（Parallel）</td> <td>多个智能体同时工作</td> </tr> </tbody> </table> <p><strong>多智能体的典型应用场景：</strong></p> <p><strong>场景一：并行委托</strong></p> <p>一个”父智能体”将任务委托给多个”子智能体”并行工作。</p> <p><strong>示例：Anthropic 的深度研究搜索产品</strong></p> <ul> <li>主”协调器”智能体决定创建几个子智能体</li> <li>子智能体同时执行大量搜索任务</li> <li>用户更快获得最终答案</li> </ul> <p><strong>场景二：上下文保护</strong></p> <p>主智能体将繁重的子任务外包给子智能体。</p> <p><strong>示例：代码库搜索</strong></p> <ul> <li>任务可能需要消耗数万个 token（在庞大代码库中查找）</li> <li>最终答案却很简短（文件名和行号）</li> <li>子智能体在自己的上下文中处理，只返回简短答案</li> <li>保护主智能体的上下文窗口</li> </ul> <h3 id="35-claude-学习成为管理者">3.5 Claude 学习成为”管理者”</h3> <p><strong>实现机制：</strong></p> <p>多智能体通过<strong>工具调用框架</strong>实现。对主智能体来说，子智能体就像一个可调用的”工具”。</p> <p><strong>Claude 的”管理挑战”：</strong></p> <p>Erik 目前的研究重点之一，是训练 Claude 成为更好的”管理者”。</p> <blockquote> <p><strong>Claude 会犯和人类”新手管理者”一样的错误。</strong></p> </blockquote> <p><strong>常见错误：</strong></p> <ul> <li>向子智能体提供不完整或含糊的指令</li> <li>错误地期望子智能体拥有和它一样的上下文背景</li> </ul> <p><strong>改进方向：</strong></p> <p>通过训练，Claude 开始：</p> <ul> <li>变得更啰嗦、更详细</li> <li>有意识地向子智能体提供任务的”整体背景”</li> <li>学习如何成为更称职的管理者</li> </ul> <hr/> <h2 id="四智能体开发者的核心最佳实践">四、智能体开发者的核心最佳实践</h2> <h3 id="41-保持简单按需增加复杂性">4.1 保持简单，按需增加复杂性</h3> <p><strong>首要原则：</strong></p> <blockquote> <p><strong>Start simple and make sure you only add complexity as you need.</strong></p> </blockquote> <p><strong>推荐路径：</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>单次调用 → 简单智能体循环（如 SDK） → 复杂多层系统
</code></pre></div></div> <p>智能体系统的可观测性非常困难，复杂架构会加剧这一难题。</p> <h3 id="42-采用智能体的视角换位思考">4.2 采用智能体的视角（换位思考）</h3> <p><strong>核心思维：</strong></p> <blockquote> <p><strong>从智能体的角度去思考，设身处地站在 Claude 的立场上。</strong></p> </blockquote> <p><strong>最有效的实践方法：</strong></p> <ol> <li>阅读智能体看到的原始日志</li> <li>查看它在工具调用中实际看到的信息</li> <li>问自己：”如果我是智能体，只看到这些信息，我真的有足够信息解决这个问题吗？”</li> </ol> <p><strong>关键认知：</strong></p> <p>开发者容易忘记——我们（人类）能看到一切，而模型”只看得到我们展示给它的东西”。</p> <h3 id="43-工具应映射-ui而非-api最重要">4.3 工具应映射 UI，而非 API（最重要）</h3> <p>这是 Erik 强调的<strong>最常见且最严重的错误观念</strong>。</p> <p><strong>错误观念 vs 正确心智：</strong></p> <table> <thead> <tr> <th>错误观念</th> <th>正确心智</th> </tr> </thead> <tbody> <tr> <td>工具应与后端 API 一一对应</td> <td>工具应与用户界面（UI）一一对应</td> </tr> </tbody> </table> <p><strong>核心原因：</strong></p> <p>模型（Claude）是工具的”用户”，它不像”传统程序”那样工作。</p> <p><strong>经典案例：Slack 对话理解</strong></p> <p><strong>错误方式：API 映射</strong></p> <p>后端有三个独立端点：</p> <ol> <li><code class="language-plaintext highlighter-rouge">load_slack_conversation()</code>：返回 user ID 和 channel ID</li> <li><code class="language-plaintext highlighter-rouge">turn_user_id_into_username()</code>：ID 转用户名</li> <li><code class="language-plaintext highlighter-rouge">turn_channel_id_into_channel_name()</code>：ID 转频道名</li> </ol> <p>如果提供这三个独立工具，智能体必须连续进行三次工具调用才能理解任何事情。<strong>极其低效。</strong></p> <p><strong>正确方式：UI 映射</strong></p> <p>反问：人类用户如何看待 Slack？</p> <p>我们看到的是”所有内容都已完美渲染好”的界面，不需要”点击用户 ID 来看他的名字”。</p> <p><strong>解决方案：</strong></p> <ul> <li>创建一个工具，一次性呈现所有信息</li> <li>需要尽可能少的交互</li> <li>在后台自己完成那三次 API 调用</li> <li>返回已经”渲染”好的、包含用户名和频道名的完整对话文本</li> </ul> <p><strong>核心思想：</strong></p> <blockquote> <p><strong>不要让智能体去做那些连你作为用户都会觉得”糟糕透顶”的、繁琐的交互操作。</strong></p> </blockquote> <hr/> <h2 id="五未来展望长程任务的自动交付">五、未来展望：长程任务的自动交付</h2> <h3 id="51-自我验证的闭环系统">5.1 自我验证的闭环系统</h3> <p>Erik 预测，智能体将变得更加普及，首先从”可验证”领域开始，比如软件工程。</p> <p><strong>当前状态：</strong></p> <ul> <li>开发者在智能体写完代码后，必须自己充当”QA 工程师”测试</li> </ul> <p><strong>未来突破：</strong></p> <ul> <li>智能体能够自己”封闭测试循环”</li> <li>不仅编写网络应用，还能自己打开、测试、找到自己的 Bug</li> <li>不再等待人类发现问题</li> </ul> <h3 id="52-计算机使用能力的革命性影响">5.2 计算机使用能力的革命性影响</h3> <p><strong>关键能力融合：</strong></p> <p>将”软件工程能力”与”计算机使用”(Computer Use) 能力相结合。</p> <p><strong>计算机使用的含义：</strong></p> <ul> <li>像人一样操作计算机</li> <li>滚动、点击、编辑文本</li> </ul> <p><strong>解锁的新场景：</strong></p> <p>一旦智能体掌握了这种能力，将解锁大量目前被”拒之门外”的领域。</p> <p><strong>具体案例：Google Doc 编辑</strong></p> <p><strong>现状：</strong></p> <ul> <li>在 Claude 界面和文档之间来回复制粘贴</li> </ul> <p><strong>未来：</strong></p> <ul> <li>直接说：”嘿 Claude，帮我清理一下这篇 Google Doc”</li> <li>Claude 直接在文档中操作：滚动、点击、编辑文本</li> </ul> <blockquote> <p><strong>“无论你在哪里，Claude 都能与你同在”</strong>——这将是截然不同的、更高效的交互体验。</p> </blockquote> <h3 id="53-从编码智能体到通用智能体">5.3 从编码智能体到通用智能体</h3> <p><strong>演进路径：</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>编码智能体 → 自我测试编码智能体 → 计算机使用智能体 → 通用自主智能体
</code></pre></div></div> <p>当智能体能够：</p> <ol> <li>理解任务需求</li> <li>编写代码实现</li> <li>自我测试验证</li> <li>操作任何软件界面</li> </ol> <p>它就真正成为了可以自主完成长程任务的通用智能体。</p> <hr/> <h2 id="结语从工具到伙伴的跃迁">结语：从工具到伙伴的跃迁</h2> <h3 id="核心要点回顾">核心要点回顾</h3> <p><strong>1. 编码能力是基础</strong></p> <ul> <li>强大的编码能力可以泛化到所有领域</li> <li>“先训练最难的”策略证明有效</li> </ul> <p><strong>2. 工具演进路径清晰</strong></p> <ul> <li>SDK 解决了基础架构问题</li> <li>Skills 提供了从指令到资源的跃迁</li> </ul> <p><strong>3. 架构复杂度需谨慎</strong></p> <ul> <li>从简单开始，按需增加复杂性</li> <li>可观测性随复杂度指数级下降</li> </ul> <p><strong>4. UI 映射原则至关重要</strong></p> <ul> <li>工具设计应模拟用户界面</li> <li>最小化智能体的交互次数</li> </ul> <h3 id="对开发者的启示">对开发者的启示</h3> <p><strong>构建智能体时的三个关键转变：</strong></p> <ol> <li><strong>思维转变</strong>：从”调用 API”到”模拟 UI”</li> <li><strong>架构转变</strong>：从”链接提示”到”链接智能体”</li> <li><strong>角色转变</strong>：从”编写代码”到”管理智能体”</li> </ol> <h3 id="通往未来的路径">通往未来的路径</h3> <p>当前的智能体开发仍处于早期阶段，但演进路径已经清晰：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>单一智能体 → 智能体工作流 → 多智能体系统 → 自主长程任务执行
</code></pre></div></div> <blockquote> <p><strong>最终目标不是替代人类开发者，而是让 AI 成为真正的协作伙伴。</strong></p> </blockquote> <p>当智能体能够理解你的意图、自主规划任务、执行并验证结果，我们就进入了一个全新的人机协作时代。这不是科幻，而是正在发生的现实。</p> <hr/> <p><strong>来源：</strong> Anthropic 官方播客<br/> <strong>整理：</strong> Anthropic 研究员深度解析 Claude 智能体构建方法论<br/> <strong>整理时间：</strong> 2025年11月6日</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[Anthropic 研究员详解：构建高效 Claude 智能体的完整方法论]]></summary></entry><entry><title type="html">Claude Code自定义命令在知识管理与内容创作中的系统化应用研究</title><link href="https://emigmo.github.io/blog/2025/claude-code-knowledge-management/" rel="alternate" type="text/html" title="Claude Code自定义命令在知识管理与内容创作中的系统化应用研究"/><published>2025-11-05T00:00:00+00:00</published><updated>2025-11-05T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/claude-code-knowledge-management</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/claude-code-knowledge-management/"><![CDATA[<h1 id="claude-code自定义命令在知识管理与内容创作中的系统化应用研究">Claude Code自定义命令在知识管理与内容创作中的系统化应用研究</h1> <p><em>来源：知乎专栏</em><br/> <em>作者：郑二八斤</em><br/> <em>发布时间：2025-11-05</em><br/> <em>原文链接：https://zhuanlan.zhihu.com/p/1969435547006133543</em></p> <hr/> <h2 id="概述">概述</h2> <p>本文介绍了如何利用 Claude Code 的自定义命令能力，构建一个模块化、可迭代、数据驱动的知识管理与内容创作自动化系统。通过四周的实践，整体工作效率提升了 <strong>65%</strong>，从原来的 280 分钟缩短到 98 分钟。</p> <h2 id="核心问题">核心问题</h2> <p>在信息过载的时代，知识工作者面临的主要困境：</p> <ol> <li><strong>收集入口分散</strong>：灵感、会议纪要、阅读笔记散落在不同工具和格式中</li> <li><strong>整理规则不一致</strong>：标签体系混乱，笔记之间缺乏有效链接</li> <li><strong>创作流程重复</strong>：每次写作都要重复全流程，缺乏模板化和自动化</li> <li><strong>互动响应滞后</strong>：读者留言和私信堆积，响应不及时</li> <li><strong>迭代缺乏依据</strong>：缺少量化指标，无法评估工作流程的效率瓶颈</li> </ol> <h2 id="系统设计原则">系统设计原则</h2> <h3 id="1-单一职责原则">1. 单一职责原则</h3> <p>每个命令只负责一个具体任务，避免功能耦合</p> <h3 id="2-可组合性">2. 可组合性</h3> <p>命令之间可以串联组合，形成完整工作流（类似 Unix 管道）</p> <h3 id="3-数据驱动迭代">3. 数据驱动迭代</h3> <p>为每个命令建立量化评估指标，通过持续监测识别瓶颈并优化</p> <h2 id="系统架构">系统架构</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>输入层：语音、文本、截图、链接
    ↓
  /daylog
    ↓
处理层：
  ├─ 调研层：/research-snap
  ├─ 规划层：/topic-outline
  └─ 生成层：/content-draft
    ↓
输出层：/reply-kit
    ↓
存储层：Daily/、Topics/、Projects/
    ↓
反馈层：数据统计、命令优化
</code></pre></div></div> <h2 id="核心命令详解">核心命令详解</h2> <h3 id="1-daylog---知识捕获">1. /daylog - 知识捕获</h3> <p><strong>理论基础</strong>：Zettelkasten（卡片盒）方法<br/> <strong>功能</strong>：将日常笔记、灵感自动整理为结构化笔记，建立知识网络<br/> <strong>效果</strong>：</p> <ul> <li>知识捕获时间：20分钟 → 2分钟（-90%）</li> <li>笔记交叉链接：1.2个 → 4.8个（+300%）</li> </ul> <p>配置文件：<a href="./commands/daylog.md">daylog.md</a></p> <h3 id="2-research-snap---知识综合">2. /research-snap - 知识综合</h3> <p><strong>理论基础</strong>：循证决策（Evidence-Based Decision Making）<br/> <strong>功能</strong>：快速收集和整理研究材料，结构化引用<br/> <strong>效果</strong>：</p> <ul> <li>调研时间：30分钟 → 5分钟（-83%）</li> <li>引用数量：2-3个 → 8-12个（+300%）</li> <li>引用完整度：50% → 100%</li> </ul> <p>配置文件：<a href="./commands/research-snap.md">research-snap.md</a></p> <h3 id="3-topic-outline---结构设计">3. /topic-outline - 结构设计</h3> <p><strong>理论基础</strong>：认知负荷理论（Cognitive Load Theory）<br/> <strong>功能</strong>：根据不同平台（微信、小红书、知乎）生成优化的内容大纲<br/> <strong>效果</strong>：</p> <ul> <li>大纲规划时间：40分钟 → 5分钟（-87%）</li> <li>文章完读率：42% → 53%（+26%）</li> <li>互动率：3.2% → 4.8%（+50%）</li> </ul> <p>配置文件：<a href="./commands/topic-outline.md">topic-outline.md</a></p> <h3 id="4-content-draft---内容生成">4. /content-draft - 内容生成</h3> <p><strong>理论基础</strong>：Flower &amp; Hayes 写作认知过程模型<br/> <strong>功能</strong>：将大纲和调研材料转化为完整初稿<br/> <strong>效果</strong>：</p> <ul> <li>初稿撰写时间：120分钟 → 60分钟（-50%）</li> <li>引用遗漏率：20% → 5%（-75%）</li> </ul> <p>配置文件：<a href="./commands/content-draft.md">content-draft.md</a></p> <h3 id="5-reply-kit---互动管理">5. /reply-kit - 互动管理</h3> <p><strong>理论基础</strong>：共情沟通（Empathetic Communication）<br/> <strong>功能</strong>：快速生成高质量的读者回复<br/> <strong>效果</strong>：</p> <ul> <li>回复时间（10条）：30分钟 → 6分钟（-80%）</li> <li>二次互动率：12% → 28%（+133%）</li> </ul> <p>配置文件：<a href="./commands/reply-kit.md">reply-kit.md</a></p> <h2 id="工作流程示例">工作流程示例</h2> <h3 id="完整创作流程">完整创作流程</h3> <pre><code class="language-mermaid">graph TB
    A[灵感/素材收集] --&gt;|/daylog| B[结构化日记]
    B --&gt; C[选题确认]
    C --&gt;|/research-snap| D[调研报告]
    D --&gt;|/topic-outline| E[内容大纲]
    E --&gt;|/content-draft| F[初稿]
    F --&gt; G[人工审阅优化]
    G --&gt; H[发布]
    H --&gt; I[读者反馈]
    I --&gt;|/reply-kit| J[互动回复]
    J --&gt; K[数据复盘]
    K --&gt; A
</code></pre> <h3 id="效率对比">效率对比</h3> <table> <thead> <tr> <th>工作环节</th> <th>优化前</th> <th>优化后</th> <th>提升幅度</th> </tr> </thead> <tbody> <tr> <td>灵感捕获与整理</td> <td>20分钟</td> <td>2分钟</td> <td>-90%</td> </tr> <tr> <td>调研资料整合</td> <td>30分钟</td> <td>5分钟</td> <td>-83%</td> </tr> <tr> <td>大纲规划</td> <td>40分钟</td> <td>5分钟</td> <td>-87%</td> </tr> <tr> <td>初稿撰写</td> <td>120分钟</td> <td>60分钟</td> <td>-50%</td> </tr> <tr> <td>审阅修改</td> <td>40分钟</td> <td>20分钟</td> <td>-50%</td> </tr> <tr> <td>留言回复(10条)</td> <td>30分钟</td> <td>6分钟</td> <td>-80%</td> </tr> <tr> <td><strong>总计</strong></td> <td><strong>280分钟</strong></td> <td><strong>98分钟</strong></td> <td><strong>-65%</strong></td> </tr> </tbody> </table> <h2 id="roi-分析">ROI 分析</h2> <p><strong>初始投入</strong>：16小时（学习2h + 设计8h + 测试6h）<br/> <strong>每周收益</strong>：3小时<br/> <strong>回本周期</strong>：约5.3周<br/> <strong>年收益</strong>：156小时（约15,600元，按时薪100元计算）<br/> <strong>ROI</strong>：875%</p> <h2 id="适用场景">适用场景</h2> <h3 id="高度适用-">高度适用 ⭐⭐⭐⭐⭐</h3> <ul> <li>内容创作者（公众号运营、博主、自媒体）</li> <li>知识工作者（研究员、咨询顾问、产品经理）</li> <li>个人知识管理需求者</li> </ul> <h3 id="适度适用-">适度适用 ⭐⭐⭐</h3> <ul> <li>团队协作（需要统一输出标准）</li> <li>教育场景（教学素材整理、学习笔记）</li> </ul> <h3 id="不适用-">不适用 ⭐</h3> <ul> <li>高度创意性工作（纯艺术创作、原创性学术研究）</li> <li>实时性要求极高的场景（新闻报道）</li> <li>需要深度人际互动的场景（心理咨询）</li> </ul> <h2 id="系统局限性">系统局限性</h2> <ol> <li><strong>依赖明确的规则</strong>：任务缺乏明确标准时，输出质量下降</li> <li><strong>初期学习成本</strong>：前两周需要频繁调整，学习曲线较陡</li> <li><strong>创造性天花板</strong>：AI倾向于生成”安全”但平庸的内容</li> <li><strong>上下文窗口限制</strong>：超长内容可能遗漏部分信息</li> <li><strong>成本考量</strong>：频繁调用API会产生费用</li> </ol> <h2 id="快速开始">快速开始</h2> <h3 id="1-创建命令目录">1. 创建命令目录</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> .claude/commands
</code></pre></div></div> <h3 id="2-配置命令文件">2. 配置命令文件</h3> <p>将本目录下 <code class="language-plaintext highlighter-rouge">commands/</code> 中的5个命令文件复制到项目的 <code class="language-plaintext highlighter-rouge">.claude/commands/</code> 目录</p> <h3 id="3-验证配置">3. 验证配置</h3> <ul> <li>打开 Cursor</li> <li>在聊天框输入 <code class="language-plaintext highlighter-rouge">/</code></li> <li>查看是否显示自定义命令</li> <li>测试运行</li> </ul> <h2 id="进阶优化">进阶优化</h2> <h3 id="数据追踪表格示例">数据追踪表格示例</h3> <table> <thead> <tr> <th>日期</th> <th>命令</th> <th>耗时</th> <th>质量</th> <th>需优化点</th> <th>优化措施</th> </tr> </thead> <tbody> <tr> <td>11-05</td> <td>/daylog</td> <td>2min</td> <td>4.5/5</td> <td>链接推荐精准度</td> <td>增加语义相似度阈值</td> </tr> <tr> <td>11-05</td> <td>/research-snap</td> <td>5min</td> <td>5/5</td> <td>完美</td> <td>-</td> </tr> </tbody> </table> <h3 id="扩展命令建议">扩展命令建议</h3> <p><strong>垂直领域定制</strong>：</p> <ul> <li><code class="language-plaintext highlighter-rouge">/legal-research</code>：法律领域调研</li> <li><code class="language-plaintext highlighter-rouge">/academic-paper</code>：学术论文生成</li> <li><code class="language-plaintext highlighter-rouge">/product-spec</code>：产品需求文档</li> </ul> <p><strong>工作流程深化</strong>：</p> <ul> <li><code class="language-plaintext highlighter-rouge">/taxonomy-audit</code>：标签一致性检查</li> <li><code class="language-plaintext highlighter-rouge">/content-repurpose</code>：多平台改写</li> <li><code class="language-plaintext highlighter-rouge">/seo-optimizer</code>：SEO优化建议</li> </ul> <h2 id="核心发现总结">核心发现总结</h2> <ol> <li><strong>自动化收益与任务重复性正相关</strong>：重复性高的任务自动化收益最大（80-90%）</li> <li><strong>质量提升与标准化程度正相关</strong>：有明确标准的任务质量提升显著（30-40%）</li> <li><strong>系统价值呈现网络效应</strong>：多个命令组合形成工作流网络时，价值呈指数级增长</li> <li><strong>人机协作优于纯AI或纯人工</strong>：”AI生成初稿+人工审阅优化”的模式最优</li> </ol> <h2 id="参考资料">参考资料</h2> <ul> <li>Zettelkasten方法：Niklas Luhmann的卡片盒笔记法</li> <li>认知负荷理论：Sweller, 1988</li> <li>写作认知过程模型：Flower &amp; Hayes, 1981</li> <li>循证决策理论：Evidence-Based Decision Making</li> </ul> <h2 id="讨论话题">讨论话题</h2> <ol> <li>当AI接管重复性任务后，如何避免创造性思考能力的退化？</li> <li>在追求效率和标准化的过程中，如何保持内容的个性和独特性？</li> <li>在知识管理中，工具选择重要还是系统设计重要？</li> <li>如何评估自动化系统的长期价值？</li> <li>这套系统在团队协作场景中的应用有哪些挑战和机遇？</li> </ol> <hr/> <h2 id="文件结构">文件结构</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2025-11-05-claude-code-knowledge-management/
├── README.md                    # 本文档
└── commands/                    # 命令配置文件
    ├── daylog.md               # 知识捕获命令
    ├── research-snap.md        # 知识综合命令
    ├── topic-outline.md        # 结构设计命令
    ├── content-draft.md        # 内容生成命令
    └── reply-kit.md            # 互动管理命令
</code></pre></div></div>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[Claude Code自定义命令在知识管理与内容创作中的系统化应用研究]]></summary></entry><entry><title type="html">18个改变人生的习惯：科学证据支持的长期主义指南</title><link href="https://emigmo.github.io/blog/2025/habit/" rel="alternate" type="text/html" title="18个改变人生的习惯：科学证据支持的长期主义指南"/><published>2025-11-01T00:00:00+00:00</published><updated>2025-11-01T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/habit</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/habit/"><![CDATA[<h1 id="18个改变人生的习惯科学证据支持的长期主义指南">18个改变人生的习惯：科学证据支持的长期主义指南</h1> <p><em>整理时间：2025年11月1日</em> <em>内容来源：综合研究整理</em></p> <hr/> <h2 id="前言">前言</h2> <blockquote> <p><strong>「复利效应在人生各个维度都存在，但只有极少数人能坚持足够久。」</strong></p> </blockquote> <p>本文汇总了18个经过科学验证、能够显著提升生活质量的长期习惯。这些习惯涵盖健康、学习、财富、人际关系四个核心维度，每一个都有权威研究支持，且具备可操作性。</p> <p>真正的改变不在于某一天的大动作，而在于每一天的微小坚持。</p> <hr/> <h2 id="一健康基石身心健康是一切的根基">一、健康基石：身心健康是一切的根基</h2> <h3 id="1-每天运动30分钟--exercise-for-30-minutes-every-day">1. 每天运动30分钟 | Exercise for 30 minutes every day</h3> <p>《柳叶刀》研究显示，每天30分钟中等强度运动可降低27%全因死亡率，相当于每运动1分钟延长寿命7分钟。</p> <p><strong>科学机制</strong>：运动时身体会分泌脑源性神经营养因子（BDNF），促进大脑神经元生长，提高记忆力和专注力。</p> <p><strong>实践建议</strong>：</p> <ul> <li>选择快走、游泳等可持续性运动</li> <li>配合2-3次/周的力量训练效果更佳</li> </ul> <h3 id="2-保证7小时睡眠--ensure-7-hours-of-sleep">2. 保证7小时睡眠 | Ensure 7 hours of sleep</h3> <p>哈佛医学院研究发现，持续睡眠不足6小时的人群，肥胖风险增加30%，记忆力下降40%。</p> <p><strong>科学机制</strong>：深度睡眠阶段大脑会进行”记忆整理”，清除β淀粉样蛋白（阿尔茨海默病的致病蛋白）。</p> <p><strong>实践建议</strong>：</p> <ul> <li>设置固定作息时间</li> <li>睡前1小时避免蓝光刺激</li> <li>室温控制在18-22℃最佳</li> </ul> <h3 id="3-定期口腔护理--regular-oral-care">3. 定期口腔护理 | Regular oral care</h3> <p>WHO数据显示，牙周病患者心脏病风险是普通人的2倍，糖尿病风险增加3倍。每年洗牙可预防牙龈萎缩，节省后续数万元的种植牙费用。</p> <p><strong>实践建议</strong>：</p> <ul> <li>使用含氟牙膏+巴氏刷牙法</li> <li>配合冲牙器清理牙缝</li> <li>每半年进行一次专业洁治</li> </ul> <h3 id="4-冥想训练--meditation-training">4. 冥想训练 | Meditation Training</h3> <p>哈佛医学院脑扫描显示，每日冥想10分钟，8周后杏仁核（焦虑中枢）缩小19%。</p> <p><strong>实践建议</strong>：</p> <ul> <li>从呼吸冥想入门，逐步练习身体扫描等技巧</li> <li>使用潮汐APP引导</li> <li>选择固定时段练习效果更佳</li> </ul> <hr/> <h2 id="二持续学习终身学习是核心竞争力">二、持续学习：终身学习是核心竞争力</h2> <h3 id="5-每周深度阅读10小时--10-hours-of-in-depth-reading">5. 每周深度阅读10小时 | 10 hours of in-depth reading</h3> <p>耶鲁大学长达12年的追踪研究表明，每周阅读3.5小时书籍的人群平均寿命延长23个月。</p> <p><strong>科学机制</strong>：深度阅读时大脑的默认模式网络(DMN)会被激活，提升同理心和决策能力。</p> <p><strong>实践建议</strong>：</p> <ul> <li>选择纸质书减少干扰</li> <li>配合康奈尔笔记法记录重点</li> </ul> <h3 id="6-掌握第二语言--mastering-a-second-language">6. 掌握第二语言 | Mastering a second language</h3> <p>《神经科学》期刊研究发现，双语使用者老年痴呆症发病时间平均延迟4.5年。</p> <p><strong>科学机制</strong>：语言学习能增加大脑灰质密度，LinkedIn数据显示掌握双语者薪资溢价达15-20%。</p> <p><strong>实践建议</strong>：</p> <ul> <li>使用沉浸式学习法</li> <li>每天30分钟结合影视剧/播客输入</li> </ul> <h3 id="7-刻意练习写作--deliberately-practicing-writing">7. 刻意练习写作 | Deliberately practicing writing</h3> <p>斯坦福大学写作项目显示，定期写作的人职业晋升速度快37%。</p> <p><strong>核心价值</strong>：写作能梳理思维盲点，自媒体时代更是个人品牌放大器。</p> <p><strong>实践建议</strong>：</p> <ul> <li>从每天300字日记开始</li> <li>逐步尝试观点文、产品文案等不同体裁</li> </ul> <h3 id="8-建立错题本--establish-a-mistake-book">8. 建立错题本 | Establish a mistake book</h3> <p>剑桥大学研究证实，定期复盘错误的学习者，知识保留率比普通学习者高65%。</p> <p><strong>实践建议</strong>：</p> <ul> <li>用电子笔记记录职场/生活中的重大失误</li> <li>每月分析1次共同模式，制定改进方案</li> </ul> <h3 id="9-掌握ai工具链--master-the-ai-toolchain">9. 掌握AI工具链 | Master the AI toolchain</h3> <p>麦肯锡最新报告指出，熟练使用AI工具的职场人生产效率提升40%，被自动化取代风险降低73%。</p> <p><strong>实践建议</strong>：</p> <ul> <li>每月掌握1个新工具</li> <li>如ChatGPT提示词工程、Midjourney视觉生成、Notion AI知识管理等</li> </ul> <hr/> <h2 id="三财富积累理性规划与长期投资">三、财富积累：理性规划与长期投资</h2> <h3 id="10-系统学习理财--systematic-learning-of-financial-management">10. 系统学习理财 | Systematic learning of financial management</h3> <p>诺贝尔经济学奖得主马科维茨证实，合理资产配置可降低30%风险并提高15%收益。</p> <p><strong>实践建议</strong>：</p> <ul> <li>按”4321法则”分配资金：40%投资+30%生活+20%储蓄+10%保险</li> <li>从指数基金定投开始，逐步学习企业财报分析等技能</li> </ul> <h3 id="11-每月定投指数基金--monthly-fixed-investment-index-fund">11. 每月定投指数基金 | Monthly fixed investment index fund</h3> <p>沃顿商学院研究显示，定投标普500指数20年以上的投资者，93%跑赢主动管理基金。</p> <p><strong>核心优势</strong>：指数基金费率仅0.03%-0.15%，避免了个股暴雷风险。</p> <p><strong>实践建议</strong>：</p> <ul> <li>设置工资到账自动扣款</li> <li>采用”低估多买、高估持有”策略</li> </ul> <h3 id="12-购置核心资产--purchase-core-assets">12. 购置核心资产 | Purchase core assets</h3> <p>诺贝尔经济学奖得主席勒研究表明，核心地段房产长期回报率跑赢通胀3-5%。</p> <p><strong>实践建议</strong>：</p> <ul> <li>首套房建议选择”交通+学区+商业”至少占两项的物业</li> <li>面积控制在90-120㎡流动性最佳</li> <li>避免过度装修，硬装预算控制在2000元/㎡以内</li> </ul> <h3 id="13-发展副业收入--developing-sideline-income">13. 发展副业收入 | Developing sideline income</h3> <p>Upwork调研显示，拥有多重收入的职场人抗风险能力提高5倍。</p> <p><strong>实践建议</strong>：</p> <ul> <li>在保持主业的同时，从技能型副业起步（咨询/设计/写作）</li> <li>逐步建立”睡后收入”系统</li> <li>副业收入达到主业30%时再考虑辞职</li> </ul> <hr/> <h2 id="四人际关系与心理建设软实力决定上限">四、人际关系与心理建设：软实力决定上限</h2> <h3 id="14-维护关键人脉--maintain-key-networks">14. 维护关键人脉 | Maintain key networks</h3> <p>LinkedIn数据显示，85%的高薪职位通过弱关系（朋友的朋友）获得。</p> <p><strong>实践建议</strong>：</p> <ul> <li>每月至少联系1位优质人脉</li> <li>采用”价值分享+适度求助”的维护策略</li> <li>见面时准备3个有深度的话题，避免单纯的寒暄</li> </ul> <h3 id="15-学习非暴力沟通--learning-nonviolent-communication">15. 学习非暴力沟通 | Learning Nonviolent Communication</h3> <p>心理学家马歇尔研究发现，采用”观察-感受-需要-请求”沟通模式，能提升83%的矛盾解决率。</p> <p><strong>关键技巧</strong>：</p> <ul> <li>用”我注意到”代替”你总是”</li> <li>提出具体请求而非模糊要求</li> </ul> <h3 id="16-培养感恩心态--cultivate-a-grateful-mindset">16. 培养感恩心态 | Cultivate a grateful mindset</h3> <p>《积极心理学杂志》实验显示，持续6周感恩练习可使抑郁症状减少28%。</p> <p><strong>实践建议</strong>：</p> <ul> <li>每晚记录3件值得感恩的小事</li> <li>定期向帮助过自己的人表达感谢</li> <li>感恩日记最好手写，激活大脑情感中枢更充分</li> </ul> <h3 id="17-培养幽默感--cultivate-a-sense-of-humor">17. 培养幽默感 | Cultivate a sense of humor</h3> <p>《人格与社会心理学》杂志指出，幽默感强的人抗压能力提升40%，人际关系质量提高58%。</p> <p><strong>实践建议</strong>：</p> <ul> <li>每天收集3个幽默素材（段子/趣图）</li> <li>在社交场合适时���用</li> </ul> <hr/> <h2 id="五人生哲学建立可持续的行动框架">五、人生哲学：建立可持续的行动框架</h2> <h3 id="18-设置人生基准线--set-a-life-baseline">18. 设置人生基准线 | Set a life baseline</h3> <p>斯坦福大学建议设立不可妥协的底线标准（如每日阅读30分钟/每周运动3次）。</p> <p><strong>核心理念</strong>：当状态低迷时，优先完成基准线任务即可，避免陷入”全有或全无”的极端思维。</p> <p><strong>实践价值</strong>：基准线思维能帮助我们在低谷期维持最小化进展，避免彻底放弃导致的习惯断裂。</p> <hr/> <h2 id="结语">结语</h2> <p>这18个习惯不是要求你立刻全部开始，而是提供一个可选择的清单。建议：</p> <ol> <li><strong>先选择3个最适合当前阶段的习惯</strong></li> <li><strong>用21天建立初步习惯，90天固化为自然行为</strong></li> <li><strong>每季度复盘一次，逐步增加新习惯</strong></li> </ol> <p>记住：<strong>真正的成长，是在时间的复利下，一点点积累出来的。</strong></p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[18个改变人生的习惯：科学证据支持的长期主义指南]]></summary></entry><entry><title type="html">KIMI创始人杨植麟深度访谈：攀登无限之山</title><link href="https://emigmo.github.io/blog/2025/kimi-yang-dialogue/" rel="alternate" type="text/html" title="KIMI创始人杨植麟深度访谈：攀登无限之山"/><published>2025-10-23T00:00:00+00:00</published><updated>2025-10-23T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/kimi-yang-dialogue</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/kimi-yang-dialogue/"><![CDATA[<h1 id="kimi创始人杨植麟深度访谈攀登无限之山">KIMI创始人杨植麟深度访谈：攀登无限之山</h1> <p><em>主持人：张小珺</em><br/> <em>时间：2025年7月</em><br/> <em>地点：北京知春路京东科技大厦13层</em></p> <hr/> <h2 id="目录">目录</h2> <h3 id="第一章-一座无限的山">第一章 一座无限的山</h3> <ul> <li><a href="#01-the-beginning-of-infinity">01 The Beginning of Infinity - 无穷的开始</a></li> <li><a href="#02-它还是一个缸中之脑">02 它还是一个”缸中之脑”</a></li> <li><a href="#03-l1到l5不一定是串行关系">03 L1到L5不一定是串行关系</a></li> </ul> <h3 id="第二章-k2是乔戈里峰">第二章 K2是乔戈里峰</h3> <ul> <li><a href="#04-喂一样多的数据脑子长得更多">04 喂一样多的数据，”脑子”长得更多</a></li> <li><a href="#05-muon你去训的时候它会炸">05 Muon你去训的时候，它会炸</a></li> <li><a href="#06-当从缸中之脑变成跟世界交互的系统">06 当从”缸中之脑”变成跟世界交互的系统</a></li> </ul> <h3 id="第三章-既简单又复杂的系统">第三章 既简单又复杂的系统</h3> <ul> <li><a href="#07-开源-vs-闭源">07 开源 vs 闭源</a></li> <li><a href="#08-多模态不损伤脑子已经很好了">08 多模态不损伤”脑子”已经很好了</a></li> <li><a href="#09-当你通过新的交互收集的信号噪声减少">09 当你通过新的交互，收集的信号噪声减少</a></li> <li><a href="#10-long-context架构会影响智商">10 Long Context架构会影响”智商”</a></li> <li><a href="#11-边界与现实">11 边界与现实</a></li> </ul> <h3 id="第四章-在自己的故事里面">第四章 在自己的故事里面</h3> <ul> <li><a href="#12-用rl的方式去管理而不是用sft">12 用RL的方式去管理，而不是用SFT</a></li> <li><a href="#13-ai是人类文明的放大器">13 AI是人类文明的放大器</a></li> <li><a href="#14-任何中间状态都有可能成为被批评的对象">14 任何中间状态都有可能成为被批评的对象</a></li> </ul> <hr/> <h2 id="第一章-一座无限的山-1">第一章 一座无限的山</h2> <h3 id="01-the-beginning-of-infinity">01 The Beginning of Infinity</h3> <p>张小珺：在你创业第一年结束的时候，2024年我们访谈标题是《向延绵而未知的雪山前进》。现在又过了一年，站在此刻，2025年7月，你最新的感受是怎样的？</p> <p>杨植麟：你刚刚提到这个词，我感觉好像过了很久……AI一天，人间一年。AI的一年我不知道是人间多少天。确实很多东西发生了变化，但你说的”雪山的感觉”，倒是差不太多。</p> <p>往山顶方向，我们又走了一段距离。</p> <p>张小珺：现在行进到哪里了？</p> <p>杨植麟：站在现在看，模型的进步挺大——两年前写一篇文章都写不太明白，现在不光可以写很好的文章，还能连续工作几小时，帮你完成一个很复杂的代码任务。这在两年前很难想象。</p> <p>在爬雪山的过程中，解锁了一些新的场景，大概知道中间这条路是什么样的；但同时，在往上的过程中，你还是观察到类似的景象——接下来，仍然会有很多未知的技术问题要去解决。</p> <p>张小珺：在四下都是大雪的山峰上，你是更清晰了，还是更迷惘了？</p> <p>杨植麟：肯定会有很多东西变得更清楚。两年前各种强化学习（RL，Reinforcement Learning）的范式该怎么做，怎么让模型有更强的推理能力，或更强的Agentic（智能体式）能力，当时都没那么清楚。那时更多关注模型的预训练（Pre-Training）怎么做得更好，怎么用RLHF（Reinforcement Learning from Human Feedback，人类反馈强化学习）提升对话体验。</p> <p>但现在有一些问题得到了答案，同时这些答案又展开，带来了新的其他问题。</p> <p>现在虽然可以做强化学习，但它最终还是依赖一个很好的评估或验证机制。你让模型去做一道数学题，或者做一些有test case（测试样例）的编程任务，可能做得比较好。但如果让它去做一个更复杂的端到端任务，有时很难找到一个合适的评估或衡量方式。所以，这个系统又会产生新的问题。</p> <p>这有点像我最近在看的一本书，叫The Beginning of Infinity（无穷的开始），我看了好几遍。书中说，有两句话可以刻在石头上：一句是”问题是不可避免的”，另一句是”问题是可以解决的”。</p> <p>《无穷的开始》（The Beginning of Infinity）是物理学家David Deutsch撰写的一本科普哲学著作。</p> <p><strong>书中观点</strong>：科学知识和理性探究能带来无限的进步和理解，世界上的问题无穷无尽，但绝大多数问题都可以通过理性思考和科学方法得到解决。知识的增长无界限，科学探索是一个不断解决问题、产生新问题的过程，这是人类文明不断前进的动力。</p> <p>你可以认为，启蒙运动前，这个社会是静态的。大家不追求创新，会用很多神秘主义来解释所看到的现象，但这些解释并不是好的解释。比如，你看到天上打雷，会觉得雷公在发怒；你看到冬天下雪，会说某个神心情不好——整个社会结构是静态的，只有极少数人在真正做科学研究或知识创造。</p> <p>但启蒙运动之后，社会变成了动态的，新的知识不断被创造。每当你解决一个问题，就会带来新的问题。问题是源源不断的，因为你的知识边界在拓展，你就会遇到新的问题。</p> <p>现在做AI研发也恰好处于这样的状态。你解决了强化学习的一些问题，接下来就遇到评估、衡量、验证这些问题，又需要我们寻找新的答案。</p> <p>但这也正是它有意思的地方——你总是会有新的问题去解决，而每当你解决一个问题，技术就能再往上攀登几百米。</p> <p>也许有一天会发现，这座雪山没有尽头——我不知道——我希望它一直没有尽头。</p> <p>The Beginning of Infinity的意思就是这样：</p> <p>它是一座无限的山。</p> <h3 id="02-它还是一个缸中之脑">02 它还是一个”缸中之脑”</h3> <p>张小珺：我们回望一下，过去一年全球大模型在你脑海中最重要的几件事是什么？有哪些是人工智能范式级的变化？</p> <p>杨植麟：一个是长思考的推理模型（Reasoning Model），以o1作为第一个做出来的代表。本质上，它通过让模型在过程中做很多尝试和反思，反思是其中的重点。</p> <p>反思是两种能力：一种是提出新的猜想，一种是验证猜想。</p> <p>你可以理解为：模型在解决问题的过程中，会不断提出新的猜想，这个猜想会得到自我验证。比如，它提出这个猜想之后，要判断是对是错，需要具备一定的验证能力。虽然你不是显式地训练一个验证模型，但它在推理过程中隐式地进行了验证。它对这个问题做了多次猜想和验证，最后得到一个答案。</p> <p>这大大提升了模型的能力。你本来只能做一次，直接给出一个答案，这个答案可能对，也可能错，你没有这个过程。但现在你可以不断提出猜想去验证，相当于等价尝试了好几次。</p> <p>你可以把Pass@k变成Pass@1，本质是这样的道理。</p> <p><strong>Pass@k</strong>：用于评估大模型的一种指标，表示模型在一次任务中生成k个候选答案时，至少有一个答案正确的概率。</p> <p>这跟人做科研或解题的过程很像，不断提出新的猜想，然后验证它。</p> <p>张小珺：是一个自由探索的过程，而不是一个线性的过程。</p> <p>杨植麟：它有效的工作方式，很多时候还是比较线性的。如果不考虑并行采样，假设做的是串行采样，它就是线性过程。</p> <p>你每次提出新的猜想，这个猜想可能基于之前的猜想，甚至是你已经否定过的猜想，再提出一个新的猜想。它会接近更线性的过程。</p> <p>现在你也可以把线性过程和并行策略搭配，比如同时采样很多个，结合了并行和串行两种方式。最近也有一些paper讲串行采样上限更高，这跟我们的实验结论一致。</p> <p>上面说的是一种范式，但它还是一个“缸中之脑”（brain in a vat），并不需要跟外界交互。</p> <p>张小珺：缸中之脑？</p> <p>杨植麟：想象一个鱼缸，你把一个脑子放在里面，跟外界没有联系。它只是在自己大脑里面想，一直想，不需要跟外界产生任何交互，就能解一道题。</p> <p>但有另一个很重要的范式，就是基于多轮的Agent（智能体）强化学习范式，或者通过强化学习技术训练出来的Agentic模型，它的特点是会跟外界做很多交互。</p> <p>比如我边思考边去做一些操作，可能做很多轮操作，一会儿调用一个搜索，一会儿使用一下浏览器，一会儿写几行代码，通过多轮解决一个问题。</p> <p>它就不再是“缸中之脑”，是跟外界有交互的——我的下一步行为，是根据交互中得到的反馈，和外界给我的新状态的更新有关系。</p> <p>但这两个东西都指向了同一个东西，是：test-time scaling（测试时扩展）。意思是，可以在测试时，或者在推理时，做到更好的规模化。</p> <p>比如之前做Chat（聊天），更多是单轮地输出一个结果：我让你写一篇文章，你就写一篇文章；我再让你润色一下，你又输出几百个token（词元），这个token数量是很少的。但不管是基于长思考的强化学习，还是Agent的强化学习，本质上都是一种在预测时对token进行规模化扩展的方式。</p> <p>你不管是把轮数打得更多，还是在每一轮有更多思考的token，都是一种规模化token的方法，让你能完成更复杂的任务。</p> <p>这也伴随完成时间更长。你现在花几小时去做一件复杂的事，过程中不需要人工参与。比如，把一个代码仓库克隆下来，翻译成另一种新语言，调试、测试，把所有bug（漏洞）修复，让它能正常运行。这样的工作可以端到端完成，得益于测试时计算（test-time）的规模化。</p> <p>还有一个很有意思的趋势是，现在有更多模型公司去做“一方的Agent产品”。</p> <p><strong>“一方的Agent产品”</strong>：指模型公司自己下场做产品，自己控制上下文环境、工具接口、prompt结构等，也就是自己当”使用方”，而不是只给别人提供模型API。 一开始，我们看半年前或者去年，很多产品是基于基础模型，你在上面搭一些脚手架，或者设计一些工具去更好地让模型使用，从而搭建一个产品。 张小珺：享受模型溢出的能力。 杨植麟：对，它本质做的是逆向工程这个模型的训练过程。因为模型训练过程也是通过各种手段——你可以认为Anthropic在内部（in-house）的环境、工具、脚手架，可能训练出这样一个模型，但它没有直接开放给你。</p> <p>你通过逆向，更接近拟合它的分布——到底用什么工具效果会好？到底用什么样的System Prompt（系统提示）效果会好？到底用什么样的Context Engineering（上下文工程）效果会好？这是一个逆向的过程。</p> <p>但你会发现，如果模型公司去做“一方的产品”，逻辑完全不一样。</p> <p>你不再需要这个逆向的过程，更多是正向的做法。我先把这些工具设计好，我的Context Engineering（上下文工程）的方法都设计好，我就在这个环境里训练这个模型，所以模型天然在你的环境里表现更好。</p> <p>这是两种不同思路，但第二种思路上限也许更高。</p> <p>你可以更好整合工具和模型。模型如果有些地方解决不好，你可以调整工具设计，把它设计得更好，同时又可以端到端训练。这也是在开发方式一个比较大的变量。</p> <p>张小珺：让大家比较好理解。第一种你说的脚手架，就是像Manus这种开发方式；第二种是你们这样的端到端训练模型的方式。</p> <p>杨植麟：对。当然，我们现在在“一方产品”投入还不算特别多，是以模型为主线。但Claude Code或ChatGPT Agent，就是“一方的产品”，应该也会是一个很大趋势。</p> <p>后面就看“一方”和“三方”产品怎么配合，在生态里会是什么样的状态。</p> <h3 id="03-l1到l5不一定是串行关系">03 L1到L5不一定是串行关系</h3> <p>张小珺：说到主线，OpenAI设置了从L1到L5的分级，我一直很好奇它背后的内在逻辑。</p> <p>L1是Chatbot（聊天机器人），L2是Reasoner（推理者），L3是Agent（智能体），L4是Innovator（创新者），L5是Organizer（组织者）。</p> <p>为什么是在有了Chatbot和Reasoner之后才有Agent？为什么接下来又是Innovator和Organizer？这个顺序在能力结构上是怎样递进的？</p> <p>杨植麟：它是能力一步一步的依赖。Agent的（L3）上限取决于，你有很强的Reasoning（L2、推理）能力，但并不是必须先有Reasoning。</p> <p>假设我们把技术发展的顺序稍微换一下，你先做出来Agent能力，再去做狭义的Reasoning，就是long CoT（长链式思维）的Reasoning，可能也是成立的。</p> <p>你可以认为Claude的路线就是bet（押注）这一点：它在Reasoning上做得不是特别多，但在Agent上做得非常好。这背后是不同技术路径的bet（押注）。但最终你绕不开，如果你想往山顶再多爬几步，这两个能力都要有，只是时间问题。</p> <p>所以，他们并不一定是互相依赖的。不是非得先做Reasoning再做Agent。但要做到最好的Agent，就必须把Reasoning也做到最好。而有了Agent的能力，就可以去做后面的一些事。</p> <p>为什么说下一阶段（L4）是Innovation（创新）？这里面最关键的是：模型什么时候能参与模型本身的开发？只有当模型参与到开发过程，才能解锁真正的Innovator（创新者）阶段。</p> <p>我们希望K2能参与到K3的开发，如果你没有Agentic能力，很难做到这件事。但当你具备了Agentic能力，它就可以提出一些新的想法，做对应的实验，分析实验结果，得出结论，迭代下一版想法，或者优化某个Infra（基础设施）性能。这些都依赖很强的Agentic能力才能做。</p> <p>Innovation（L4、创新）和Organization（L5、组织）也不一定是完全线性的关系，有的也是并行的。我们已经看到这样的趋势——</p> <p>比如说，当你有了一个Agent，就可以拓展成Multi-Agent System（多智能体系统）。你可以从一个Agent fork（分叉）出很多不同的Agents，让它们去做不同的事情。有些是串行的，有些是并行的，然后再合并，再拆分成不同的task（任务）。可能有的写测试，有的写文档，有的设计软件框架，有各自的分工。</p> <p><strong>Multi-Agent System（多智能体系统）</strong>：由多个相互独立但可以协作或竞争的智能体组成的系统。多智能体系统可以表现为多个由语言模型驱动的Agent在一个环境中协作、对话、解决任务，例如：一个Agent负责规划（planner），一个负责执行（executor），另一个监督结果（evaluator）。多个Agent扮演不同专家角色，围绕一个复杂问题展开”思辨式”讨论。 所以它不一定是线性关系，可能两个（L4和L5）同时发生。</p> <p>但Reasoning（L2）和Agent（L3），可能是Innovation（L4）和Organization（L5）的前提。</p> <p>张小珺：Innovation（L4）的标志是模型自我迭代，那Organization（L5）呢？</p> <p>杨植麟：比较简单的一个思考是，它会是一个 Multi-Agent（多智能体）系统。</p> <p>当然，你怎么把Multi-Agent系统很好地做端到端训练，不要过拟合到某几种Agent类型，让它有更好的泛化性，比较有挑战。</p> <p>张小珺：Organization是模型的“雪山封顶”吗？</p> <p>杨植麟：也不是，可能它真的就没有顶。</p> <p>张小珺：所以你把L1到L5的分级当作一个什么样的刻度？</p> <p>杨植麟：它是几个重要的技术milestone（里程碑），但它并不一定是串行关系。它不是我们预期某一个能力被解决之后，才去解决下面的问题。</p> <p>比如Reasoning。如果你真的要解决开放性推理问题，就需要做很强的Innovation，提出新的模型架构，而这又对推理能力提出了更高要求。本质上，是用L4的方法去解决L2的问题，把L2做得更好。</p> <p>这几个能力，随着时间推移，会持续变得更好。</p> <p>不过，你在里面有不同技术bet，会让你短期路径出现一些区别。这些短期路径的区别也会有影响——因为你面对的是动态的市场。</p> <p>张小珺：以前我们会固化认为终点是AGI，今天的终点不是AGI了，那么AGI是什么？</p> <p>杨植麟：AGI不是某一级台阶，你爬到了这级台阶，突然一夜之间达到AGI。而是说：它是一个方向。</p> <p>今天在很多领域，你可以认为已经AGI了，做得比99%人类更好。很多数学或编程竞赛，按现在的提升速度，预想很快有很多问题被充分解决。</p> <p>AGI有两个层面：一方面技术一直在提升；另一方面是技术对人类社会的影响。后者是一个更长周期的事情，也是AGI的一部分。</p> <p>这有点像蒸汽机产生后，社会变化需要几十、几百年去消化。一些工作变得不再必要，但会产生新的工作。每个人变成“超人”，可以做更多事情。社会的工作方式和运行效率会发生巨大变化。</p> <p>虽然我们用“登月”（Moonshot）命名公司，但它跟登月有区别。登月是你站上月球那一刻，可以号称“我达到了”。而AI很难在某个时间突然喊出口号，说“我们此时此刻实现了AGI”。</p> <p>你一直在往上爬。</p> <p>甚至，过一段时间之后，不一定是你自己在爬，是你用AI在爬。现在我们让K2做数据处理、模型分析、模型训练，以前都要人工做，以后慢慢交给模型做——它像一个放大器，帮你更好地攀登这座山。</p> <p>张小珺：如果雪山是无尽的，你追求的是什么？</p> <p>杨植麟：就是攀登的过程。</p> <p>你原来在山底下，现在又往上提升了一点，能看到的景色不一样。</p> <p>它是一个动态进化的过程。</p> <h2 id="第二章-k2是乔戈里峰-1">第二章 K2是乔戈里峰</h2> <h3 id="04-喂一样多的数据脑子长得更多">04 喂一样多的数据，“脑子”长得更多</h3> <p>张小珺：我们来复盘一下，你创业这两年的关键决策。2023到2024年你的关键决策是——在23年2月决定了创业、开始融资、组建团队；到了下半年，Kimi上线了、bet了长文本。</p> <p>2024到2025年，这一年你的几个关键决策是什么？</p> <p>杨植麟：很重要的一点是，技术上我们从以预训练和SFT（Supervised Fine-Tuning，监督微调）为重点研发范式，转变成以预训练和强化学习为重点的方式。这就需要做很多事——不管是人才的储备，还是研发方式的改变。</p> <p>另一点是，从对话到Agent是一个重要范式转变，很大程度影响我们的实际工作方式。</p> <p>张小珺：过去半年你们推出了K1.5和K2，它们分别对Kimi意味着什么？</p> <p>杨植麟：K1.5更多是强化学习技术的验证。</p> <p>张小珺：追赶o1？</p> <p>杨植麟：对，我们比较早在这个技术路线投入，得到一些结果，看背后的技术到底怎么做。</p> <p>当时我们发现不太需要太多的process reward（过程奖励），或者value function（价值函数），甚至它们在训练过程中还有一些副作用。</p> <p>我们发现，你可能直接用端到端的reward（奖励信号），就能把训练做得非常好。这在早期还不是非常明确。这个过程中，我们积累了一些强化学习的基建，还有一些算法的know-how（诀窍）。</p> <p>K2的重点有几个：一是我们希望它是一个非常好的基础模型（Base Model）。如果你想有更好的Base Model，就要去看现在整个领域预训练的瓶颈在哪。</p> <p>我们发现高质量数据的增长确实很缓慢，多模态数据又无法很好提升文本本身的“智商”，你可以认为高质量数据接近一个常数。这种情况下，我们希望能最大化地使用每一份数据，就是所谓的token efficiency（token效率）。</p> <p>你希望在吃下一样多数据的情况下，脑子能长得更多，你能得到更多智能。</p> <p>这里和之前的思路不太一样。你现在假设在训练系统做很多性能优化，让训练更快，这当然有价值。但训得更快本身，并不能提升智能上限，因为token数量还是那么多。你训得更快，只是更短时间完成训练，但模型效果不一定变好，这是训练效率或compute efficiency（计算效率）上的优化。</p> <p>之前有人做过这方面，我们现在更希望提升token efficiency，把一份数据当成几份用。</p> <p>我们很关注，比如Muon优化器。它很有意思，对token efficiency提升很大。像Adam优化器用了10年，大部分的模型训练都会用Adam，但它的token efficiency并不够好。</p> <p>Muon优化器不是把每个元素独立考虑，而是把一个矩阵的参数整体考虑它们之间的dependency（依赖关系）。通过这种方式，获得更好的学习效率——你学同样一份数据，能学到更多智能。</p> <p>我们早期的实验，如果在compute optimal（计算最优）的情况下，基本会有两倍提升。也就是，你学一份数据，相当于用Adam学两份数据。</p> <p>假设你有30T的高质量token，等价于你现在有60T的高质量token。</p> <p>张小珺：但它实际还是那么多数据啊。</p> <p>杨植麟：它学了之后，脑子会长得更快。因为学习效率更高，优化器更好，吸收得更快。</p> <p>你喂它一样多的数据，它吸收得更好，压缩率会涨得更快，loss（损失）会降得更快。</p> <p>张小珺：Muon优化器是你们原创吗？</p> <p>杨植麟：Muon优化器是Keller Jordan（一位计算机科学家和机器学习工程师，于2024年12月加入OpenAI）提出的，我们在他的基础上做了很多优化，使它能适配并训练非常大规模的语言模型。</p> <p>我们之前有一个Moonlight的工作，让它能够第一次在一定规模的语言模型上训练。后来在进一步规模化的过程中，发现了很多新的坑，比如max logit（最大logit值，观察训练是否正常的一个指标）可能会出现爆炸的问题。</p> <p>这个问题在小规模实验中很难发现，但在大规模训练时会遇到。于是我们提出了一些新的方法，比如clipping（截断）技术，让它在非常大规模的情况下，仍然能很好地训练。</p> <p>这非常重要——因为token数量有限，你希望每一份token能产生更大价值。</p> <p>张小珺：我读了你们的技术报告，你们尝试已有模型改写现有数据，生成新的语料，具体的改写策略是什么样的？——这个在报告里没有提。</p> <p>杨植麟：我们会对数据做很多Rephrase（改写）操作。比如你有30T token，但其中高质量数据更少，可能只有几十b或者几百b级别（百亿到千亿级参数量）。你希望这些高质量数据能被很好地利用，我们对这些数据做了一些改写，让它们更好地被模型吸收，并且有更好的泛化能力。</p> <p>主要思想是，如果你同一份数据学很多次，它可能泛化不一定那么好，有一些过拟合问题。我们希望通过改写，让它有一定程度的泛化。</p> <p>具体改写方式有非常多种，我们找到一种在实验里效果比较好的。</p> <p>张小珺：哪种？</p> <p>杨植麟：这个空间也很大，有非常多的研究机会。</p> <p>张小珺：你怎么看待一种观点——“改写和扩充其实没用，能够写出来知识，说明知识本身就在里面，没有新知识，除非改写的时候用到其他方法。”</p> <p>杨植麟：这是一个很好的问题，确实跟改写方式有关系。理论上，还是看你有没有新的熵的输入。它对改写方式有一些要求。但我们现在也不一定用了最好的改写方式，有很多探索的空间。</p> <p>回到刚讲的点，K2这个模型，一方面是希望它成为一个好的Base Model。我们很希望提升它的token efficiency，这些是我们对应的设计，包括通过更大的稀疏度去加更多的参数。</p> <p>那它的token efficiency也会更高，因为你参数多了之后，虽然学一样多的数据，但你会吸收得更好。反正通过实验可以验证，确实有更好的token efficiency。</p> <p>第二是我们希望它有好的Agentic能力。你通过各种强化学习，或者对工具和环境的模拟，让它能有比较好的泛化性。</p> <p>对于一个Agentic模型来讲，现在最大挑战是在模型的泛化上。</p> <p>因为现在的RL技术，局限性在于，不管是训练任务还是评价指标，很多时候都是单点。比如你就训SWE-bench同分布的数据，它就提升SWE-bench，是很确定的东西。但是你的指标提升上去之后，并不意味着模型的泛化会变得更好。</p> <p><strong>SWE-bench</strong>（ICLR 2024）：一个用于评估大模型在真实软件工程任务中表现的基准测试集。</p> <p>我们也在尝试去解决一部分泛化问题。不希望过拟合到某一些工具，或者过拟合到某一些环境，或者过拟合到某一些具体任务上。这些任务可能是很好的观测，但我们不希望过拟合它。</p> <p>这个问题在Agent训练更严重。相比于对话模型，Agent的泛化是一个更大挑战。</p> <p>张小珺：Agentic能力现在更多在Post-Train阶段训练，为什么不在Pre-Train阶段去训？</p> <p>杨植麟：这个也是接下来我们想探索的东西。</p> <p>张小珺：这有可能提高泛化性吗？</p> <p>杨植麟：取决于你的做法，比如数据分布是不是足够广泛，有没有很好的方法评估。</p> <p>现在整体的评估，是阻碍Agent模型变得更泛化的重要瓶颈。你会慢慢观察到，现在Agent能用的Benchmark（基准测试）不是非常多。你在那些Benchmark上观察到一个分数，很多时候它并不是对这个能力的反映，比较片面。这是大家要去想办法解决的问题。</p> <p>有一种潜在思路，我们需要用更AI native（原生人工智能）的方式去训练AI。我们希望让模型参与到更多训练过程。比如，如果你的AI能做很好的alignment research（对齐研究），它理论上会有更好的泛化，不仅只是在优化一些单点任务。</p> <p>今天Agent还不像对话有这么好的泛化性——接下来雪山上几百个台阶，有可能是这个。</p> <p>张小珺：听起来K1.5是跟着OpenAI跑，K2是在抢跑。</p> <p>杨植麟：我们借鉴了很多技术上的方向，但也希望有一些自己的创新。</p> <p>至少在公开资料，我们是第一个使用非Adam的，或者基于矩阵正交化的方式，去用新的优化器，在这么大规模的模型上去训练。这是一个创新点。</p> <p>我们在一些Agent数据的做法，至少在公开可查资料里也是比较早去做的。</p> <p>很有意思的是，当你在雪山上越往上爬，你会发现空间是在变大。因为现在你完成同一个任务用到的token在变多，问题复杂度变得更复杂。</p> <p>就像刚刚讲的：问题不可避免，但问题总可以被解决。</p> <p>这些不可避免的问题，看起来会比之前更多，但你的研究空间也会随之更广阔。</p> <p><em>（相比2024年1月访谈时，月之暗面已搬到一个更明亮的办公室。在北京知春路的京东科技大厦13层，那架白色钢琴还在。）</em></p> <h3 id="05-muon你去训的时候它会炸">05 Muon你去训的时候，它会炸</h3> <p>张小珺：我们具体说到K2这个项目，它是怎么立项的？中间筹备了多长时间？</p> <p>杨植麟：筹备是比较长时间的，涉及很多技术从去年开始研究。</p> <p>像Muon的技术，研究需要比较长周期。你一开始做早期实验，发现这个想法有潜力。我们会有一些小的实验验证这个idea（想法）潜力有多大。</p> <p>有了想法之后，到最后你能把它放到一个万亿模型去训练，要通过不同的scaling（规模化）实验去验证它的有效性。有些问题只有当你scale（扩展）到一定规模之后才会发现——所以周期比较长。</p> <p>当然，如果你只看这个模型训练，从按下训练按钮到训练结束，时间并没有那么长。但研发需要更前置做很多事，才能最后保证训练比较顺利。</p> <p>张小珺：做Agentic LLM（智能体大语言模型）这个bet是什么时间点？</p> <p>杨植麟：也要做很多积累，只是说，不同时间点做法不太一样。</p> <p>一开始你不一定端到端去做，但会积累一些环境和数据，到了后面更端到端去做强化学习。你中间需要很多基建和数据积累。很难说一两个月就能做得非常好。</p> <p>我整体觉得，大模型和相关技术很需要时间积累。还是“要做时间的朋友”吧。技术曲线还是有点陡峭，不是今天想做就能做出来。</p> <p>张小珺：所以什么时候立项的？为什么立这个项？</p> <p>杨植麟：一年前积累各种技术，但K2肯定是最近几个月，我们决定要去训一个这样的模型，然后把哪些技术用上，大概是这样一个决策。但不是我今天想训这个模型，从0去搞。</p> <p>我们一直训练下一代模型嘛。无非是一个决策：我下一代模型要加入哪些技术？你期待它是什么样的模型？就像现在也在考虑，K2之后下一代模型应该长什么样？——这是持续要思考和决策的。</p> <p>每次会看，现在工具箱又多了很多新东西，把哪些拿出来用？是这么一个过程。</p> <p>张小珺：你们做研究和做训练的团队是分开的吗？如果一年前已经开始研究这些技术，做正式训练，是一个团队在做整件事吗？</p> <p>杨植麟：是一个团队，这些东西很难分开。你在实际训练中会遇到问题，如果之前不了解，没办法解决它。</p> <p>张小珺：K2研发过程中遇到什么挑战没有？</p> <p>杨植麟：Muon你去训的时候，它就会炸。</p> <p>我们有画一些图在paper里，你的max logit会涨得非常高，涨到几百甚至更高。我们认为这个东西对训练稳定性有影响，你可能训久了，它很多所谓的内科指标（internal metrics）不正常，对模型上限有害。</p> <p>我们等于又回过头去revisit（重新审视），修复它。因为这个东西是你在小规模实验上没办法预测的，小规模上不会有爆炸的问题。</p> <p>其他基本还好，都在小规模上做了很多实验，是可迁移的，问题不是很大。唯一有这个问题是你小规模上验证不了，需要在scale（扩展）过程中再临时解决。</p> <p>张小珺：最近K2火了，你的心情有起伏变化吗？</p> <p>杨植麟：也没有，还好——这是一个漫长的旅程。</p> <p>要持续去做下一代模型，还是回到那两句刻在石头上的话：会产生新的问题，然后就去解决它。这也是最有意思的部分。</p> <p>张小珺：听说，你在内部群里形容K2意味着乔戈里峰。</p> <p>杨植麟：K2本来就是世界上最难攀登的山峰之一，名字有点重合。</p> <p>它不是终点，因为它不是最高的山峰，但可能是最难的。这是因为现在有很多范式转变，从对话到Agent，你的Base Model规模进一步变大，本身存在难度。</p> <p><strong>乔戈里峰（K2）</strong>：世界第二高峰，海拔8611米，仅次于珠穆朗玛峰（海拔8848米），位于中国与巴基斯坦交界的喀喇昆仑山脉中。它以极其险峻著称，攀登难度远超珠穆朗玛峰，被登山界誉为”山中之王”或”杀手峰”，因其恶劣的天气条件、复杂的地形和高死亡率而闻名。乔戈里峰多次成为登山者挑战极限的象征，也代表着极端高山探险的终极考验。 张小珺：K2发布的结果超出你预期了吗？</p> <p>杨植麟：差不多，模型训得怎么样，在过程中就已经知道了，没什么惊喜或意外。</p> <p>张小珺：你在K2训练中收获的最重要几个know-how（技术诀窍）是什么？</p> <p>杨植麟：我们都写在paper（论文）里写了。</p> <p>我们都很open（开放），还是想更多跟社区分享嘛。</p> <h3 id="06-当从缸中之脑变成跟世界交互的系统">06 当从“缸中之脑”变成跟世界交互的系统</h3> <p>张小珺：因为K2是一个Agentic大语言模型，你会怎么定义Agent（智能体）并对Agent进行分类？</p> <p>杨植麟：它可能是一个从“缸中之脑”变成可以跟世界交互，因为所谓Agent最重要的特征，就是它可以多轮地使用工具。</p> <p>有两个关键点：一个是多轮，一个是工具。</p> <p>多轮就是你能做很多次，是test time scaling（测试时扩展）的一种方式；工具则是连接这个“脑”跟外部世界的方式。</p> <p>比如，你用搜索引擎，就可以把模型跟整个互联网连接起来；你可以写代码，就能让“脑”跟数字世界连接，因为数字世界几乎所有自动化都可以用代码描述，它能拥有这种自动化能力。</p> <p>这两个是我想象中Agent的特征，接下来会有越来越多的工具。当然，工具会呈现长尾分布。如果模型泛化得好，它不只是使用常见工具，能使用非常个性化的工具。</p> <p>比如，模型能访问公司内部数据库、个人文档，甚至访问定制的API，完成退票、下单等业务操作。它应该能泛化到没见过的工具上。我一直觉得Agent最缺的是泛化能力。</p> <p>如果泛化能力强，大家讨论的各种垂直Agent就没那么必要了。因为通用Agent泛化到长尾工具上，很多领域专有问题都能通过接入不同工具解决。只要给它加上定制数据库、定制API、定制文档接口，就能做一个非常垂直的Agent。它的普适性会强很多。</p> <p>多轮主要是实现test time scaling（测试时扩展），可以做复杂任务。不像对话模型一次输出一轮，这个可以做不同的事情——就像人一样——人每天的工作，你可以认为是，多轮使用工具的序列。你希望把人的序列拟合进去，但你又搜集不到这样的数字化数据，你就可以用强化学习来构造。</p> <p>它本质是在模拟人的行为——不过，你也不能简单说是模拟人，叫“模拟人的行为”不太准确，它其实是通用的。</p> <p>张小珺：什么叫不能简单说在模拟人，人也很通用。</p> <p>杨植麟：对，人是通用的，人是所谓的universal constructor（万能构造器）。</p> <p><strong>“万能构造器”（universal constructor）</strong>：指一种能够制造任何物体或系统的机器或装置。这个概念源自理论计算机科学和自动机理论，最早由数学家约翰·冯·诺依曼提出。万能构造器能够读取自身的”蓝图”或程序指令，然后根据这些信息复制自己，或者制造其他复杂结构，理论上可以构建任意复杂的系统。 但它主要目的不是去模拟人，主要目的是通用性，这才是设计的目的。</p> <p>它跟人的做法类似，只是一个恰巧的结果，并不是设计系统的目的。</p> <p>张小珺：这就好比设计飞机，是为了让它成为交通工具，目的并不是像鸟一样能飞。</p> <p>杨植麟：我们做Agent系统，更多是为了做一个通用的智能，是跟这个目标对齐；但它刚好跟人相似。</p> <p>张小珺：怎么提高Agent通用性？你们探索到什么方法没有？</p> <p>杨植麟：这是很难的问题。今天Agent的泛化有一个风险，可能会陷入某些Benchmark过拟合，但现在又缺少很好的Benchmark。这是接下来的挑战。</p> <p>不过可能有些解法，我还是觉得能用更多的AI去训练AI，可以一定程度缓解这个问题。</p> <p>张小珺：什么时候能做到用AI训练AI？现在的瓶颈是什么？</p> <p>杨植麟：现在部分已经做到，但你希望它做更多。现在很多还是依赖人的设计。</p> <p>张小珺：这样就到下一个阶段Innovator（L4、创新者）的阶段了。</p> <p>杨植麟：这很有意思，你要用一些Innovation方式去解决Agent问题。因为Agent泛化不够，你得用创新去解决——用L4的技术去解决L3的问题。</p> <p>所以L1到L5的定义可能真的不是线性的。没有好的Innovation，没有用AI去训练，或者用AI对齐AI的方式，Agent很难做到好的泛化。</p> <p>你人工定义一些 task（任务），只fit（拟合）那个 task，但在别的看不见的task表现就不好。只刷几个task分数，但用户在更多OOD（分布外，out-of-distribution）场景中体感没有那么好。</p> <p>现在这个领域面临的是，Benchmark不够用或失效，Agent泛化有问题的阶段。</p> <p>张小珺：为什么数学和代码是相对容易泛化的领域？</p> <p>杨植麟：其实也没有。如果做强化学习，现在也有类似问题。</p> <p>强化学习本身的泛化性，比做SFT（监督微调）要好，因为过程中有更多的on-policy（基于当前策略）sample（采样），模型从自身采样中学习，泛化看起来更好，而且有负梯度。这两个因素导致从证据来看，泛化表现更好。</p> <p>但泛化是有限的。比如说，你在某种类型的数学竞赛做到99分，别的数学问题可能提升5个点，但很难直接做到99分。如果不做对应的RL任务，就很难直接做到这样的泛化性。</p> <p>所以做数学题也有类似问题，是被分布所制约的——还是“种瓜得瓜，种豆得豆”。</p> <p>但是我们希望强化学习或后训练用更多AI ，让模型摆脱“种瓜得瓜”的情况。</p> <p>张小珺：有没有可能最终就是摆脱不了，大幅提升不了泛化性？</p> <p>杨植麟：还是回到刚刚说的，问题不可避免，但问题可以被解决。你每次都会往前推进——泛化会变得更好，它不一定有尽头，一直会有更好的泛化。</p> <p>张小珺：对于Agent来说，任务和环境非常重要。怎么定义好的任务？怎么定义好的环境？你在探索过程中有没有一些思考？</p> <p>杨植麟：一种方式是，我给定一个模型，然后设计一些环境，去逆向拟合这个模型。当然你也可以正向设计，假设你是一方的开发者，正向设计工具和环境，让模型在这些环境里提升能力。</p> <p>关键是让这个设计有更好的通用性。它能做很多任务，不应该为了某些特定任务专门设计工具和环境。当设计足够通用时，模型能在这其中学习，而不是反过来拟合模型。这可能是更好的做法。</p> <p>张小珺：我注意到一点：一般大家认为在任务设计上，倾向于设计一个足够有挑战的任务，这样会催生一些更本质的新方法；但K2设计的是一些中等难度任务。这是出于什么考虑？这会影响通用性吗？</p> <p>杨植麟：它也是一个爬山过程。你不能一上来就让模型去证明一个还没有人证明过的数学问题，样本效率（sample efficiency）会非常低。</p> <p>现在比较好的方法是，强化学习如果搭配好的采样策略，本质是隐式的课程学习（curriculum learning）机制，希望模型从合适的难度开始学习，逐步提升难度，而不是一开始就学非常难的任务。否则采样效率低，基本学不到什么东西，算力可能都会被浪费掉。</p> <p>但挑战在于，今天的很多任务还是基于人类存量数据或人工设计的任务，AI native的部分还比较少，会带来泛化性问题。</p> <p>张小珺：在你眼中，Coding Agent（编程智能体）和通用Agent（通用智能体）是什么关系？</p> <p>杨植麟：Coding Agent是任务的一个子集，但可能是很重要的一个子集。</p> <p>最后还是希望不仅仅做Coding。包括现在我们训练的模型，也不是只让它做Coding，因为它本身有一些局限性。</p> <p>张小珺：可以这样说？——Coding相当于人类的手。</p> <p>相对来说，Coding对Agent是比较容易的任务，是吗？</p> <p>杨植麟：它比较好验证，所以比较好学习。它也会面临类似挑战——泛化性问题，即便是Coding Agent也会遇到一样的挑战。</p> <p>Coding Agent是很重要的一个子集在于，它代表了数字世界的自动化。现在很多Agent工具集合是固定的，如果你想创建一个新的工具，本质是写一段或者一大段代码实现。或者如果你想做更好的上下文管理（Context Engineering），背后也对应一个工具，这个工具可能也用代码实现。代码在这里面有独特的位置和作用。</p> <p>但并不是做了Coding Agent就足够。因为很多非程序员也会用Claude Code完成任务，比如律师、产品经理、设计师，他们用Claude Code是因为模型在一定程度上有泛化能力，不仅仅是写代码。</p> <p>张小珺：你们想做的是通用Agent，而不是Coding模型？</p> <p>杨植麟：我们还是希望做通用的模型。</p> <p>张小珺：从写代码到操纵整个数字世界，Agent目前还缺乏哪些能力？</p> <p>杨植麟：现在这些高频工具使用还不够好，能力上有很大空间。这也说明现在缺少更好的Benchmark观测。SWE-bench现在可能马上会饱和，很多Benchmark不够好，不够真实反映实际用户体验。</p> <p>高频工具本身会有空间。长尾工具，在一些你没有见过、完全OOD（Out-of-Distribution，分布外）的情况下，怎么有更好的泛化？也是很重要、需要解决的问题。</p> <p>张小珺：对于Agent ，Long Context（长上下文）和Long-Term Memory（长期记忆）重要吗？</p> <p>杨植麟：Long Context也很重要。因为现在很多任务，128K或256K这种Context完全解决不了，你需要百万级甚至更多。</p> <p>而挑战在于，你不仅要能处理这么长的Context，还要保证“脑子好用”，智商要非常高。</p> <p>这对于模型的训练，挑战是很大的。一方面你希望压缩率足够高，模型要足够大；另一方面你希望它比较长。这两者之间天然存在一些冲突，所以需要更好的架构。</p> <p>但是有些架构你会发现，它在更长Context下效果会有提升，但在短Context下不一定会有提升，甚至会有下降，这就涉及架构的平衡问题。</p> <p>不过这些问题接下来可以逐步被解决，我觉得有一些解法。</p> <p>此外，当前的RL训练方式还有很大提升空间。比如在训练复杂的多智能体系统（Multi-Agent System）时，如果只使用端到端的reward，很可能不够。中间的reward如何产生？是否能摆脱一些人工设计？</p> <p>这也是非常值得探索的方向。</p> <h2 id="第三章-既简单又复杂的系统-1">第三章 既简单又复杂的系统</h2> <h3 id="07-开源-vs-闭源">07 开源 vs 闭源</h3> <p>张小珺：我回看我们去年的对话，有一个问题非常想问你。 你去年说，开源会落后于闭源。因为开源的方式跟以前不一样，以前所有人都可以贡献到开源，而现在的大模型开源，本质是中心化的，社区贡献没有经过算力验证。相比之下，闭源阵营会人才和资本聚集，是对市场资源的整合。</p> <p>你当时说：“领先者不会开源，只有落后者才会这么做。”</p> <p>但今天你们开源了。</p> <p>杨植麟：因为我们现在，在全球范围内还没有完全领先（笑）。</p> <p>有些判断在大方向上是成立的：当你的模型发布，社区可以贡献一些东西。比如，你在推理侧可以做很多事，你可以让模型被更多人免费使用。</p> <p>但如果要贡献到模型本身、让模型变得更强，目前只有原厂能做。</p> <p>当然，如果你看Base Model，确实如此；但如果基于一个开源模型去做大量后训练（Post-Training），尤其是Agentic的Post-Training，可能催生新的机会。</p> <p>假设你现在非常想做一个法律相关Agent，你是创业公司，那你完全可以基于K2，在你的特定工具集合之下训练一个Specialized Agent（专用智能体），它可以在你关注的场景下表现得非常好。这种机会存在。</p> <p>更多是赋能下游应用，而不是反哺基础模型的提升。当然这个问题要动态观察。</p> <p>张小珺：你们会长期选择开源吗？</p> <p>杨植麟：这是我们希望长期做的，但不一定只做开源。我们希望跟社区分享技术know-how，这是加速技术提升的重要点。</p> <p>大家可以不完全是竞争，也可以有合作，甚至所有开源公司形成一个生态，更好地推动技术发展——雪山可以爬得更好，race to the top（冲向顶峰）。</p> <p>但也不一定所有都开源。比如跟某些公司合作，不一定都开出来。</p> <p>张小珺：总的来说，开源是一个技术体系的信仰，还是一个市场博弈的策略？</p> <p>杨植麟：客观说都有，而且都有好处。但最终我们希望通过这个让技术更安全、更快达到更好的水平。</p> <p>张小珺：开闭源的生态会怎么演进？在你的认知中，最终开源和闭源全球会剩下几家？</p> <p>杨植麟：不会很多，但几家还是会有的。你如果看过去两年，这个趋势比较明确——市场逐渐更集中、更收敛、更聚焦。可能一开始有几百个，到几十个，到几个。</p> <p>几个，或许是最终稳定数量，现在看是大概率的事。</p> <p>张小珺：你们属于开源那一边还是闭源这一边？</p> <p>杨植麟：这要动态去观察，我们希望长期分享更多技术。</p> <p>张小珺：为什么中国公司大部分都开源了？</p> <p>杨植麟：客观说，有市场博弈的因素。但这对社区是好事。</p> <h3 id="08-多模态不损伤脑子已经很好了">08 多模态不损伤“脑子”已经很好了</h3> <p>张小珺：你怎么看AI时代的产品？做AI产品跟做移动互联网产品有什么不一样？——你以前很喜欢说“模型即产品”。</p> <p>杨植麟：我只能说AI产品，移动互联网产品没做过。</p> <p>（模型即产品）现在没有变化。你做一个Agent产品，需要把模型跟工具和Context结合起来。但你会发现，训练模型的时候，基本得把这一整套系统搭好，才能训练这个模型。</p> <p>模型训练完成，产品也基本完成了。在这个基础上做一些交互上的改进当然有价值，但那是锦上添花的一步。</p> <p>你的模型性能在训练中已经打磨好，跟工具和环境有非常好的适配——也就是，产品是在训练过程中完成的。</p> <p>张小珺：去年，你提到现在的开发方式已经演变成——你要做一个巨大的系统，就像20世纪初Google做搜索引擎系统。你今天对于AI时代的巨大系统，有更多的想象吗？</p> <p>杨植麟：现在的系统复杂性在于，你想让这个模型变得通用。一方面它变简单了，另一方面它变复杂了。</p> <p>简单在于，你只要把所有东西放在同一个模型，不需要维护那么多模型，也不需要搞一堆的routing策略（路由策略）。从概念上，或从工程实现上，它变简单了。</p> <p>但同时，它也变得复杂。如果你希望它通用，就希望这个模型在各种场景下都能工作。比如你做Agent模型，你不希望它只在你的工具集工作，而是希望别人用这个模型时，即便是别的工具集，甚至你没见过的工具，或者定义和实现方式不同的工具，它也能工作。这个要求很高。</p> <p>像现在Agent里面，可能会有几种不同类型的任务，不管是Coding Agent（代码智能体）、Search Agent（搜索智能体），还是其他Agents，你要把它放在同一个通用模型，就可能有打架的问题。也许工具定义不一样，或者数据pattern（模式）不一样。</p> <p>就是，你把它做成一个通用模型的过程，有很多技术挑战。</p> <p>但如果你不做成通用模型，它的泛化性又没那么好，只能做一件事。特别是现在的Agent，它需要很多步才能完成任务。即便是程序员，也不仅仅是写代码；就算写代码，也不仅只做SWE-bench。你要做出很通用的、真正可用的东西，随着步数变多，对通用性要求会更高。</p> <p>它的系统复杂性，体现在训练模型的过程中，要让这个模型足够通用，而不是只拟合到某些单点能力上。你如果只拟合单点能力，可能Benchmark分数很好看，但通用性不够——这是现在这个系统，我能观察到的比较大的挑战。</p> <p>有一个例子是，如果你想往模型里加多模态能力，你就需要让这个多模态能力不要损伤它的“脑子”。</p> <p>张小珺：多模态只能做到不损伤？</p> <p>杨植麟：对，能不损伤已经很好了。</p> <p>你希望在多模态模式下，跟文本模式下，共用一个“脑子”；你希望它在多模态的模式下，也能把文本那部分的智商激发出来，而不是进入另外一部分参数，那它可能完全丢掉了原来文本学习的部分。</p> <p>当你做一个通用模型，会面临这样的挑战。当你有各种模态、各种任务类型，还有Agent、Reasoning、Chat这些，要全部融合到一起，是存在挑战的。</p> <p>而且现在不仅是做SFT，还要做RL，挑战进一步加重。</p> <p>通用的Pre-Training比较好做，你只要把所有文本放在一起，它基本不会有太多问题。但越到Post-Train后期，越到RL，这个问题会更加严重——这是它的系统复杂性。</p> <h3 id="09-当你通过新的交互收集的信号噪声减少">09 当你通过新的交互，收集的信号噪声减少</h3> <p>张小珺：你看，搜索引擎系统是构建在PC互联网之上，推荐引擎系统是构建在手机，也就是移动互联网之上。在AI时代，新的超级节点会出现在哪里？</p> <p>杨植麟：它会跑在很多数据中心，Jensen（英伟达创始人兼CEO黄仁勋）经常说的AI factory（人工智能工厂）。但还是会有更多终端，终端有些复用现在的，有些可能是新的。</p> <p>张小珺：会诞生新的交互方式吗？</p> <p>杨植麟：肯定会。两年前看，Chat是一种新的交互方式。现在Agent，有很多新的交互方式，比如你让它异步执行一个任务，可以看中间结果。</p> <p>你看Coding，一开始是Copilot，之后有Cursor，再之后有Claude Code——每一代的交互都发生了变化，交互是随着模型的变化而变化。</p> <p>当你有新一代模型，能力提升很多，就会发现交互可以改了。你不再需要一个一个点accept（接受）修改，而是多步执行一个Agentic Coding任务。</p> <p>当然，今天Claude Code的交互也不是终极形态，因为模型还会继续提升，能力提升之后，交互会持续变化。比如你有一个Multi-Agent System，交互方式会怎么样？可能随着能力边界不断变化。</p> <p>张小珺：今天的Scaling Law（扩展定律）放缓了吗？</p> <p>杨植麟：Scaling Law遇到数据墙了，这是客观事实。你要突破数据墙，就需要提高token efficiency。这也是我们为什么做提高token efficiency的事情，数据墙是存在的，同时你要scale（扩展）更多算力到各种RL任务上。</p> <p>但我们现在观察，模型变好的速度并没有减少，甚至在加速。</p> <p>张小珺：为什么AI产品发展到今天，还没有形成数据飞轮？</p> <p>杨植麟：因为基于算力的scaling太强大了。</p> <p>比如你先去scale Pre-Training（预训练），再去scale RL（强化学习），而RL的scaling效率又比Pre-Training高很多。因为它是on-policy（基于当前策略）且带负梯度的训练，所以scaling效率更高。</p> <p>当你有很高scaling效率的时候，你直接去scale compute（算力）、scale FLOPs（浮点运算次数）带来的提升非常大，相比之下其他手段带来的提升很小。这是一方面。</p> <p>另一方面，所谓数据飞轮很依赖外部环境的feedback（反馈）。这个feedback，我们不希望它有很多噪声。但现在还没有把这个问题解决得非常好。</p> <p>大模型的学习对噪声比较敏感，它跟传统的，比如推荐系统不太一样。推荐系统可能没那么怕噪声，但大模型是敏感的。</p> <p>现在看起来，基于FLOPs的scaling是更有效路径。但这个平衡什么时候会发生变化？——也有可能你通过新的交互，让你收集到的信号的噪声能够减少。</p> <p>张小珺：这就需要创造一种新的交互范式。</p> <p>杨植麟：对。但这个交互又要适配模型能力的发展。你的交互不能超越模型能力，应该是在当前模型能力范围内，设计一个好的交互。</p> <p>这是值得尝试的。只是在今天看，去scale FLOPs的维度，或者提升学习效率，是一个确定性更高、更有效的方法。</p> <p>张小珺：如果按闫俊杰（MiniMax创始人兼CEO）的说法，用户数据无法提高模型的智能，那今天是不是没必要做To C（面向消费者）产品，就一门心思提升智能就好了。</p> <p>杨植麟：这要看怎么理解。你可能没办法直接使用用户反馈去训练；但有一定用户量的好处是，你知道需求分布是什么样的，知道哪些地方用户用得好或不好，可以把这些东西抽象成evaluation（评估），再去优化模型。如果模型完全没人用，你不知道该往哪个方向优化。</p> <p>另外也要看用户的商业价值。现在又到了一个新的分水岭：用户是有可能产生商业价值的。你看OpenAI，C端用户产生了很大商业价值，占了它营收比较大比例。</p> <p>特别是现在很多Agent产品，能端到端产生价值，所以也要看你是什么用户。如果只是闲聊、查天气，商业价值没那么大。但如果是Agent专业用户，本身有很好的生产力价值。</p> <p>张小珺：最近一年，你对C端产品有哪些新的思考？</p> <p>杨植麟：更多还是想模型怎么做，因为模型训好了，产品基本做得差不多了。我们还是会沿着这个方式一直做。</p> <p>张小珺：有人说，Kimi是从最初想做“中国的OpenAI”——当然你以前并不认同这一说法——转而想做“中国的Anthropic”。你们内部有这样的定位转换吗？</p> <p>杨植麟：很难用这样的方式去定义。中美的语境、土壤不一样，今天更多是从全球视角去思考问题。“做中国的某某”，不太成立。</p> <p>其实简单一点，我们希望继续爬山，做时间的朋友，和社区一起加速技术的推进。</p> <h3 id="10-long-context架构会影响智商">10 Long Context架构会影响“智商”</h3> <p>张小珺：作为Founder，你现在生活节奏是什么样的？</p> <p>杨植麟：可能睡得比较晚，哈哈，每天不一样。</p> <p>但也还好，花很多时间看怎么把模型训得更好。</p> <p>张小珺：你的时间主要投入在模型训练上？</p> <p>杨植麟：是吧，但模型训练是个抽象概念，重要的是技术战略，这是公司战略里最关键的一部分——下一步哪些要做，哪些不要做，因为技术空间很大，总要选一些方向重点投入。</p> <p>我们在很多方向上的bet比较早，且是有效的。我们很早去做long CoT的RL，反应比较快；去做优化器；去做更大规模的Pre-Training；去做第一个Open的Agentic模型，这些都是技术关键决策。这些决策能决定公司五六成走向。</p> <p>但你要做很好的决策，需要很多证据，还得做很多实验。你得非常了解实验的具体结果，不能拍脑袋，得知道更多信息。</p> <p>张小珺：这些决策中令你最纠结的是哪个？</p> <p>杨植麟：也还好。关键是一个收集数据的过程。做实验，看实验是不是扎实。加上你对技术的理解去判断。很多时候只要数据足够充分，判断比较显然。</p> <p>接下来——至少现在，K2的性能潜力还没完全被压榨出来。我们之前放的更接近是一个Base Model。我们可以加更多Post-Training阶段的FLOPs（浮点运算量），上限应该比现在高很多。</p> <p>我们还会做下一代模型，但具体怎么做，我们通过实验来决策。</p> <p>张小珺：也会加多模态吧？</p> <p>杨植麟：多模态是比较确定的。</p> <p>但多模态的能力本身要做好不容易。里面有很多工作：怎么让它去借鉴文本的脑子，而不是自己单开一个脑子。比如你MoE（专家混合，Mixture of Experts）里假设有20个expert（专家），专门在做多模态，你可能不希望这种情况出现——这样，你可能学出来的多模态是个“傻的多模态”。</p> <p>我们希望它是个“聪明的多模态”。</p> <p>张小珺：接下来还会有哪些重要的技术里程碑？</p> <p>杨植麟：Agent的泛化性是最重要的。</p> <p>Long Context的支持，我们会继续研究。特别是在智商很高的情况下，还能有更长的Context，也是很重要的问题。</p> <p>现在很多Long Context架构还是会影响“智商”。</p> <p>张小珺：为什么Long Context架构会影响“智商”？</p> <p>杨植麟：纯粹的Linear Attention（线性注意力机制）可能就是会影响智商，因为这个架构会有一些bias（偏差），这些bias在一些场景下效果没有那么好。但一定程度上是可以被解决的。</p> <p>张小珺：你怎么看张祥雨（阶跃星辰首席科学家）说的next token prediction（下一个token预测）的本质缺陷？</p> <p>他的意思是，随着模型规模扩大，对话能力、知识量和情商都在变强，但推理能力尤其是数据表现是先上升后平缓，再扩大反而下降。用更大的模型做数学题，容易跳步、不老实。这是next token prediction的本质缺陷。</p> <p>杨植麟：所以要搭配强化学习的scaling（扩展）。如果今天不做强化学习，模型很难说很聪明。像数学题，不一定做得非常好。</p> <p>但更大的base（基础模型），强化学习的上限会更高。因为知识更多，本质是激活一个推理的范式，让它能把知识解锁出来。它的上限更高，但需要搭配RL去激活。</p> <p>张小珺：你怎么看待世界模型？——有人说，做世界模型是造世界，做Agent是造人。</p> <p>杨植麟：用AI训练AI有点这个意思。你有一个很好的世界模型，就能模拟这些东西，就是用AI训练AI的方式。</p> <p>它可能是通往更好泛化的一种路径。</p> <h3 id="11-边界与现实">11 边界与现实</h3> <p>张小珺：现在，我们来讨论一些现实问题。</p> <p>基座模型公司和做Agent产品的应用公司，长期看边界在哪？</p> <p>杨植麟：我没有明确答案。只能说今天，“一方产品”有个好处，可以垂直整合，把模型放在里面训练，模型和工具融为一体，不是分开做再逆向工程。</p> <p>但因为Agent领域广阔，“一方产品”不一定能做得过来。如果能找到一些空间，比如工具的实现需要非常多领域的know-how，或者evaluation是“一方产品”考虑不过来的东西，是有机会的。</p> <p>因为有像K2这样的开源模型，大家可以在上面微调（fine-tune），更容易产生Specialize Agent（专用智能体）、垂直Agent的可能性。</p> <p>张小珺：那要看通用模型通用到什么程度了。</p> <p>杨植麟：不管多通用，总还是有一些工具你要做。你有可能不一定做模型，而是把工具做得非常好。</p> <p>这个工具如果做得太通用，就会和“一方产品”overlap（重叠）比较大——这种情况下，垂直整合优势更大。</p> <p>但如果工具是专门针对某个场景，甚至别人做不了。比如你掌握了一些线下服务入口，你的下订单或者成交的工具别人做不出来，你可能产生独特价值。</p> <p>当然也有另外一种可能性，当“一方产品”或通用Agent的流量和商业模式足够成熟，很多专有的、本来垄断的工具也愿意接入，因为整体商业化效率会更高。但商业化效率的提升需要时间。在这段时间窗口内，专有Agent也会有空间。</p> <p>最终，通用之所以有效，是因为整体商业化效率更高。今天，包括很多内容平台，最终有可能你把内容接到通用Agent，商业化效率会比今天更高——但可能要花很长时间。</p> <p>张小珺：像Manus这种公司，会是你的潜在客户还是竞争对手？</p> <p>杨植麟：还很早期，很难判断到底是什么样，产品本身也会演进。</p> <p>短期，更多是合作大于竞争。今天Cursor、Perplexity、Genspark也都可以看到K2的身影。</p> <p>但未来会演进，有点像Claude和Cursor的关系。Cursor也可能需要动态调整产品策略。它可能需要一方面具备一定的模型能力，因为技术曲线还很陡峭；另一方面，它能不能有一些别人做不到的工具或环境？</p> <p>现在没法直接回答。只能说目前，一方的整合优势还存在。</p> <p>张小珺：你今天怎么思考商业模式？API是好生意吗？</p> <p>杨植麟：目前明确的商业模式：一是API服务，二是“一方产品”。我们都会做一些尝试。今天最主要优先级是把模型做得更好，这依然是首要目标。</p> <p>在模型提升过程中，如果在某些方面领先，确实有商业化空间。今天市场规模增长非常快，头部公司有几十亿甚至上百亿美元ARR（年度经常性收入），每一两个季度可能实现两三倍增长。我们会动态观察，做出相应尝试。</p> <p>张小珺：一位你们的用户说，他很喜欢Kimi，但也担心Kimi赚不到钱，你们能赚钱吗？</p> <p>杨植麟：还是先投资。能不能赚钱，取决于模型效果怎么样。</p> <p>我们也愿意服务用户的最后一公里体验，为用户交付高价值问题的deliverable（可交付成果）。</p> <p>全球百亿美金且高速增长的AI市场里，专注把技术做好，其他反而更有确定性。</p> <h2 id="第四章-在自己的故事里面-1">第四章 在自己的故事里面</h2> <h3 id="12-用rl的方式去管理而不是用sft">12 用RL的方式去管理，而不是用SFT</h3> <p>张小珺：过去一年，你对组织有新的思考没有？</p> <p>杨植麟：好问题。最近一直在想一个事，有几个东西是联系在一起。</p> <p>你看科研，或者创造新知识的过程，很像一个强化学习（RL）的过程。</p> <p>之前有种经验主义理论说，人是通过经验获取知识。但后来很多观点认为不是这样。人类在地球上存在非常多年，直到几百年前没有任何人说“地球是个球”。你一直在经验里，但你不知道事实是什么样的——经验并不能直接给你知识。而是，你提出这个猜想，说我认为“这个球是圆的”，我想各种方法验证它。</p> <p>包括训练神经网络，你可能观察到一些内科指标不太对，你提出“为什么它会这样”的猜想，设计实验去验证——这个过程和强化学习非常像。</p> <p>同时，你发现，管理一个团队，也是这样的方法。这是Tim（周昕宇，月之暗面联合创始人）天天跟我讲的——要用RL的方式去管理，而不是用SFT。</p> <p>当然现在，你做RL的时候也希望加一点SFT，因为SFT是很好的先验，防止模型“飞太远”。但你又要管住自己的手，你不能SFT太多。SFT太多，团队成员会失去主观能动性，没办法创新。这点我现在也在实践，看起来有一些效果。</p> <p>核心是掌握SFT和RL的平衡。</p> <p>SFT是你告诉他“这个事情该这样、这样做”； RL是你给他一个奖励，如果做成这样是好的，更多反映在目标上。</p> <p>可能要以RL为主，用一部分SFT通过先验去控制，或者防止它遗忘重要的东西。</p> <p>RL是一种很本质的东西。在科研、模型训练、组织管理上，是相通的。</p> <p>但这也带来一个挑战：在RL过程中，怎么定义reward（奖励）。你简单设定一个目标，比如“把所有Benchmark拉高”，大家会不择手段去overfit指标。但分数高了，模型本身并没有真的更好。</p> <p>所以，奖励的定义就很重要，需要你很理解具体细节是怎么运作的。不然会出现reward hacking（奖励机制被滥用）。</p> <p>张小珺：所以在组织内部，怎么定义reward？</p> <p>杨植麟：你建立更多观测指标，尽可能不要过拟合，是一定程度有效的。这样你才有更好的泛化，不会被hack（利用漏洞）。</p> <p>用RL管理团队最大问题是，你容易被hack。大家看起来各种结果很好，但实际并没有达到你最终想要的——这是风险。用SFT管理团队的风险，是大家失去创造力。最后这几个东西要有一定程度的balance（平衡）。</p> <p>当然我也在学习，今天不是做到很完美。</p> <h3 id="13-ai是人类文明的放大器">13 AI是人类文明的放大器</h3> <p>张小珺：过去一年Kimi在波峰波谷来回震荡，身处其中，你的心态是什么样的？需要平衡自己的心态吗？</p> <p>杨植麟：心态就是，做时间的朋友吧。</p> <p>张小珺：真实的人的心态不会这么简单的。</p> <p>杨植麟：像你说的，会有高点和低点。可能很重要的是，还是喜欢做这个事情，想把它做好。所以你也不用想别的，就想怎么把它做好。好像也比较简单，没有什么特别复杂的。</p> <p>很多复杂性都是人为强行加上去的，实际并没有那么复杂。</p> <p>张小珺：你对人性有更多的理解没有？</p> <p>杨植麟：这也还需要时间的打磨，不敢说那么深入。</p> <p>只能说是在自己的这个故事里面——你不断地感受自己到底是什么样的一个人，你为什么要做这个事情——不断去思考这些问题。</p> <p>张小珺：你是一个什么样的人？你为什么要做这样事情？</p> <p>杨植麟：就是觉得有意思。</p> <p>张小珺：什么有意思呢？做实验有意思，做科研有意思，还是做AI有意思？</p> <p>杨植麟：寻找真相的过程。去不断发现新的问题、解决它的过程。</p> <p>张小珺：那你也可以解决别的问题啊，为什么一定要解决这个问题？</p> <p>杨植麟：因为这个东西很重要，AI很重要。</p> <p>这个问题我也问过Kimi。他说，这个东西是“人类文明的放大器”，我觉得很有道理。</p> <p>又回到The Beginning of Infinity，从启蒙运动到现在，人类一直在寻找新的方法突破知识的边界。但是，可能下一个突破边界的，是靠AI，它是一个巨大的杠杆。</p> <p>你今天在任何一个前沿学科，要花二三十年，才能学到最前沿知识。但是AI一夜之间就能学会，往下去做新的突破。</p> <p>AI会成为Meta science（元科学）。</p> <p>它是人类文明的放大器。</p> <p>张小珺：它有可能摧毁人类文明吗？</p> <p>杨植麟：这个风险不能说不存在，但我们可以有很多事去做。不管是更安全地对齐，还是更好的社会机制。</p> <p>比如说，当AI可以做一些事情，它很有可能创造一些新的工作，我们需要有一些方法去完成这个过渡。</p> <p>我exactly问过Kimi这个问题。他说，虽然有这样的风险，但我们不能放弃。因为如果放弃，就等于放弃了人类文明的上限——你不知道上限能做到什么样，有一点“因噎废食”的感觉。</p> <p>但我承认，我们得做很多去应对。因为你今天看到很多AI的能力，是有点让人震惊的，半年前根本想不到它能做到。</p> <p>同时我认为，人类的独特价值在这个过程中会持续存在。人的体验、情感，没有办法被AI替代。所以，可能会有不同的活法，希望是能活得更好。</p> <p>张小珺：什么样不同的活法？</p> <p>杨植麟：我之前觉得，人的一生有几个意义：创造、体验和爱。当然每个人不一样，对我是这样。</p> <p>“创造”的很大一部分，也许AI可以做。我享受这个过程，但不得不承认，有一天很多创造性工作是AI去做。但后两条，“体验”和“爱”，会是以人为中心的。</p> <p>张小珺：如果AI把创造拿走了，也把生产力拿走了。</p> <p>杨植麟：这无所谓，人可以享受生产的结果，如果我们有好的机制。</p> <p>但它是一个缓慢过程，不会一两年做完，需要一二十年逐渐调整。</p> <p>张小珺：你会频繁地和Kimi聊天吗？</p> <p>杨植麟：当然。我要测试模型。</p> <p>张小珺：你会和他聊一些很深刻的话题或者自我探索的话题吗？</p> <p>杨植麟：有时候会。也还好，还有一些是工作上的问题。</p> <p>张小珺：过去这一年，你有经历过情绪很低落的时刻吗？</p> <p>杨植麟：我觉得也还好。更多是：有些东西会work，有些东西不work；会解决一些问题，会有新的问题产生——不断在这个过程中。</p> <p>只要你觉得这个东西有意思，就一直想继续做下去。</p> <h3 id="14-任何中间状态都有可能成为被批评的对象">14 任何中间状态都有可能成为被批评的对象</h3> <p>张小珺：过去一年，你有没有走过一些弯路？</p> <p>杨植麟：肯定有。过程中会有很多很多决策，有一些技术决策，有一些业务决策。</p> <p>很重要的是，一个公司在这个过程中逐渐调整的能力。知识创造的过程也是这样的过程——不可能创造出来的知识，所有东西都是对的，你会发现有些东西也是错的，但它在一定时间内可能是对的，一定时间可能又是错的。但当它错了之后，你就得去做调整。</p> <p>比如像牛顿做的很多东西，在当时是最好的理论，但不是完美的，在一些场景下是完全错的。万有引力需要有一些别的解释，需要有一些相对论的解释，通过时空的扭曲去解释。</p> <p>我觉得组织的进化、公司的发展也是一样，是一个动态过程。任何的中间点，你在某个时间点是对的，在另一个时间点可能就是错的。</p> <p>这也是Kimi跟我讲的——任何中间状态都有可能成为被批评的对象。你总是会有这个时代的局限性。</p> <p>更重要的是，你怎么在这个过程中，一方面投入一些“不变”的东西，比如人才、技术积累；另一方面适应和调整，针对环境变化和反馈信号做调整。这两个都很重要。</p> <p>张小珺：互联网产品通过市场推广去扩大DAU，扩大市场规模；AI产品似乎有所不同，增长和获客更依赖模型能力的大幅跃升——智能和推广，哪个更本质？</p> <p>杨植麟：它还是取决，两个变量哪个大。在技术快速发展的阶段，你很难通过市场推广的方式去赢得战争。它更多是一个辅助手段。</p> <p>只是说这个辅助手段跟你的主要手段之间，到底什么样的配比？需要动态调整，也取决于你现在商业化进展，或者PMF到底有多强。不同时间点有不同策略。</p> <p>甚至这个策略也许再过一两年，你发现又是一个好的策略。我觉得也不一定。</p> <p>我们是用更open的心态去看，但在每个时间点最重要的是去抓住——哪个是最大的变量。</p> <p>张小珺：又过了一年，你觉得Kimi的成功概率增大了还是失败概率增大？</p> <p>杨植麟：我觉得（成功概率）增大了。你只要每往上爬，成功概率会变大，因为会有一些人不继续爬了。</p> <p>张小珺：你恐惧摔下去吗？</p> <p>杨植麟：肯定有恐惧。</p> <p>更多要关注你当前这一步，能做什么？——想这个问题更重要。</p> <p>张小珺：我一直在问你的情绪，你都说：唉，还好、还好。你最近一次或两次“颅内自嗨”是什么情境？</p> <p>杨植麟：不以物喜，不以己悲。虽然很难做到，但要避免情绪化决策。</p> <p>张小珺：你会情绪化吗？</p> <p>杨植麟：多少肯定会——你是一个人嘛。但要避免一些情绪化决策。最终落实到决策和执行上，要更理性一点。</p> <p>张小珺：过去一年，你最大成长是什么？</p> <p>杨植麟：认识到这一点：问题不可避免，它会一直存在，持续解决新问题是最重要的，可能也是最有意思的——这是心态上的变化，它会改变很多做事的方式。</p> <p>张小珺：这听上去是一种正念。</p> <p>杨植麟：我不知道怎么理解，但可能差不多。（笑）</p> <p>张小珺：我问你最后几个快问快答。</p> <p>一个全球范围内你喜欢的食物。</p> <p>杨植麟：拉面！</p> <p>张小珺：为什么？</p> <p>杨植麟：好吃！</p> <p>张小珺：一个少有人知道但必须知道的知识点。</p> <p>杨植麟：我好像不太擅长回答这种问题。</p> <p>张小珺：基于所有读过的书，推荐必读书。</p> <p>杨植麟：有一本我刚刚一直在讲，就推荐这本。</p> <p>张小珺：你心目中影响AI进程的几篇论文是什么？</p> <p>杨植麟：最重要的几篇论文是Backpropagation（反向传播）、Transformer、GPT-3。</p> <p>当然有一些是building block（基础模块），也很重要，比如ResNet（残差网络），它可能是优化的基础。还有Adam（自适应矩估计优化算法）。现在可能还有Muon。</p> <p>张小珺：基于当下认知，一个最关键的bet是什么？</p> <p>杨植麟：泛化的Agent（智能体）。</p> <p>用Innovation（创新），用L4做L3。</p> <p>张小珺：这一年，你有没有任何的顿悟时刻？</p> <p>杨植麟：不知道，我感觉我脑子已经糊了。</p> <p>我已经把一年的话都讲了。</p> <p>现场第三位人士：这可能就是，Long Context影响智商。</p> <p>杨植麟：没办法。</p> <p>碳基生物的局限性。（笑）</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[KIMI创始人杨植麟深度访谈：攀登无限之山]]></summary></entry><entry><title type="html">OpenAI双巨头首次详解GPT-5：不是下一代GPT，终极形态是AI研究员</title><link href="https://emigmo.github.io/blog/2025/openai-gpt5-researcher-vision/" rel="alternate" type="text/html" title="OpenAI双巨头首次详解GPT-5：不是下一代GPT，终极形态是AI研究员"/><published>2025-10-23T00:00:00+00:00</published><updated>2025-10-23T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/openai-gpt5-researcher-vision</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/openai-gpt5-researcher-vision/"><![CDATA[<h1 id="openai双巨头首次详解gpt-5不是下一代gpt终极形态是ai研究员">OpenAI双巨头首次详解GPT-5：不是下一代GPT，终极形态是AI研究员</h1> <p><em>访谈对象：Jakub Pachocki（OpenAI首席科学家）、Mark Chen(OpenAI首席研究官)</em><br/> <em>时间：2025年10月</em><br/> <em>来源：硅谷风投a16z深度访谈</em></p> <hr/> <h2 id="前言">前言</h2> <blockquote> <p><strong>“我们希望模型能自己发现新想法,自己推进研究。”</strong></p> <p><strong>“GPT-5不是GPT-4的简单升级,而是一个重要的转折点。”</strong></p> </blockquote> <p>前不久,OpenAI的两大巨头——首席科学家Jakub Pachocki与首席研究官Mark Chen共同接受硅谷风投a16z深度访谈,首次系统性地揭示了GPT-5的真实定位。震撼的不是GPT-5本身,而是它背后的野心：<strong>打造”自动化研究员”</strong>。</p> <p><strong>三个关键信号值得所有人关注：</strong></p> <ol> <li><strong>融合革命</strong>：GPT-5将整合GPT系列的”快速响应”与o1系列的”深度推理”,让模型自主判断”这个问题需要几秒还是几小时思考”</li> <li><strong>硬科学突破已现</strong>：物理学家、数学家试用后震惊地发现,GPT-5能提出非平凡的新数学结果</li> <li><strong>从”vibe coding”到”vibe researching”</strong>：OpenAI的真正目标是与AI协同做研究,这将彻底改变科研的传统方式</li> </ol> <hr/> <h2 id="目录">目录</h2> <ul> <li><a href="#一gpt-5诞生从快速响应到深度推理">一、GPT-5诞生：从”快速响应”到”深度推理”</a></li> <li><a href="#二评估标准大转向从考高分到创造新可能">二、评估标准大转向：从”考高分”到”创造新可能”</a></li> <li><a href="#三强化学习推理的引擎潜力仍然巨大">三、强化学习：推理的”引擎”,潜力仍然巨大</a></li> <li><a href="#四ai编程革命从vibe-coding到vibe-researching">四、AI编程革命：从”vibe coding”到”vibe researching”</a></li> <li><a href="#五ai的未来瞄准自动化研究员">五、AI的未来：瞄准”自动化研究员”</a></li> <li><a href="#六团队文化持续学习长期主义">六、团队文化：持续学习,长期主义</a></li> <li><a href="#结语ai从回答者变成合作者">结语：AI从”回答者”变成”合作者”</a></li> </ul> <hr/> <h2 id="一gpt-5诞生从快速响应到深度推理">一、GPT-5诞生：从”快速响应”到”深度推理”</h2> <h3 id="11-两条技术路线的融合">1.1 两条技术路线的融合</h3> <p>Mark介绍,过去OpenAI有两条并行的技术路线：</p> <p><strong>GPT系列</strong>：</p> <ul> <li>从GPT-2到GPT-4</li> <li>特点是快速响应、即时输出</li> </ul> <p><strong>o系列</strong>：</p> <ul> <li>不追求速度,而是”思考更久”</li> <li>力求给出最优答案</li> </ul> <blockquote> <p><strong>GPT-5做了一件关键的事：把这两条路线彻底融合。</strong></p> </blockquote> <p>它能自主判断”这个问题需要几秒钟还是几个小时思考”,不用用户手动选择模式。这让”推理能力”和”类代理能力”成了模型的默认配置,也让GPT-5成为首个真正意义上的”推理模型”。</p> <h3 id="12-推理过程的突破">1.2 推理过程的突破</h3> <p>在推理过程中,GPT-5像人类一样,也会经历<strong>“尝试—失败—调整—再尝试”</strong>的过程。更重要的是,它能显著延长”不跑偏”的持续推理时长,解决了行业里”步骤过多就会质量下降”的老难题。</p> <h3 id="13-训练中的挑战">1.3 训练中的挑战</h3> <p>不过,训练GPT-5的过程也并不是一帆风顺的。最常见的麻烦是<strong>“双重bug”</strong>：</p> <ul> <li>既有代码层面的漏洞</li> <li>也有研究者思维上的”偏差假设”</li> </ul> <p>这些问题一旦出现,可能让几个月的实验白费。Jakub坦言,很多重大突破的本质,其实就是”识别并修正这些隐藏的错误”。</p> <h3 id="14-gpt-5-codex推理智能落地编程">1.4 GPT-5 Codex：推理智能落地编程</h3> <p>除了推理能力的融合,GPT-5还有一个重要延伸——<strong>GPT-5 Codex</strong>,专门让推理智能落地到编程场景。</p> <p><strong>Codex团队做了三个升级：</strong></p> <ol> <li><strong>处理更复杂的真实编码环境</strong>：适配工业级开发需求</li> <li><strong>关注开发者的风格和习惯</strong>：能根据需求调整”模型主动性”</li> <li><strong>优化延迟时间</strong>：简单题快速答复,复杂题花更多时间求最优解,解决了过去”简单题耗时、难题不深入”的失衡问题</li> </ol> <hr/> <h2 id="二评估标准大转向从考高分到创造新可能">二、评估标准大转向：从”考高分”到”创造新可能”</h2> <h3 id="21-传统评估的饱和">2.1 传统评估的饱和</h3> <p>在GPT-2到GPT-4的时代,模型进步靠”评测(evals)”验证,分数从98%追到99%,已经逼近饱和。</p> <blockquote> <p><strong>但根据Jakub所言,GPT-5的价值,并不被定位在”答对多少题”上,而在于”能不能提出全新解法”。</strong></p> </blockquote> <h3 id="22-新的评估维度">2.2 新的评估维度</h3> <p>现在,OpenAI更关注三个维度：</p> <ol> <li>模型能不能自主发现问题？</li> <li>能不能在开放领域持续推进研究？</li> <li>能不能在没有提示的情况下找到新路径？</li> </ol> <h3 id="23-顶尖赛事中的表现">2.3 顶尖赛事中的表现</h3> <p>这种评估转向,在实际场景里已经有了清晰体现。比如在AtCoder、IMO(国际数学奥林匹克)、IOI(国际信息学奥林匹克)这些顶尖赛事中,GPT-5已经接近人类顶尖水平。</p> <p>但Jakub强调：</p> <blockquote> <p><strong>“这些比赛的排名不是重点,真正的进步是模型开始能发现新思路。”</strong></p> </blockquote> <h3 id="24-硬科学领域的突破">2.4 硬科学领域的突破</h3> <p>更让人惊喜的是硬科学领域的突破。OpenAI团队邀请许多物理学家、数学家试用后发现:</p> <ul> <li>GPT-5能提出”非平凡的新数学结果”</li> <li>过去学生要花数月计算的内容,模型几乎能自动完成</li> <li>对研究者来说,这简直是”灵光一现的时刻”</li> </ul> <h3 id="25-开放领域的探索">2.5 开放领域的探索</h3> <p>而在没有明确对错的开放领域,GPT-5的能力也很关键。</p> <p>Jakub认为：</p> <blockquote> <p><strong>“真正要推动科研,有明确定义的问题和开放性问题之间的界限会逐渐模糊。”</strong></p> </blockquote> <p>就像数学千禧难题,需要跨物理、数学分支设计研究路线,这和AI的推理本质高度契合。现在GPT-5正用长时推理能力,在这些开放领域探索未知路径。</p> <hr/> <h2 id="三强化学习推理的引擎潜力仍然巨大">三、强化学习：推理的”引擎”,潜力仍然巨大</h2> <h3 id="31-rl远未到顶点">3.1 RL远未到顶点</h3> <p>外界很多人觉得,强化学习(RL)的潜力已经耗尽了,但Jakub并不认同这种观点：</p> <blockquote> <p><strong>“RL还远未到顶点,它正让语言模型学会在复杂目标中自我进化。”</strong></p> </blockquote> <h3 id="32-语言建模与强化学习的结合">3.2 语言建模与强化学习的结合</h3> <p>其实OpenAI早在大语言模型出现前,就开始探索强化学习了。近些年来,他们最大的突破,是把”语言建模”和”强化学习”结合到了一起：</p> <ul> <li>语言模型为RL提供丰富的环境</li> <li>RL则让模型学会执行复杂目标、自主决策和修正</li> </ul> <h3 id="33-对企业的启发">3.3 对企业的启发</h3> <p>这对企业来说也有启发。现在很多公司不知道怎么设计RL奖励模型,Jakub给出了建议：</p> <blockquote> <p><strong>RL会逐渐变得更自然,未来会从”人工设置奖励”走向”类人学习”模式。</strong></p> </blockquote> <p>大家别被”当前的做法”限制,给模型一些试错空间,比制定更多规则更重要。</p> <hr/> <h2 id="四ai编程革命从vibe-coding到vibe-researching">四、AI编程革命：从”vibe coding”到”vibe researching”</h2> <h3 id="41-编程能力的飞跃">4.1 编程能力的飞跃</h3> <p>作为曾经的竞技编程选手,Jakub和Mark对AI编程的进步感触很深。现在GPT-5在很多编程比赛中已经接近顶尖人类水平,差距还在快速缩小。</p> <p>过去Jakub并不习惯使用AI工具进行编程工作,但现在他坦言：</p> <blockquote> <p><strong>“GPT-5能在15分钟里完美重构30个文件,这种生产力提升根本无法忽视。”</strong></p> </blockquote> <h3 id="42-当前的临界期">4.2 当前的临界期</h3> <p>不过目前行业还处在一个”有点不自然的临界期”——模型不像真正的同事,但大家又必须依赖它。</p> <p>就像Mark说的,很多年轻人已经把<strong>“vibe coding”(与模型协同写代码)</strong>当成默认工作方式。</p> <h3 id="43-更远大的目标">4.3 更远大的目标</h3> <p>但OpenAI的目标更加远大,他们想尽快跨过这个阶段,进入<strong>“vibe researching”(与模型协同做研究)</strong>的新时代。</p> <hr/> <h2 id="五ai的未来瞄准自动化研究员">五、AI的未来：瞄准”自动化研究员”</h2> <h3 id="51-长远目标">5.1 长远目标</h3> <p>谈到未来1-5年的路线,Jakub明确表示,GPT-5的长远目标是成为”自动化研究员”。</p> <p>他再次强调：</p> <blockquote> <p><strong>“我们希望模型能自己发现新想法,自己推进研究。”</strong></p> </blockquote> <p>而且这不只限于机器学习领域,还要推动物理、数学等其他科学领域的自动化进展。</p> <h3 id="52-两个关键突破方向">5.2 两个关键突破方向</h3> <p>要实现这个目标,有两个关键方向要突破。</p> <h4 id="521-延长思考跨度">5.2.1 延长”思考跨度”</h4> <p>目前GPT-5能连续推理1-5小时解决复杂任务,下一步要让它在更长时间线上保持规划和记忆能力,像人类研究者一样”持续推进工作”,而不是只做”短平快”的答题。</p> <h4 id="522-资源支撑">5.2.2 资源支撑</h4> <p>和过去相同,OpenAI仍然倾向于把计算资源投入核心算法研究,而不是单纯优化产品。</p> <p>Mark直言：</p> <blockquote> <p><strong>“在前沿AI研究中,计算力几乎决定一切。”</strong></p> </blockquote> <p>目前行业仍受算力限制,而非外界传言的”数据瓶颈”。</p> <h3 id="53-驻留研究员项目">5.3 驻留研究员项目</h3> <p>为了培养更多研究人才,OpenAI还推出了<strong>“驻留研究员”项目</strong>。</p> <p>这个项目能让物理、金融等非AI背景的研究者快速上手,通过”亲手实现核心成果、在错误中建立直觉”,相当于”加速版博士训练”,正好补充了学术界”长期攻坚”的优势。</p> <hr/> <h2 id="六团队文化持续学习长期主义">六、团队文化：持续学习,长期主义</h2> <h3 id="61-永不停止的学习">6.1 永不停止的学习</h3> <p>OpenAI能一直保持领先,离不开它独特的团队文化。Mark一句话道出了核心：</p> <blockquote> <p><strong>“在OpenAI,你永远不会停止学习。”</strong></p> </blockquote> <p>这里每周都有新突破,研究者必须全力以赴才能跟上,避免了其他公司”前两年学习、后续进入平台期”的困境。</p> <h3 id="62-多元化背景">6.2 多元化背景</h3> <p>背景的多元化也注定了研究者们需要具备持续学习的能力。OpenAI最成功的研究者,很多来自物理、数学、金融等非AI领域。</p> <p>他们的共同点不是背景,而是：</p> <ul> <li>扎实的技术基础</li> <li>能坚持攻克极具挑战的问题</li> </ul> <h3 id="63-用人标准">6.3 用人标准</h3> <p>在用人层面,OpenAI团队并不简单看重”社交媒体活跃度”或”表面成果”,而是更认可两种人：</p> <ol> <li><strong>擅长”提出新方向”的</strong>：不局限于实现现有想法,而是能打开全新研究思路</li> <li><strong>擅长”深挖与验证”的</strong>：能把一个想法彻底落地,通过反复实验验证价值</li> </ol> <h3 id="64-研究与产品的平衡">6.4 研究与产品的平衡</h3> <p>作为兼具顶尖研究机构和优秀产品公司属性的组织,OpenAI从研究人员的特质出发,努力做到<strong>“研究与产品”的平衡</strong>：</p> <p><strong>对研究人员的安排：</strong></p> <ul> <li>关心产品的研究员会和产品团队紧密合作</li> <li>专注基础探索的研究员则能自由创新</li> </ul> <p><strong>团队协作方式：</strong></p> <ul> <li>产品团队和领导层从不把现有产品当终点</li> <li>和研究团队一起,锚定”自动化研究员”的长期目标</li> <li>把扩散模型、代码推理等多样化方向,统一到同一路线图中</li> </ul> <h3 id="65-应对外界反馈的定力">6.5 应对外界反馈的定力</h3> <p>面对外界反馈,比如竞品发布新模型,OpenAI也有自己的定力——不被短期产品反应左右研究的优先级。</p> <p>Mark强调：</p> <blockquote> <p><strong>“研究的节奏是长期的,产品迭代更快。”</strong></p> </blockquote> <p>团队始终聚焦”未来一两年甚至更久的重大问题”,不会陷入”竞速思维”。</p> <hr/> <h2 id="结语ai从回答者变成合作者">结语：AI从”回答者”变成”合作者”</h2> <p>GPT-5不只是”长时推理时代”的开端,更在编程、硬科学领域打开了新可能。现在它不再是被动的”回答者”,而是能和人类协同研究、创造新解法的”合作者”。</p> <h3 id="对企业的三个转变">对企业的三个转变</h3> <ol> <li><strong>使用AI时</strong>：从”提问等待”升级为”共同研究”</li> <li><strong>衡量AI时</strong>：从”分数高低”转向”创新与洞见”</li> <li><strong>管理团队时</strong>：从”追热点”转向”培养长期攻坚与学习能力”</li> </ol> <h3 id="通往未来的里程碑">通往未来的里程碑</h3> <p>GPT-5不是终点,而是通往”自动化研究员”的关键里程碑。当AI能提出新数学定理、”vibe researching”成为常态,知识边界与创新模式都会被彻底重塑。</p> <blockquote> <p><strong>AI已经进入”长思考”时代,我们要学的不只是怎么用它,更是怎么跟上它的思考速度。</strong></p> </blockquote> <hr/> <p><strong>来源：</strong> 硅谷风投a16z深度访谈<br/> <strong>整理：</strong> OpenAI双巨头首次系统性揭示GPT-5定位</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[OpenAI双巨头首次详解GPT-5：不是下一代GPT，终极形态是AI研究员]]></summary></entry><entry><title type="html">Jason Wei：理解2025年AI进展的三种关键思路</title><link href="https://emigmo.github.io/blog/2025/jason-wei-ai-insights/" rel="alternate" type="text/html" title="Jason Wei：理解2025年AI进展的三种关键思路"/><published>2025-10-19T00:00:00+00:00</published><updated>2025-10-19T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/jason-wei-ai-insights</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/jason-wei-ai-insights/"><![CDATA[<h1 id="jason-wei理解2025年ai进展的三种关键思路">Jason Wei：理解2025年AI进展的三种关键思路</h1> <p><em>演讲者：Jason Wei（前OpenAI核心研究员、CoT作者）</em><br/> <em>时间：2025年10月19日</em><br/> <em>地点：斯坦福大学AI Club</em><br/> <em>整理：深度访谈</em></p> <hr/> <h2 id="前言">前言</h2> <blockquote> <p><strong>「所有能被验证的任务，最终都会被 AI 解决。」</strong></p> <p><strong>「智能未来将成为一种商品，未来获取知识或进行某种推理的成本和可及性将趋近于零。」</strong></p> </blockquote> <p>最近，前 OpenAI 核心研究员、CoT（思维链）作者 Jason Wei 在斯坦福大学 AI Club 做了一场精彩的演讲。这也是他加入 Meta 超级智能实验室后少有的公开分享。</p> <p>Jason Wei 提出了三个理解和驾驭 2025 年 AI 发展至关重要的核心思想：<strong>验证者定律</strong>、<strong>智能的锯齿状边缘</strong>和<strong>智能商品化</strong>。</p> <p>某种意义上来说：</p> <ul> <li><strong>验证者定律</strong>决定「哪些点会被率先突破」</li> <li><strong>智能商品化</strong>解释「突破后如何被规模化与降本」</li> <li><strong>锯齿状边缘</strong>则强调「能力突破的时间序与不均衡版图」</li> </ul> <p>虽然没提创业，但似乎又句句不离创业。</p> <p><strong>Jason Wei 背景</strong>：目前在 Meta 超级智能实验室工作。在加入 Meta 之前，是 OpenAI 的核心科学家，参与了 o1 模型和 Deep Research 产品的创建，也是 CoT（Chain of Thought，思维链）的作者之一。</p> <hr/> <h2 id="一智能商品化智能和知识会变得又快又便宜">一、智能商品化：智能和知识会变得又快又便宜</h2> <h3 id="11-ai发展的两个阶段">1.1 AI发展的两个阶段</h3> <p>首先，我们来谈谈「智能商品化」。我认为，AI 的发展可以分为两个阶段：</p> <p><strong>第一阶段：推动前沿</strong></p> <ul> <li>AI 还不能很好地完成某项任务</li> <li>研究人员正在努力解锁这项新能力</li> <li>以 MMLU（大规模多任务语言理解）为例，过去五年的表现曲线显示性能逐渐提升</li> </ul> <p><strong>第二阶段：能力商品化</strong></p> <ul> <li>一旦 AI 掌握了某种能力，它就会被商品化</li> <li>达到特定性能水平所需的成本（以美元计）每年都在下降</li> <li>使用达到特定智能水平的模型成本趋近于零</li> </ul> <h3 id="12-自适应计算的突破">1.2 自适应计算的突破</h3> <blockquote> <p><strong>为什么这种趋势会持续下去？</strong></p> </blockquote> <p>我的观点是，这是深度学习历史上，<strong>自适应计算（adaptive compute）第一次真正奏效</strong>。</p> <p><strong>过去的模式</strong>：</p> <ul> <li>无论任务简单还是困难，用于解决特定问题的计算量都是固定的</li> <li>回答「加利福尼亚州的首府是什么」和解决奥数竞赛题使用相同的计算量</li> </ul> <p><strong>现在的突破</strong>：</p> <ul> <li>进入自适应计算时代，可以根据任务调整所使用的计算量</li> <li>如果任务非常简单，可以将计算成本降到最低</li> <li>不再需要持续扩大模型规模</li> </ul> <p><strong>o1模型的证明</strong>：</p> <ul> <li>一年多前发布的 o1 模型是最初突破</li> <li>在测试阶段为解决数学问题投入更多计算资源</li> <li>模型在基准测试上的表现显著提升</li> </ul> <h3 id="13-信息检索的四个时代">1.3 信息检索的四个时代</h3> <p>AI 商品化还有另外一个方面是，<strong>获取公共信息的时间会越来越短</strong>。我把信息检索分成了四个时代：</p> <table> <thead> <tr> <th>时代</th> <th>示例问题</th> <th>所需时间</th> <th>方法</th> </tr> </thead> <tbody> <tr> <td><strong>前互联网时代</strong></td> <td>1983年釜山的人口</td> <td>几小时</td> <td>开车去图书馆，翻阅百科全书</td> </tr> <tr> <td><strong>互联网时代</strong></td> <td>同上</td> <td>几分钟</td> <td>搜索引擎，浏览网页</td> </tr> <tr> <td><strong>聊天机器人时代</strong></td> <td>同上</td> <td>即时</td> <td>直接询问ChatGPT</td> </tr> <tr> <td><strong>智能Agent时代</strong></td> <td>1983年亚洲最多人口30城市的结婚人数排序</td> <td>几小时→几分钟</td> <td>AI自主搜索、分析、整合</td> </tr> </tbody> </table> <p><strong>复杂查询示例</strong>：</p> <ul> <li>问题：「1983 年釜山有多少人结婚」</li> <li>GPT-3：无法完成</li> <li><strong>OpenAI Operator</strong>：可以做到 <ul> <li>访问韩国统计信息服务（KOSIS）数据库</li> <li>自主点击查找</li> <li>找到正确的数据库查询</li> <li>返回答案</li> </ul> </li> </ul> <h3 id="14-browsecomp基准测试">1.4 BrowseComp基准测试</h3> <p>为了衡量这种能力，OpenAI 创建了 <strong>BrowseComp 基准测试</strong>：</p> <p><strong>特点</strong>：</p> <ul> <li>答案容易验证，但找答案非常耗时</li> <li>示例：「找出符合所有这些限制条件的足球比赛」</li> </ul> <p><strong>人类vs AI表现</strong>：</p> <ul> <li>人类：平均需要两个多小时，很多问题在两小时内无法完成</li> <li><strong>OpenAI Deep Research</strong>：可以解决其中大约一半的问题</li> </ul> <h3 id="15-智能商品化的三大影响">1.5 智能商品化的三大影响</h3> <p><strong>1. 领域民主化</strong></p> <blockquote> <p>那些过去因为知识门槛（比如编程）而受限的领域，会变得更加开放。</p> </blockquote> <ul> <li><strong>编程</strong>：技术门槛大幅降低</li> <li><strong>个人健康</strong>：过去去医生那里说「我想改善我的鼻呼吸」，医生可能只会说「试试我告诉你的方法」。但现在，ChatGPT 几乎能提供一个好医生能给你的所有信息</li> </ul> <p><strong>2. 私有信息价值提升</strong></p> <blockquote> <p>既然公共信息的成本降得这么低，那那些私密的、内部的、不公开的信息，相对价值就会高得多。</p> </blockquote> <ul> <li>非市场挂牌出售的房屋信息更值钱</li> <li>内部数据、专有知识的价值凸显</li> </ul> <p><strong>3. 个性化信息流</strong></p> <blockquote> <p>你访问的不再是人人共享的公共互联网，而是一个为你量身定制的个性化互联网。</p> </blockquote> <ul> <li>获取信息变得无摩擦、毫不费力</li> <li>AI会专门为你展示你想知道的内容</li> </ul> <hr/> <h2 id="二验证者定律训练ai解决任务的能力与任务的可验证性成正比">二、验证者定律：训练AI解决任务的能力与任务的可验证性成正比</h2> <h3 id="21-验证与求解的不对称性">2.1 验证与求解的不对称性</h3> <p><strong>核心概念</strong>：</p> <blockquote> <p>对于某些任务，验证解决方案比找到解决方案要容易得多。</p> </blockquote> <p><strong>典型示例</strong>：</p> <table> <thead> <tr> <th>任务类型</th> <th>生成难度</th> <th>验证难度</th> <th>不对称性</th> </tr> </thead> <tbody> <tr> <td><strong>数独</strong></td> <td>中等</td> <td>容易</td> <td>正向（易验证）</td> </tr> <tr> <td><strong>Twitter代码</strong></td> <td>困难（需几千工程师）</td> <td>容易（刷网页点几下）</td> <td>正向（易验证）</td> </tr> <tr> <td><strong>竞赛数学题</strong></td> <td>困难</td> <td>中等</td> <td>平衡</td> </tr> <tr> <td><strong>数据处理脚本</strong></td> <td>简单（自己写）</td> <td>困难（理解别人代码）</td> <td>反向（难验证）</td> </tr> <tr> <td><strong>事实性文章</strong></td> <td>容易（编造听起来有理的事实）</td> <td>困难（逐条核实）</td> <td>反向（难验证）</td> </tr> <tr> <td><strong>饮食方案</strong></td> <td>容易（随口说出）</td> <td>困难（需长期实验验证）</td> <td>反向（难验证）</td> </tr> </tbody> </table> <h3 id="22-验证者定律的定义">2.2 验证者定律的定义</h3> <blockquote> <p><strong>验证者定律（Verifiers Law）：训练 AI 解决任务的能力，与该任务的可验证性成正比。</strong></p> </blockquote> <p><strong>推论</strong>：</p> <ul> <li>任何<strong>可解决且易于验证</strong>的任务，最终都会被 AI 攻克</li> <li>首先被自动化的任务，将是那些非常容易验证的任务</li> </ul> <h3 id="23-可验证性的五个维度">2.3 可验证性的五个维度</h3> <p>具体来说，我认为「可验证性」体现在以下五个方面：</p> <ol> <li><strong>客观性</strong>：有没有明确的对错标准？</li> <li><strong>验证速度</strong>：检查起来快不快？</li> <li><strong>可批量验证</strong>：能不能一次性检查几百万个方案？</li> <li><strong>低噪音</strong>：验证结果是否稳定可靠？</li> <li><strong>连续反馈</strong>：是只有「对」和「错」两种结果，还是能给出具体的分数，衡量质量的好坏？</li> </ol> <h3 id="24-特权信息改变任务位置">2.4 特权信息改变任务位置</h3> <p>有意思的是，你可以通过提供一些<strong>特权信息（privileged information）</strong>来改变任务在「生成-验证」图上的位置：</p> <p><strong>示例</strong>：</p> <ul> <li><strong>竞赛数学</strong>：如果给你提供答案，检查就变得非常容易</li> <li><strong>编程任务</strong>：如果给你测试用例（如 SWE-bench），检查也变得非常容易</li> </ul> <blockquote> <p><strong>核心思想</strong>：有些任务你可以预先做一些工作，从而增加验证的不对称性。</p> </blockquote> <h3 id="25-ai基准测试的快速攻克">2.5 AI基准测试的快速攻克</h3> <p>大多数 AI 基准测试，从定义上来说，都是易于验证的。</p> <p><strong>历史证明</strong>：</p> <ul> <li>过去五年中关注的所有基准测试都相对快速地被 AI 解决</li> <li>这是验证者定律的最好实例</li> </ul> <h3 id="26-deepmind-alphadev的实践">2.6 DeepMind AlphaDev的实践</h3> <p><strong>AlphaDev项目</strong>是利用验证不对称性的绝佳例子：</p> <p><strong>任务示例</strong>：</p> <blockquote> <p>「找到这 11 个六边形的放置方式，使得围绕它们绘制的最小外围六边形面积最小。」</p> </blockquote> <p><strong>符合五个标准</strong>：</p> <ul> <li>✅ 结果客观（画出来就能验证）</li> <li>✅ 验证速度快且可扩展（计算性的，能批量检查）</li> <li>✅ 噪音低（每次检查结果都一样）</li> <li>✅ 连续反馈（外接六边形的大小直接反映方案优劣）</li> </ul> <p><strong>进化式搜索算法</strong>：</p> <pre><code class="language-mermaid">graph LR
    A[生成Sample] --&gt; B[评估Grade]
    B --&gt; C[迭代Iterate]
    C --&gt; A
    style A fill:#e1f5ff
    style B fill:#ffe1e1
    style C fill:#e1ffe1
</code></pre> <ol> <li><strong>生成（Sample）</strong>：让大语言模型生成大量候选解决方案（代码）</li> <li><strong>评估（Grade）</strong>：任务高度可验证，自动、快速地给每个方案打分</li> <li><strong>迭代（Iterate）</strong>：将得分最高的方案作为「灵感」，反馈给大语言模型</li> </ol> <p><strong>成果</strong>：</p> <ul> <li>通过投入海量计算资源进行循环</li> <li>能够发现比人类专家设计的算法更优的解</li> </ul> <p><strong>聪明之处</strong>：</p> <ul> <li>巧妙地避开了「泛化」问题</li> <li>训练和测试是同一个任务</li> <li>只关心解决这一个具体问题</li> <li>需要挑选那些有可能找到比已知答案更好的答案的问题</li> </ul> <h3 id="27-创业启示">2.7 创业启示</h3> <blockquote> <p><strong>未来一个非常重要的领域（无论是你想创业还是看好它会发展），就是发明衡量事物的方法。</strong></p> </blockquote> <p><strong>机会点</strong>：</p> <ul> <li>如果你能为某个原本难以衡量的领域（比如创造力、用户体验）设计出一套快速、客观、可扩展的评估体系</li> <li>那么接下来就可以利用 AI 来大规模地优化它</li> </ul> <hr/> <h2 id="三智能的锯齿状边缘发展不均衡">三、智能的锯齿状边缘：发展不均衡</h2> <h3 id="31-对ai影响的分歧">3.1 对AI影响的分歧</h3> <blockquote> <p><strong>「AI 的发展会怎么改变我们的世界？」</strong></p> </blockquote> <p>你会发现，不同的人会给出完全不同的答案：</p> <p><strong>量化交易朋友的看法</strong>：</p> <blockquote> <p>「ChatGPT 确实很酷，但它做不了我工作中那些具体的事情。」</p> </blockquote> <p><strong>顶尖实验室AI研究员的看法</strong>：</p> <blockquote> <p>「我们基本上只剩下两到三年的工作时间了，之后 AI 就会取代我们的工作。」</p> </blockquote> <p><strong>Boaz的观点</strong>：</p> <blockquote> <p>「东海岸的人低估了即将到来的变革，他们可能会说’哦，当前的模型做不到这个’，而不太考虑其发展轨迹。而在湾区，可能又会低估把我们训练出来的模型真正落地应用，需要克服多少障碍，耗费多少时间。」</p> </blockquote> <p><strong>Roon的观点</strong>：</p> <blockquote> <p>「现在不应该给出或接受任何职业建议。所有人普遍低估了变革的广度和规模，还有未来职业生涯的巨大不确定性。」</p> </blockquote> <h3 id="32-为什么快速起飞不会发生">3.2 为什么「快速起飞」不会发生</h3> <p>长期以来，有一个假说叫做<strong>「快速起飞」（fast takeoff）</strong>：</p> <p><strong>假说内容</strong>：</p> <ul> <li>一旦 AI 在某个方面超越了人类</li> <li>就会突然变得比人类强大得多</li> <li>在很短的时间内，实现智能的爆炸式增长</li> </ul> <p><strong>我的观点</strong>：这种情景可能不会发生。</p> <p><strong>更现实的场景</strong>：</p> <table> <thead> <tr> <th>时间线</th> <th>AI能力状态</th> </tr> </thead> <tbody> <tr> <td><strong>第一年</strong></td> <td>AI连研究代码库都跑不起来</td> </tr> <tr> <td><strong>第二年</strong></td> <td>AI可以勉强训练一个模型，但效果很差</td> </tr> <tr> <td><strong>第三年</strong></td> <td>AI可以自主训练了，但效果不如顶尖的人类研究团队</td> </tr> <tr> <td><strong>第四年</strong></td> <td>AI训练得很好，但偶尔还需要人类介入来解决疑难杂症</td> </tr> </tbody> </table> <blockquote> <p><strong>这更像是一个自我改进能力的「光谱」，而不是一个二元的选择。</strong></p> </blockquote> <h3 id="33-锯齿状边缘的形成">3.3 锯齿状边缘的形成</h3> <p><strong>核心观点</strong>：</p> <ul> <li>自我改进的速度，应该按「每个具体任务」来考量</li> <li>各种任务就像锯齿状的边缘</li> </ul> <pre><code class="language-mermaid">graph TD
    A[AI能力版图] --&gt; B[高峰：表现出色的领域]
    A --&gt; C[低谷：表现较弱的领域]
    B --&gt; D[复杂数学题]
    B --&gt; E[编程竞赛题]
    C --&gt; F[9.11 vs 9.9比较]
    C --&gt; G[特林吉特语翻译]
    style B fill:#90EE90
    style C fill:#FFB6C1
</code></pre> <p><strong>高峰示例</strong>（AI表现出色）：</p> <ul> <li>复杂的数学题</li> <li>某些编程竞赛题</li> </ul> <p><strong>低谷示例</strong>（AI表现较弱）：</p> <ul> <li>ChatGPT 曾经很长时间都说 9.11 比 9.9 大</li> <li>特林吉特语（Tlingit）翻译：只有几百个美洲原住民才会说的语言</li> </ul> <blockquote> <p><strong>我并不认为我们会看到这样的情况：一个自我改进的模型，突然之间就什么都搞定了。</strong></p> </blockquote> <h3 id="34-预测ai进步速度的三个窍门">3.4 预测AI进步速度的三个窍门</h3> <h4 id="窍门1ai擅长数字任务">窍门1：AI擅长数字任务</h4> <p><strong>原因</strong>：</p> <ul> <li>核心是<strong>迭代速度</strong></li> <li>搞数字任务，扩展计算资源比用真实机器人做实验容易多了</li> </ul> <p><strong>示例</strong>：</p> <ul> <li>「家庭作业机器」漫画（1981年）：对AI工作方式的描绘在今天看来还挺准</li> <li>《我，机器人》那种场景：也许很快会有，但目前还没实现</li> </ul> <h4 id="窍门2对人类来说越容易的任务ai往往也觉得越容易">窍门2：对人类来说越容易的任务，AI往往也觉得越容易</h4> <p><strong>推论</strong>：</p> <ul> <li>可以想象一下人类任务难度的分布</li> <li>AI可能能完成人类因为生理限制而无法完成的任务 <ul> <li>示例：预测乳腺癌（如果能看过1000万张图像）</li> </ul> </li> </ul> <h4 id="窍门3数据越充足ai就越如鱼得水">窍门3：数据越充足，AI就越如鱼得水</h4> <p><strong>实证</strong>：</p> <ul> <li>语言模型在不同语言中的数学表现</li> <li>某个语言的「使用频率」（数据量）和它的表现呈正相关</li> <li><strong>趋势非常明显：数据越多，AI在这个任务上就表现得越好</strong></li> </ul> <p><strong>强化学习的补充</strong>：</p> <blockquote> <p>如果存在一个明确的、单一的客观评估指标，那么你就可以采用 AlphaEvolve 或 AlphaZero 的策略，通过强化学习来生成「假数据」，实现自我训练。</p> </blockquote> <p><strong>Danny Do的推特</strong>：</p> <blockquote> <p>「只要任务提供清晰的评估指标，可以作为训练时的奖励信号，任何基准测试都可以被迅速解决。」</p> </blockquote> <h3 id="35-ai任务时间表预测">3.5 AI任务时间表预测</h3> <table> <thead> <tr> <th>任务</th> <th>人类难度</th> <th>是否数字任务</th> <th>数据充足度</th> <th>预计时间</th> </tr> </thead> <tbody> <tr> <td><strong>翻译（前50种语言）</strong></td> <td>不难</td> <td>✅</td> <td>充足</td> <td>✅ 已完成</td> </tr> <tr> <td><strong>调试基础代码</strong></td> <td>中等</td> <td>✅</td> <td>充足</td> <td>✅ 2023年</td> </tr> <tr> <td><strong>竞赛数学</strong></td> <td>难</td> <td>✅</td> <td>充足</td> <td>✅ 2024年</td> </tr> <tr> <td><strong>AI研究</strong></td> <td>很难</td> <td>✅</td> <td>中等</td> <td>🔮 2027年？</td> </tr> <tr> <td><strong>化学研究</strong></td> <td>难</td> <td>❌</td> <td>中等</td> <td>🔮 比AI研究晚</td> </tr> <tr> <td><strong>拍电影</strong></td> <td>非常难</td> <td>✅</td> <td>充足</td> <td>🔮 2029年？</td> </tr> <tr> <td><strong>预测股市</strong></td> <td>极难</td> <td>✅</td> <td>充足</td> <td>❓ 不确定</td> </tr> <tr> <td><strong>翻译特林吉特语</strong></td> <td>不难（对懂的人）</td> <td>✅</td> <td>极少</td> <td>❌ 可能性极低</td> </tr> <tr> <td><strong>修水管</strong></td> <td>中等</td> <td>❌</td> <td>不确定</td> <td>❓ 不确定</td> </tr> <tr> <td><strong>理发</strong></td> <td>中等</td> <td>❌</td> <td>中等</td> <td>❌ 很难</td> </tr> <tr> <td><strong>手工地毯制作</strong></td> <td>非常难</td> <td>❌</td> <td>极少</td> <td>❌ 短期不可能</td> </tr> <tr> <td><strong>带女朋友约会让她开心</strong></td> <td>😊</td> <td>❌</td> <td>极少</td> <td>❌ 永远搞不定！</td> </tr> </tbody> </table> <blockquote> <p><strong>注</strong>：这些年份都是我随口说的，大家别当真。</p> </blockquote> <h3 id="36-总结与启示">3.6 总结与启示</h3> <p><strong>核心观点</strong>：</p> <ol> <li>不会出现某种快速的超级智能起飞</li> <li>每项任务的能力和改进速度都不同</li> <li>AI影响最大的，是那些符合特定属性的任务： <ul> <li>✅ 数字任务</li> <li>✅ 对人类来说不难</li> <li>✅ 数据丰富</li> </ul> </li> </ol> <p><strong>影响预测</strong>：</p> <pre><code class="language-mermaid">graph LR
    A[AI影响] --&gt; B[极大加速的领域]
    A --&gt; C[保持不变的领域]
    B --&gt; D[软件开发]
    B --&gt; E[数据分析]
    B --&gt; F[内容创作]
    C --&gt; G[理发]
    C --&gt; H[水管维修]
    C --&gt; I[手工艺品]
    style B fill:#90EE90
    style C fill:#FFE4B5
</code></pre> <ul> <li><strong>某些领域将因 AI 而极大地加速</strong>：例如软件开发</li> <li><strong>另一些领域可能会保持不变</strong>：例如理发</li> </ul> <hr/> <h2 id="四结语与思考">四、结语与思考</h2> <p>Jason Wei 的三个核心思想为我们理解2025年AI发展提供了一个完整的框架：</p> <h3 id="核心框架总结">核心框架总结</h3> <pre><code class="language-mermaid">graph TD
    A[AI发展框架] --&gt; B[验证者定律]
    A --&gt; C[智能商品化]
    A --&gt; D[锯齿状边缘]
    B --&gt; E[决定哪些点被率先突破]
    C --&gt; F[解释突破后如何规模化与降本]
    D --&gt; G[强调能力突破的时间序与不均衡]
    style A fill:#FFD700
    style B fill:#87CEEB
    style C fill:#90EE90
    style D fill:#FFB6C1
</code></pre> <h3 id="对创业者的启示">对创业者的启示</h3> <p>虽然演讲没有直接提到创业，但其中蕴含的洞察对创业者极具价值：</p> <ol> <li> <p><strong>选择易验证的问题域</strong></p> <ul> <li>优先进入那些可以快速验证结果的领域</li> <li>投资于建立评估体系</li> </ul> </li> <li> <p><strong>关注成本降低趋势</strong></p> <ul> <li>智能商品化意味着私有信息的价值提升</li> <li>专注于获取和利用独特的数据资源</li> </ul> </li> <li> <p><strong>理解不均衡发展</strong></p> <ul> <li>不要指望AI在所有领域同步突破</li> <li>在数字任务、数据丰富的领域优先布局</li> <li>物理世界的服务仍将是人类的优势领域</li> </ul> </li> </ol> <h3 id="对研究者的启示">对研究者的启示</h3> <ol> <li> <p><strong>关注验证机制设计</strong></p> <ul> <li>好的验证机制能加速AI在该领域的进步</li> <li>评估体系的设计本身就是重要的研究课题</li> </ul> </li> <li> <p><strong>重视自适应计算</strong></p> <ul> <li>Test-time compute是重要的研究方向</li> <li>如何根据任务难度动态调整计算资源</li> </ul> </li> <li> <p><strong>数据效率仍然重要</strong></p> <ul> <li>长尾任务、小语种、特殊领域的数据仍然稀缺</li> <li>如何用有限数据达到好效果仍是关键问题</li> </ul> </li> </ol> <h3 id="最后的思考">最后的思考</h3> <p>Jason Wei 的框架提醒我们：</p> <blockquote> <p><strong>AI的发展不是一场革命，而是一系列不均衡的进化。</strong></p> </blockquote> <p>我们不应该期待某个「奇点」时刻的到来，而应该理解：</p> <ul> <li>哪些任务会被快速解决（易验证、数字化、数据丰富）</li> <li>哪些能力会被商品化（成本趋近于零）</li> <li>哪些领域仍将保持人类优势（物理世界、低数据领域）</li> </ul> <p>这种务实而精准的视角，或许正是我们在AI时代最需要的。</p> <hr/> <p><strong>演讲视频</strong>：<a href="https://www.youtube.com/watch?v=b6Doq2fz81U">Jason Wei at Stanford AI Club</a></p> <p><strong>编译整理</strong>：Founder Park</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[Jason Wei：理解2025年AI进展的三种关键思路]]></summary></entry><entry><title type="html">Nick Joseph访谈：Anthropic预训练的核心思考与实践</title><link href="https://emigmo.github.io/blog/2025/anthropic-pretraining-nick-joseph/" rel="alternate" type="text/html" title="Nick Joseph访谈：Anthropic预训练的核心思考与实践"/><published>2025-10-10T00:00:00+00:00</published><updated>2025-10-10T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/anthropic-pretraining-nick-joseph</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/anthropic-pretraining-nick-joseph/"><![CDATA[<h1 id="nick-joseph访谈anthropic预训练的核心思考与实践">Nick Joseph访谈：Anthropic预训练的核心思考与实践</h1> <p><em>访谈对象：Nick Joseph（Anthropic预训练团队负责人）</em><br/> <em>时间：2025年10月10日</em><br/> <em>来源：Y Combinator深度访谈</em></p> <hr/> <h2 id="前言">前言</h2> <p>本文基于Y Combinator于2025年9月30日对Anthropic预训练团队负责人Nick Joseph的深度访谈整理而成。Nick Joseph曾在Vicarious和OpenAI从事AI安全与模型缩放研究，深度参与了多代大语言模型的开发与优化。</p> <p><strong>核心观点速览：</strong></p> <ul> <li>预训练的核心是推动损失函数下降，这是我们一直追求的唯一目标</li> <li>对齐问题在于如何让模型分享人类的目标，尤其是在模型比人类更聪明时</li> <li>如果拥有无限计算资源，真正的挑战将是如何有效利用这些资源并解决规模扩展中的工程难题</li> <li>当前AI研究最大的瓶颈之一是计算资源受限，而非算法突破</li> </ul> <hr/> <h2 id="目录">目录</h2> <ul> <li><a href="#一从ai安全初心到anthropic预训练掌舵人">一、从AI安全初心到Anthropic预训练掌舵人</a></li> <li><a href="#二预训练基石与扩展定律自回归建模成为ai发展的核心引擎">二、预训练基石与扩展定律：自回归建模成为AI发展的核心引擎</a></li> <li><a href="#三工程实战深水区万卡集群硬件极限调试与分布式训练挑战">三、工程实战深水区：万卡集群、硬件极限调试与分布式训练挑战</a></li> <li><a href="#四合成数据风险模型评估指标设计与价值嵌入博弈">四、合成数据风险、模型评估指标设计与价值嵌入博弈</a></li> <li><a href="#五agi之路范式转移焦虑架构变革与未来展望">五、AGI之路：范式转移焦虑、架构变革与未来展望</a></li> </ul> <hr/> <h2 id="一从ai安全初心到anthropic预训练掌舵人">一、从AI安全初心到Anthropic预训练掌舵人</h2> <h3 id="11-从vicarious到openai的早期探索">1.1 从Vicarious到OpenAI的早期探索</h3> <p><strong>Ankit Gupta</strong>：大家好，今天非常高兴邀请到Anthropic的预训练负责人Nick Joseph来和我们对谈。我想先聊一聊你的背景，了解一下你是如何走到今天这一步的。</p> <p><strong>Nick Joseph</strong>：我之前在Vicarious工作，后来去了OpenAI，然后才加入Anthropic。Vicarious最初是一家以AGI为目标的实验室，我加入时他们正处于转型阶段，逐渐开始开发一些具体的产品，尤其是机器人相关的项目。我当时负责的主要是为机器人产品训练计算机视觉模型。那是我的第一份工作，所以我在那段时间主要学会了如何构建机器学习模型，以及如何搭建机器学习的基础设施。</p> <h3 id="12-选择实践而非学术的职业路线">1.2 选择实践而非学术的职业路线</h3> <p><strong>Ankit Gupta</strong>：那时候你有没有考虑过走学术路线？</p> <p><strong>Nick Joseph</strong>：其实我对这件事的想法有点不同，这很大程度上源于我之前在一个叫GiveWell的非营利机构实习的经历。GiveWell主要负责评估慈善项目的效果，当时那里有一些人提出——也许未来我们会拥有AGI，而它可能带来风险，需要提前关注这种潜在的威胁。</p> <p>那时我对这种说法并不是特别信服，我更倾向于直接去做一些可以帮助贫困人口的事情。后来因为种种原因，这条路没有走通，于是我决定至少去做AI相关的工作。这样无论未来AI安全是不是一个重大问题，我都能有所贡献：如果它真的重要，我就投身其中；如果不是，我也能用AI创造出一些能切实帮助贫困人群的东西。</p> <blockquote> <p><strong>所以我并不是以学术研究为出发点来进入这个领域。实际上，我之所以选择这条路，其中一个吸引我的地方是——我可以立即投入到AI实践中去。</strong></p> </blockquote> <h3 id="13-ai安全的早期状态与哲学探讨">1.3 AI安全的早期状态与哲学探讨</h3> <p><strong>Ankit Gupta</strong>：那么，当时AI安全这方面的研究处于什么状态呢？</p> <p><strong>Nick Joseph</strong>：在我看来，当时关于AI安全的大部分讨论都还停留在理论层面。那时候的模型其实并没有多强，也不存在真正的威胁。因此，当时的讨论更多是哲学性的——比如，假设未来我们会拥有比人类更聪明的AI，那我们是否应该提前重视这种潜在风险？</p> <h3 id="14-加入openai与代码能力的惊人突破">1.4 加入OpenAI与代码能力的惊人突破</h3> <p><strong>Ankit Gupta</strong>：接下来你去了OpenAI。当时的OpenAI是什么样的？</p> <p><strong>Nick Joseph</strong>：我当时加入的是一个安全研究团队，同时也在参与代码模型的相关研究。刚到那儿时，我看到的第一个项目就是他们对GPT-3进行微调，让它能够写代码。而且效果相当好。</p> <p>这让我突然意识到——如果人们担心AI会变得极其强大，甚至能自我编写、改进自己的代码，那么这种能力确实看起来有可能导致自我提升。于是我开始做很多评估和研究，分析哪些因素会促成这种能力。</p> <p>大概八个月后，我所在团队的几位AI安全负责人相继离开，而我之所以加入OpenAI，本身就是因为我非常关心AI安全问题，希望能和他们共事。后来他们中的一些人去了Anthropic，我也几乎在Anthropic成立初期就跟随加入了。</p> <hr/> <h2 id="二预训练基石与扩展定律自回归建模成为ai发展的核心引擎">二、预训练基石与扩展定律：自回归建模成为AI发展的核心引擎</h2> <h3 id="21-什么是预训练从互联网数据到下一个词预测">2.1 什么是预训练？从互联网数据到下一个词预测</h3> <p><strong>Ankit Gupta</strong>：既然提到了Anthropic，我们就来聊聊你现在的工作吧。如今你是Anthropic的预训练团队负责人。你能先谈谈什么是”预训练”吗？</p> <p><strong>Nick Joseph</strong>：我们知道让AI模型变得更强的关键要素之一就是”规模”。你需要投入大量的算力。而如果退一步思考，要想让模型获得尽可能多的算力，我们就需要一个拥有海量数据的训练目标。</p> <p>一个显而易见的想法是——互联网本身就是庞大的数据源，可能是人类历史上最大的数据集合。而且这些数据是无标签的，你不可能指望有人去把整个互联网内容都看一遍、逐条标注。因此，我们希望能从数据本身提取出”标签”。</p> <blockquote> <p><strong>于是，就有了这样的思路：我们可以让模型预测下一个词。这种方式的好处是，你能得到非常密集的学习信号——每一个词都是一个新的训练样本。</strong></p> </blockquote> <p>从GPT-1到GPT-2的研究发现，只要你持续增加算力、使用更多数据、训练更大的模型，模型的能力就会不断增强，表现也会更智能。可以说，这就是整个预训练理念的核心假设。</p> <h3 id="22-扩展定律与正反馈循环">2.2 扩展定律与正反馈循环</h3> <p>这里还涉及”扩展定律”的概念，也就是说，我们可以用相当精确的方式去量化：当你增加算力、数据量或模型参数时，模型在预测下一个词时的损失会以可预期的方式降低，性能也会相应提升。</p> <p>真正出乎意料的是，这一机制形成了一个<strong>“正反馈循环”</strong>：</p> <ol> <li>你训练出一个模型</li> <li>它能被用来创造有用的产品</li> <li>这些产品带来收入</li> <li>你再把收入投入更多算力</li> <li>从而训练出更好的模型</li> </ol> <p>过去五年左右，我们其实反复地在运行这一循环。</p> <h3 id="23-为什么自回归建模胜出">2.3 为什么自回归建模胜出？</h3> <p><strong>Ankit Gupta</strong>：”下一个词预测”这种自回归方式似乎已经成为主流的预训练方法。但如果回到2017年至2020年，当时其实存在各种各样不同的预训练目标。例如BERT、BART这些模型采用的是”掩码语言建模”。你对那个阶段有什么回顾或感想吗？</p> <p><strong>Nick Joseph</strong>：我认为答案主要是经验驱动的——换句话说，我们是通过大量实验得出的。我的观点是，这类问题最终要靠实证：都试一试，看哪种更有效。</p> <p><strong>自回归建模的巨大优势：</strong></p> <ul> <li>你可以直接从模型中采样生成文本，这个过程非常自然</li> <li>损失函数本身就能直接反映出你真正关心的目标</li> <li>如果你能把这个任务做到完美，模型自然就能像人一样写作</li> <li>容易转化为产品应用</li> </ul> <p>相比之下，其他一些方法并不具备这种天然的生成特性。</p> <h3 id="24-算力才是王道">2.4 算力才是王道</h3> <p><strong>Nick Joseph</strong>：没错，这种方式赋予了模型最具开放性的潜力。一般的流程是：你先训练一个基础模型，然后针对不同任务进行微调。</p> <p>不过我总体的直觉是：<strong>真正起决定作用的是算力</strong>。只要你投入足够的算力，无论采用哪种预训练目标，最终都能训练出表现不错的模型，然后再通过微调适配各种用途。</p> <blockquote> <p><strong>令人意外的是，很多我们以为关键的细节，其实远不如”增加算力”来得重要。</strong></p> </blockquote> <hr/> <h2 id="三工程实战深水区万卡集群硬件极限调试与分布式训练挑战">三、工程实战深水区：万卡集群、硬件极限调试与分布式训练挑战</h2> <p>Ankit Gupta：确实如此。而且当我们谈到“增加算力”时，这本身也有很多维度。比如，对于一个固定的模型架构，你可以给它输入更多数据；或者让模型更深，增加层数或参数量；再或者通过神经架构搜索去尝试不同结构的组合。我想如今大家已经比较明确哪种架构更有效，但在早期阶段，这方面的探索应该更具不确定性。能否谈谈当时你们是怎么思考这些问题的？你们的基础设施又是怎样支持这些探索和决策的？</p> <p>Nick Joseph：我想，简短的答案是——这真的很难。你要做的事情其实是训练一个庞大而昂贵的模型，而你面对的是一个包含上百个超参数的空间。比如你要决定层数、宽度等等，而你希望所有这些超参数都能最优。你需要在“它们到底重要到什么程度”之间取得平衡——也就是说，你能不能凭直觉选一个差不多的配置，然后只要增加算力就能得到不错的效果？</p> <p>Ankit Gupta：也就是说，没关系你怎么调都行？</p> <p>Nick Joseph：对，只要别太离谱。但有趣的是，实际上这并没有那么重要。我记得这在早期的扩展定律论文里也提到过——你可以调整这些参数获得一些小的提升，但只要你持续增加计算资源，模型的性能一般都会稳定地提升。当然，如果你调得太离谱，效果会停止提升，但你又不会知道到底出了什么问题。这其实是最难的部分之一。</p> <p>Ankit Gupta：因为你不知道反事实是什么，对吧？你没有足够长时间去跑出结果。</p> <p>Nick Joseph：对。我们有这些“扩展定律”，你可以预期，当你增加计算量时，损失值会按幂律下降——实际上是“幂律+常数”。所以最终你会看到这条曲线开始偏离幂律，这就意味着出问题了。而这时问题可能是根本性的，也可能只是你应该微调一下学习率。如何判断是哪一种，就是一大挑战。通常的做法是先在小规模下测试，再在大规模上运行。</p> <p>Ankit Gupta：这里的小规模是指数据规模，还是别的？</p> <p>Nick Joseph：是所有方面。你希望比例缩放，比如如果你要把计算量增加十倍，你就要有一套理论去指导：这十倍的算力该怎么分配——多少给层数、多少给数据、多少给注意力机制。然后在小规模下按比例缩放去验证。</p> <p>Ankit Gupta：那在Anthropic早期，你们团队可能十来个人的时候，作为一个小型但有资金的创业公司，你们当时能用到什么样的大规模基础设施？</p> <p>Nick Joseph：这其实是件挺疯狂的事。你永远不知道别人到底在做什么，但我们感觉自己就在最前沿。那时关心这件事的人其实很少。我当时的心态是：我们在做AGI，这是人类历史上最重要的技术。但我环顾四周，发现好像全世界只有30个人在认真干这件事。虽然我只是个初级工程师，但我惊讶于事情的“简单”——比如当时公开估算GPT-3的训练成本是500万美元。听起来多，但从公司角度看其实不算高。所以我们完全能买到足够的算力来训练类似规模的模型。</p> <p>Ankit Gupta：你们是用云计算，还是自建机房？</p> <p>Nick Joseph：我们用的是云服务，但其实差别不大。让我意外的是，我们真的得理解硬件的物理布局。有次同事甚至跑了聚类算法来推测芯片分布在哪些机房，因为我们怀疑不同机房间的网络延迟导致训练瓶颈。结果真能“反推”出来两个聚类，连接状况不同。这种极限优化在当时很重要——我们资金比别人少，只能靠算力效率取胜。</p> <p>Ankit Gupta：那你们具体做了什么来提升算力利用率？这听起来像早期Google那种“便宜硬件+极致优化”的故事。</p> <p>Nick Joseph：我们主要是把分布式框架调到极致。因为训练要跨大量GPU并行进行。分布式有多种模式：数据并行、流水线并行、模型并行等，要把这些都整合好。</p> <p>Ankit Gupta：那时候还没有现成的开源框架能直接用，对吧？</p> <p>Nick Joseph：对，有一些，但很有限。比如我们当时用的数据并行方法需要自己写all-reduce通信，不能完全依赖现有包。因为像PyTorch Distributed虽然有工具，但我们要扩展到比Facebook还大的规模，就得自己写，以便后续能灵活修改。</p> <p>Ankit Gupta：你刚说“我们要比Facebook更大规模”，这挺颠覆的。毕竟那时Facebook AI Research是最顶尖实验室之一。你们不觉得冒险吗？</p> <p>Nick Joseph：确实有点意外。但也许我有点自信过头吧。当时我觉得他们都忽略了重点。扩展定律已经很清楚了，而反对的论点听起来没道理。那篇原始论文横跨11个数量级，学界却争论它能不能再延伸一个数量级——我觉得，这种怀疑没什么根据。</p> <p>Ankit Gupta：毕竟都已经延展11个数量级了。</p> <p>Nick Joseph：对，所以我觉得继续下去的概率相当高。而且这类规律很多时候真的就是“直接有效”。但我能理解，从外部看就没那么显然——论文太多，每篇都说自己很重要。尤其像FAIR那种地方，研究者更独立，重视发表，而不是去协调一整个庞大的工程项目。训练一个大型语言模型需要几十人协作搭建复杂的基础设施，而那种成果不会是一篇论文，所以那类文化对这事其实并不重视。</p> <p>Ankit Gupta：你提到你们有时要修改PyTorch底层实现，那你们是停留在高层API还是直接写CUDA？</p> <p>Nick Joseph：看情况。大多数操作我停留在torch.matmul这种层面，不去管矩阵乘法内部实现。但像注意力机制这种复杂操作，我们会深入优化底层，因为它在GPU上效率很难调。</p> <p>Ankit Gupta：那你们会先纸上推导理论效率，再实现？</p> <p>Nick Joseph：对。你其实可以用纸笔算出理论上能达到的最大效率（MFU）。效率差的原因通常就是显存带宽瓶颈、CPU传输瓶颈等等。这些约束项不多，也就六七个。建模清楚后再实现。实现后用profiler分析每步耗时，对比预期，再不断优化。</p> <p>Ankit Gupta：那时有现成的分析工具吗？</p> <p>Nick Joseph：单GPU的Python profiler已经不错，但成百上千GPU的分布式Profiling几乎没人做过，我们得自己改写分析器来聚合所有追踪数据。</p> <p>Ankit Gupta：那你这些都是怎么学的？</p> <p>Nick Joseph：我入职Anthropic时，内部资料还少，我第一天就把整个内部Wiki都读完了。然后主要靠结对编程学，像Tom Brown、Sam McCandlish这些人之前都做过。我跟他们大量结对，看他们怎么调Profiler、怎么排错。比如我以前从没用过调试器，只靠print调试，后来才发现PDB调试器的效率高太多。</p> <p>Ankit Gupta：后来你们的预训练规模越来越大，算力暴涨——那策略上发生了什么变化？</p> <p>Nick Joseph：其实变化不大，真的令人惊讶。到今天我还在盯着和当年一样的指标——损失函数。可以把五年前第一个模型的loss曲线和现在的放在一起看，核心目标一直没变。</p> <p>Ankit Gupta：所以你们的OKR就是“loss越低越好”？</p> <p>Nick Joseph：对，当然现在的变化是团队更专业化。早期我会看每个PR，知道所有模块；后来各模块分工更细，有人专攻attention，有人专攻并行策略。但这样带来一个挑战：要平衡“专家”和“通才”。如果全是通才，大家懂点皮毛没人精通；全是专家，又容易出现系统割裂，需要管理者去把体系连起来。我个人更喜欢保持平衡。</p> <p>Ankit Gupta：那在算力规模暴涨后，有没有遇到一些意想不到的挑战？</p> <p>Nick Joseph：有，比如最典型的——“连接问题”。当你要并联越来越多的GPU时，最小的硬件故障都可能导致整个训练崩溃。比如一台GPU坏了，整个模型就会挂掉。你可以想象，如果模型每层在不同GPU上，那掉一个“第七层”的GPU就意味着整个网络失效。</p> <p>还有一点，整个技术栈都太新，从芯片架构到数据中心布局都在快速演进。以前我写代码出错会想“肯定是我写错了”，但现在有时真的是“电脑错了”。比如有次GPU真的坏了，换了之后一切正常。如今你得考虑的变量太多：GPU可能有瑕疵、供电可能波动、数据中心线路可能老化等等，对一个Python程序员来说，这些都是意料之外的复杂性。</p> <p>Ankit Gupta：那早期你们单次训练大概用多少GPU？</p> <p>Nick Joseph：上千个，能塞进一个机房。现在则是一整栋楼、甚至园区。那时我们还在研究：这些GPU是不是得放在同一个房间？带宽要多少？供电能不能扛得住？有时候只是一个电容不足，都可能导致整个训练任务瞬间断电崩溃。</p> <p>Ankit Gupta：那么，你们是否需要考虑不同类型芯片之间的差异？我的意思是，你们和各种不同的云服务提供商都有合作。从你的角度来看，这些芯片只是计算资源吗？还是说如果你们使用TPU和GPU，它们就像谷歌的CPU和英伟达的GPU那样？作为工程师，你在使用这两者进行训练时，是否需要采取不同的思路？</p> <p>Nick Joseph：是的，从根本上来说，它们做的都是同样的事情，对吧？都是在进行计算。进行同一类操作、矩阵乘法等等。它们实现的方式相当不同，编程方式也很不同。而且实际规格也差别很大。有些芯片可能拥有很多浮点运算能力，但内存不多；有些可能内存带宽大，但内存容量有限。所以拥有多种芯片在某些方面很有优势，这意味着你可以根据任务特点选择最合适的芯片来执行。</p> <p>Ankit Gupta：比如，有些任务更适合在TPU集群上执行，而另一些更适合在NBHP集群上？你们会怎么选择？你可以谈谈这个。</p> <p>Nick Joseph：有趣，我举个例子，推理任务通常需要更多的HBM带宽，因为你在一次时间步中需要加载每个token的所有权重，这意味着需要很高的HBM带宽。而预训练通常更依赖于浮点运算，因为批量更大。所以你可以针对不同用途选择不同芯片。但拥有多种芯片的缺点是，你可能需要为每种芯片重复编写代码。理论上可以通过抽象层来统一，但实际差异太大，很难做到。因此如果你处理所有工作负载和所有芯片，工作量会按芯片数量成倍增加。</p> <p>Ankit Gupta：关于你提到计算机有时会出故障，我记得你之前做过类似事情。我当时公司在使用Google TPU时遇到一些奇怪的segfault错误，你告诉我一个有效的方法，如果六个月前用上它们，就能解决一半的问题。我可以想象，你们在使用这些非常新的芯片时，会遇到很多问题，需要和提供商紧密合作来解决。</p> <p>Nick Joseph：是的，项目组在解决问题上很积极。有趣的是如何找到合适的合作方式。他们有强烈动机去修复问题，因为希望芯片能正常工作，从而未来能卖更多芯片。我们当然也有很强动机让芯片工作，因为我们提前大量采购这些芯片。所有工作都建立在让集群正常运行的基础上，但我们不一定能共享所有信息。有些信息不能完全共享。因此一种策略是制作小规模复现环境，当遇到问题时，你可以在单芯片或单文件上复现，然后发给对方进行调试。</p> <p>Ankit Gupta：你们是在共享的Slack上交流，互相发送问题和数据吗？还是说你们和大厂的人员实际上在同一个办公室？</p> <p>Nick Joseph：主要是共享Slack，有时候见面更有效，但Slack是常用方式。</p> <p>合成数据风险、模型评估指标设计与价值嵌入博弈 Ankit Gupta：那我们谈谈近期预训练的现状。这几年，各家公司对预训练的关注似乎有所分散，除了预训练，还有后训练的强化学习、微调和安全性调整等。从外部看，预训练似乎相对不那么受关注，而推理类模型的进展主要依赖后训练。你怎么看？这种理解是否合理？在推理和新型后训练方法的时代，有没有预训练层面仍然重要的因素，对实现优秀模型有影响？</p> <p>Nick Joseph：以前预训练意味着做一个大规模训练，但其实已经有变化，比如直接进行大量自由训练，把大部分计算资源用于训练。现在人们发现强化学习也能带来很大收益，你可以把更多算力投入强化学习，从而得到更好的模型。因此如何平衡预训练和后训练、各自作用叠加还是互补，这些问题仍在早期阶段，还没有定论。</p> <p>Ankit Gupta：你认为这些更多是经验性问题吗？像我们之前讨论的，你会尝试多种方法看效果，还是有一些基于原理的方式来判断？</p> <p>Nick Joseph：最终还是经验为主。可以提出理论，但实践中大多数理论都需要验证，多半是错的。所以最可靠的办法是收集数据再做判断。解决问题的经验方法对于做出正确决策至关重要，但在组织内很难实现。关键是不要因为你管理预训练，就坚持预训练必须胜出。</p> <p>Ankit Gupta：团队间是否存在某种竞争？还是视为同一个整体？</p> <p>Nick Joseph：我们这边合作性很强，基本上是共同产出一个模型。但据我了解，有些公司团队间存在摩擦，这是一个有趣的组织设计问题：如何设置团队，避免科学问题被个人团队观念绑架。</p> <p>Ankit Gupta：关于预训练，你如何看待高质量数据的可用性？你们已经训练了几乎所有互联网文本。外界常说数据已经枯竭，是否如此？尤其当大量数据由AI生成时，是否存在模式崩塌风险，模型会过拟合AI生成数据？</p> <p>Nick Joseph：我对数据问题看到很多自信的说法。有人说互联网数据已经用尽，我不确定实际情况。数据的质量与数量总是有权衡。基本事实是数据量巨大，增长速度慢于算力增长。</p> <p>Ankit Gupta：也就是说，虽然新数据在增加，但算力也在增加，不容易判断哪个增长更快。</p> <p>Nick Joseph：我需要强调，我并不完全确定。一方面互联网似乎无限，你可以不断生成文本。但“有用的互联网”规模无人知晓。</p> <p>Ankit Gupta：我脑子里简单想法是，用PageRank过滤出有价值的网页，不是可行吗？</p> <p>Nick Joseph：我认为不完全可行。人眼认为有价值的内容与模型学习所需的有用信息不完全相同。有些内容虽然链接少，但对模型可能很重要，尤其是处理难题的尾部知识。</p> <p>Ankit Gupta：这是原始Google的链接算法。</p> <p>Nick Joseph：对，它是质量指标，但不一定是最优指标。</p> <p>Ankit Gupta：AI的任务可能不同。</p> <p>Nick Joseph：尾部数据可能更有价值，可以通过蒸馏或智能模型生成数据训练新模型，接近原始模型的智能水平。</p> <p>Ankit Gupta：开放源代码模型中小模型蒸馏大模型的例子很多。</p> <p>Nick Joseph：完全可行。问题是，如果用现有模型生成数据训练更好模型，可能无法超越原模型，因为你只是学到了原模型的分布。如果分布有误，新模型也不会学到真实知识。</p> <p>Ankit Gupta：这是因为下一个token预测的损失对原模型生成内容很低，对吧？</p> <p>Nick Joseph：没错。模型只会学到原分布，如果原分布错误，就学不到真理。例如模型认为5+5=11，新模型也会学到11。这是一个研究难点，因为小规模研究难以直接扩展到大规模训练。还有一层问题是互联网上大量内容由LLM生成，其影响不易量化。1%的LLM内容，会浪费1%的算力，还是破坏5%-10%的模型表现？难以判断。</p> <p>Ankit Gupta：这不一定坏吧？如果训练目标是从当前分布向真实分布靠近，互联网上流通的数据本身可能有助于校正。</p> <p>Nick Joseph：你说的是通过互联网自然过滤，可能有效，但垃圾内容和恶意内容仍存在，这会影响模型。</p> <p>Ankit Gupta：你之前提到评估指标，除了模型本身，还有数据质量等指标。有没有可以量化的数据和模型质量指标，除了损失函数？</p> <p>Nick Joseph：损失函数其实很有效。理想的评估应关注实际关心的目标，而不是代理指标；评估需低噪声、快速易用。三点标准缺一不可，但第一点最难：明确你关心的到底是什么。</p> <p>Ankit Gupta：即使微小差异在评估中也要能体现出来，以便优化方向。</p> <p>Nick Joseph：对，比如GPT-4的MLA分数86.4%，下一代Gemini90%，差异明显，可用于判断优劣。评估需快速执行且易于操作。</p> <p>Ankit Gupta：例如AI医生任务，考试题可能轻松通过，但实际长时间与患者交流、提取关键信息更难，这类评估难度高。</p> <p>Nick Joseph：确实。创业公司可以做这些评估，而大实验室往往只优化标准化指标。医生场景中，我认为可用真实医生与患者的对话记录训练，预测对话文本，低噪声，模型可用于辅助诊疗。</p> <p>Ankit Gupta：这可以作为创业项目。外部讨论排列时，你能定义排列吗？它在预训练中如何体现？</p> <p>Nick Joseph：我们目标是AGI，即能完成大部分人类能做的事情。Sci-fi常被误导，人类一般想象一个机器人，而实际上应是大量智能体复制。关键问题是AI的目标是什么，目前下一token预测是目标，但人类目标不同。排列就是让模型目标与人类一致。</p> <p>Ankit Gupta：这与人类目标不同。</p> <p>Nick Joseph：是的。排列可从理论或经验角度解决，现有模型往往不符合期望。另一层是实际控制模型行为，比如通过宪法式规则或系统提示来约束模型交互方式。</p> <p>Ankit Gupta：系统提示类似提词，而非训练时调整。</p> <p>Nick Joseph：可以训练时加入，也可通过系统提示，取决于所需鲁棒性。</p> <p>Ankit Gupta：如何选择模型体现哪些价值？</p> <p>Nick Joseph：这是个难题。比喻为装方向盘，先获取控制权，再考虑驾驶者是谁。最好有民主化的价值设定，而非单人价值，以避免走向极权。</p> <p>Ankit Gupta：现阶段，你们如何实现排列？是通过后训练调整模型人格吗？</p> <p>Nick Joseph：大体正确。后训练迭代快，效果反馈快，适合调整模型。预训练用于基础科学探索，小模型无法有效模拟复杂行为。</p> <p>Ankit Gupta：必须在足够智能的模型上进行。</p> <p>Nick Joseph：对，但某些排列可以融入预训练，增强鲁棒性和智能性，例如让模型在学习智能的过程中融入人类反馈。</p> <p>Ankit Gupta：这如何在预训练中体现？</p> <p>Nick Joseph：可参考论文《Pretraining on Human Feedback》，将人类反馈特性融入预训练，观察效果。但缺点是灵活性下降，无法在后训练中快速调整。</p> <p>Ankit Gupta：你提到迭代速度关键，三个月与一天的差距巨大，可在短时间内尝试多种后训练策略并行执行。</p> <p>Nick Joseph：完全正确，自由训练本质困难，因为一次训练周期长且不可中断。</p> <hr/> <h2 id="五agi之路范式转移焦虑架构变革与未来展望">五、AGI之路：范式转移焦虑、架构变革与未来展望</h2> <h3 id="51-未来最大的挑战范式转变与隐藏的bug">5.1 未来最大的挑战：范式转变与隐藏的Bug</h3> <p><strong>Ankit Gupta</strong>：所以我现在在想，未来几年你们打算做的事情，作为团队，你们是如何考虑的呢？比如，你们会遇到哪些已知的问题，必须去应对？</p> <p><strong>Nick Joseph</strong>：我觉得我最关注的可能是<strong>范式的转变</strong>。我认为向强化学习的转变就是一个领域内的范式变化。我认为未来可能还会出现更多变化。很多人会争论，比如说，现有的范式是否足够让我们达到通用智能。我不知道，也许够，但我几乎可以肯定还会有新的范式。</p> <p>如果结果只是简单地扩大规模，没有任何意外，那会非常令人惊讶。但我真正感到最紧张的，其实是一些<strong>非常难以解决的bug</strong>。</p> <p>Ankit Gupta：这很有意思。</p> <p>Nick Joseph：是的，可能有点出乎我意料，但一个bug就可能让你搁置几个月。因为模型训练需要几个月的时间，一个小小的代码错误可能导致整代模型失效，而且难以被发现。机器学习本身就很难找到bug，但在大规模系统中，这类问题更难解决。</p> <p>Ankit Gupta：是啊，有时候甚至不知道从哪里下手。比如你写一个网络架构，漏掉了单元测试，或者没办法写测试去覆盖这种架构，你该怎么验证呢？</p> <p>Nick Joseph：你可以发送一个数据包，确认它正常传输，或者用小模型训练试一下，但即使是小模型，也不明显。</p> <p>Ankit Gupta：如果早期做ML的人遇到过经典bug，比如网络有10层，层7连到了层9，而不是8到9，模型依然可以训练，权重也在更新，但架构实际上是错的。这类bug非常隐蔽，很难发现。你说的这些随机bug，是这个意思吗？</p> <p>Nick Joseph：对，就是这种情况。随着系统复杂度上升，你可能在某个底层内核里用了错误的精度，这会让大规模模型崩掉。</p> <p>Ankit Gupta：训练一个月后才发现，甚至可能永远都发现不了。</p> <p>Nick Joseph：对，有时候甚至永远不会发现。代码量成万行，你怎么排查？所以最让我担心的就是这种微妙而棘手的bug。有时候你会看到模型崩溃或者训练速度突然下降，这类问题也非常难调试。我记得Nelson Nelha曾遇到过一个“诅咒”的bug，我遇到它时就觉得很棘手，于是把它交给别人，一个月后才松了一口气，自己可能永远解决不了。</p> <p>我觉得一个非常有价值的能力是可以深入到任何层面去分析问题，但这是一项罕见技能。我在Torch底层工作过，如果不了解CUDA，Torch底层出问题，我就无法自己解决。通讯方面也是一样，我可以发送数据，但如果底层网络协议有问题，我必须学整个领域才能理解数据包和TCP等细节。能从ML学习动态到字节级别全栈掌握的人非常少。</p> <p>Ankit Gupta：完全理解。那你们团队成员的背景是怎样分布的？外界可能认为你们全是写论文的博士研究员，但我觉得事实可能并非如此。</p> <p>Nick Joseph：是的，团队混合多种背景。我们最需要的几乎一直都是工程师。在这个领域的历史中，往往只要算力足够，模型就能工作，真正的挑战是如何正确实现。</p> <p>Ankit Gupta：行动力是关键，对吧？</p> <p>Nick Joseph：没错。实现正确并非ML问题，架构其实很简单，你甚至不必完全理解数学，只要实现正确即可。然后你面临的工程问题是如何在大规模上并行化、验证正确性。这种工程技能是关键，但与快速迭代网站等技能不同，更偏向于解决极难工程问题。</p> <p>Ankit Gupta：你们找的工程师是有类似Anthropic经验的人，还是学术出身的人？</p> <p>Nick Joseph：目前我们倾向于直接聘请有相关经验的人。早期我们会从各种背景挑选，但现在领域足够大，有经验的人可以直接上手。也有一些非常聪明、勤奋的人可以快速学习，比如一些理论物理学家，他们通过实习掌握编程，也能做出出色工作。</p> <p>Ankit Gupta：我想换个话题，谈谈未来你对AI领域其他方向的看法。你怎么看非下一步预测的方向？比如不是用Transformer架构，或者非自回归训练，有没有值得关注的？</p> <p>Nick Joseph：我觉得这些很有趣，但我不认为自回归是唯一路径。不过，自回归可能足够达到EGI。主要驱动还是规模和对基础科学的精细研究，而不是完全新颖的架构。确实存在更优新颖方法，但规模更容易、可靠性更高，而且仍有很大提升空间。</p> <p>Ankit Gupta：你花很多时间研究开源论文，看到一些中国实验室对架构做优化，比如缓存或高效注意力机制。你觉得这些优化是“加算力就行”的小改进，还是可能像Transformer一样带来革命性变化，必要时才能实现AGI？</p> <p>Nick Joseph：我认为是两者结合。我的猜测是，你会不断调整，随着算力增加，实验价值也会提升。同时还有推理优化，比如服务大量用户时做出更低成本、高效率的推理，这涉及推理堆栈和芯片细节。</p> <p>Ankit Gupta：预训练团队是否需要考虑推理问题，还是只管降低loss，然后交给别人处理？</p> <p>Nick Joseph：不，我们非常关注推理问题。我们提供的模型必须快速运行，否则用户体验会受影响。</p> <p>Ankit Gupta：能举例说明吗？</p> <p>Nick Joseph：最简单的例子是把模型做得过大，推理会变慢，或者增加不必要的通信步骤，使推理复杂化。这并非技术难题，而是架构设计问题。</p> <p>Ankit Gupta：明白。</p> <p>Nick Joseph：推理团队是我最常合作的团队，因为我们共同设计模型，使其既聪明又经济高效。有限算力下，优化推理是服务更多用户的关键。</p> <p>Ankit Gupta：如果算力无限呢？</p> <p>Nick Joseph：即便算力无限，挑战也在于如何利用它。大规模系统会遇到芯片故障等工程问题。无限算力能大幅提升速度和实验频率，但仍需解决复杂工程问题。现在的AI研究非常受制于算力，算力限制了模型训练和迭代速度。</p> <p>Ankit Gupta：你怎么看Diffusion方法，比如Gemini diffusion模型？在蛋白质设计等领域应用广泛，你觉得有发展潜力吗？</p> <p>Nick Joseph：坦白说，我们没做过图像生成，主要是扩散模型用途。我自己理解不够深入，但团队中有人可以更好回答。我觉得这类方法属于“不是范式性转变，但可能带来运行效率提升”的类别。</p> <p>Ankit Gupta：那么在近期，如果Anthropic每年持续改进模型，你认为创业公司有哪些机会？</p> <p>Nick Joseph：任何能受益于模型智能提升的方向都有机会。虽然大公司算力和资源更强，但他们做的是通用系统，我们的目标是让创业公司用这些模型完成具体工作。关键是利用当前模型基础上稍加努力就能落地的应用，但要注意不要过度依赖搭建复杂脚手架，因为下一代模型可能不需要。</p> <p>Ankit Gupta：在训练堆栈中，有没有问题是如果有公司解决，我们会直接购买产品的？</p> <p>Nick Joseph：有很多。比如芯片计算错误，如果有初创公司能检测每块芯片的精确性并报告故障，非常有用。还有组织管理方面，如果有服务能帮快速扩展团队，管理人员招聘和组织问题，也很有价值。未来更值得关注的是AGI普及后的社会影响，如何让其积极造福世界，而不仅仅是经济增长。</p> <p>Ankit Gupta：非常认同。最后一个问题，如果回到10年前，你还是学生，正从经济学转向AI，你会给学生什么建议，尤其是想进入你现在这种岗位的学生？</p> <p>Nick Joseph：时间点不同，策略也不同。如果回到10年前，我会专注AI，特别是工程技能，而不仅仅是数学或理论ML文献。同时会关注AGI相关应用，这是我认为最重要的两件事。</p> <hr/> <h2 id="结语">结语</h2> <p>Nick Joseph的访谈为我们提供了一个难得的窗口，深入了解Anthropic预训练团队的核心思考与实践。从AI安全初心到技术路线选择，从万卡集群的工程挑战到对齐问题的深层思考，这次对话展现了一个预训练团队负责人的完整视角。</p> <p><strong>核心要点总结：</strong></p> <ol> <li><strong>算力是王道</strong>：预训练的核心就是推动损失函数下降，扩展定律已被证明在11个数量级上有效</li> <li><strong>工程能力关键</strong>：正确实现比理论创新更重要，全栈能力是稀缺资源</li> <li><strong>预训练与后训练的平衡</strong>：如何平衡两者仍在早期探索阶段，强化学习带来了新的范式转变</li> <li><strong>推理与训练并重</strong>：预训练团队必须考虑推理问题，模型要既聪明又经济高效</li> <li><strong>未来挑战</strong>：范式转变和隐藏的Bug是最大威胁，AGI的社会影响值得关注</li> </ol> <p>正如Nick所说，现在AI研究最大的瓶颈是计算资源受限，而非算法突破。在这个领域，工程实践的价值远远超过纸面上的理论创新。对于想要进入这个领域的年轻人来说，培养扎实的工程技能，保持对AGI应用的关注，将是最重要的两件事。</p> <hr/> <p><strong>原视频：</strong> <a href="https://www.youtube.com/watch?v=YFeb3yAxtjE">Anthropic Head of Pretraining on Scaling Laws, Compute, and the Future of AI</a></p> <table> <tbody> <tr> <td><strong>编译：</strong> Zhenning Du</td> <td><strong>来源：</strong> Z Potentials</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[Nick Joseph访谈：Anthropic预训练的核心思考与实践]]></summary></entry><entry><title type="html">徐扬生院士：人工智能时代的教育</title><link href="https://emigmo.github.io/blog/2025/xuyangsheng-CUHK-SZ/" rel="alternate" type="text/html" title="徐扬生院士：人工智能时代的教育"/><published>2025-10-05T00:00:00+00:00</published><updated>2025-10-05T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/xuyangsheng-CUHK-SZ</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/xuyangsheng-CUHK-SZ/"><![CDATA[<h1 id="徐扬生院士人工智能时代的教育">徐扬生院士：人工智能时代的教育</h1> <p><em>演讲人：徐扬生（中国工程院院士、美国国家工程院外籍院士、香港中文大学（深圳）校长）</em><br/> <em>时间：2025年10月5日</em><br/> <em>地点：普通高中校长年会</em><br/> <em>来源：主旨报告</em></p> <hr/> <h2 id="目录">目录</h2> <ul> <li><a href="#人工智能时代的教育">人工智能时代的教育</a> <ul> <li><a href="#目录">目录</a></li> <li><a href="#引言教育的三个核心问题">引言：教育的三个核心问题</a></li> <li><a href="#一理解未来ai时代的本质特征">一、理解未来：AI时代的本质特征</a> <ul> <li><a href="#11-教育将受到最大冲击">1.1 教育将受到最大冲击</a></li> <li><a href="#12-人工智能的本质认知感知行动">1.2 人工智能的本质：认知、感知、行动</a></li> <li><a href="#13-人工智能与人类智能的七大差异">1.3 人工智能与人类智能的七大差异</a></li> <li><a href="#14-教育的真正重点比感情比创造比个性">1.4 教育的真正重点：比感情、比创造、比个性</a></li> </ul> </li> <li><a href="#二重新定义人才从会考试到真优秀">二、重新定义人才：从”会考试”到”真优秀”</a> <ul> <li><a href="#21-ai时代对人才的四项要求">2.1 AI时代对人才的四项要求</a></li> <li><a href="#22-当前教育的误区培养会考试的人">2.2 当前教育的误区：培养”会考试”的人</a></li> <li><a href="#23-真正优秀人才的四个维度">2.3 真正优秀人才的四个维度</a></li> </ul> </li> <li><a href="#三培养人才学思践悟">三、培养人才：学、思、践、悟</a> <ul> <li><a href="#31-学掌握学习的方法">3.1 学：掌握学习的方法</a></li> <li><a href="#32-思闲暇是思考的土壤">3.2 思：闲暇是思考的土壤</a></li> <li><a href="#33-践实践比学校更重要">3.3 践：实践比学校更重要</a></li> <li><a href="#34-悟事教人是教训">3.4 悟：事教人是教训</a></li> </ul> </li> <li><a href="#四教育改革的七个核心方向">四、教育改革的七个核心方向</a> <ul> <li><a href="#41-少教一点多育一点">4.1 少”教”一点，多”育”一点</a></li> <li><a href="#42-重理性轻记忆">4.2 重”理性”轻”记忆”</a></li> <li><a href="#43-增加实践环节践能生悟">4.3 增加实践环节：”践”能生”悟”</a></li> <li><a href="#44-培养创新人才学会提问">4.4 培养创新人才：学会提问</a></li> <li><a href="#45-加强艺术教育爱与美是生命支点">4.5 加强艺术教育：爱与美是生命支点</a></li> <li><a href="#46-文理融合跨学科发展">4.6 文理融合：跨学科发展</a></li> <li><a href="#47-观世界才能有世界观">4.7 观世界，才能有世界观</a></li> </ul> </li> <li><a href="#结语带着灵魂往前走">结语：带着灵魂往前走</a> <ul> <li><a href="#第一句话">第一句话</a></li> <li><a href="#第二句话">第二句话</a></li> </ul> </li> </ul> </li> </ul> <hr/> <h2 id="引言教育的三个核心问题">引言：教育的三个核心问题</h2> <p>今天现场有来自全国各地的中学校长，还有网上那么多关心教育事业的家长、同学、老师跟校长，我想把我这几十年来的一些思考跟大家分享。</p> <p>我做过中学老师，做过大学老师，做过大学校长，也教过大概三十几个国家的学生，所以有点感悟。同时我也在人工智能领域做了整整40年的研究工作，所以有一点点思考，跟大家分享。</p> <p><strong>教育是什么？</strong> 我把它定义为：<strong>教育是”为未来社会培养人才”</strong>。</p> <p>这里有两个关键词：</p> <ul> <li><strong>未来</strong>：未来到底是怎么样的？</li> <li><strong>人才</strong>：什么样的人算人才？</li> </ul> <p>由此引出第三个问题：<strong>怎么培养未来的人才？</strong></p> <p>今天就围绕这三个核心问题展开讨论。</p> <hr/> <h2 id="一理解未来ai时代的本质特征">一、理解未来：AI时代的本质特征</h2> <h3 id="11-教育将受到最大冲击">1.1 教育将受到最大冲击</h3> <p>教育，就是教书、育人。但我们需要重新审视这个定义。</p> <p><strong>学校还是教书最好的地方吗？</strong> 上个星期，我的一位美国朋友在硅谷跟我讨论这件事情。他说，教书，学校不是最好的地方。我非常吃惊，我说，哪里是最好的地方呢？他说你去看看网上，全是最好的课程，而且还都是免费的，为什么要把孩子送到学校来呢？</p> <p><strong>学校还是育人最好的地方吗？</strong> 在美国西部，有些家庭，大概十来个小孩，他们组成一个团体，做一个像小作坊这样的东西，来培养自己的孩子。</p> <blockquote> <p><strong>如果一个学校不知道教什么书，育什么样的人，那要我们学校干什么？</strong></p> </blockquote> <p>人工智能时代来了，<strong>最直接的冲击在哪里？教育。</strong></p> <p>为什么？因为人工智能可能是社会乃至人类历史上最重要的进展，整个社会要重构，社会阶层要重新划分，职业也会重新划分，会深深影响人类思想文明的走向，而教育就是为人类社会培养各阶层各领域的人才。</p> <h3 id="12-人工智能的本质认知感知行动">1.2 人工智能的本质：认知、感知、行动</h3> <p>人工智能是什么？实际上，<strong>人工智能就是仿照人的智能去做一些事情</strong>。</p> <p><strong>1. 认知</strong> — 怎么认识这个世界，怎么做判断。比如这是白衣服，白衣服大概是衬衣，衬衣大概是夏天穿的。这些都是判断、推理、决策。</p> <p><strong>2. 感知</strong> — 生活环境反馈给我们的信息。比如视觉、听觉，我们看到这个人穿白衣服，不是黑衣服。</p> <p><strong>3. 行动</strong> — 对感受到的世界有认知，有意愿去参与，就要付诸行动，比如走路、挥手。</p> <p>一个东西能够做到有认知的功能、感知的功能、行动的功能，合起来，我们叫作人工智能。</p> <p><strong>AI发展的现状：</strong></p> <ul> <li><strong>认知部分</strong>：尤其是大语言模型出来以后，做得很好，大家感觉真的是智能</li> <li><strong>感知部分</strong>：稍微差一点。比如打开一瓶茅台酒让它闻闻，这是不是酱香型的？它不一定懂</li> <li><strong>感情部分</strong>：更加不行</li> <li><strong>行动部分</strong>：进步很慢。我40年前开始做这件事情，到今天，进度非常之慢，目前状态跟二十几年前差不多</li> </ul> <p>现在都在讲具身智能，这个名字很好，也是我们提出来的。但很多人有点夸张。其实，<strong>现在的人工智能就是一个能说会道的残疾人</strong>。与人类共建社会是在将来，距离现在有多远，很难说。</p> <h3 id="13-人工智能与人类智能的七大差异">1.3 人工智能与人类智能的七大差异</h3> <p>很多人觉得，人工智能都比人聪明了，那人怎么办？朋友们，<strong>人工智能跟人的智能不是一回事</strong>。</p> <table> <thead> <tr> <th>维度</th> <th>人工智能</th> <th>人类智能</th> </tr> </thead> <tbody> <tr> <td><strong>知识与智慧</strong></td> <td>知道得越多越聪明</td> <td>知道得多不一定更聪明</td> </tr> <tr> <td><strong>时间取向</strong></td> <td>向后看（总结、整理）</td> <td>向前看（创造、预见）</td> </tr> <tr> <td><strong>智能分布</strong></td> <td>集中型智能</td> <td>分布性智能（全身都是智能体）</td> </tr> <tr> <td><strong>思维模式</strong></td> <td>重理性</td> <td>感性与理性结合</td> </tr> <tr> <td><strong>认知方式</strong></td> <td>重视分析与整合</td> <td>重视直觉与体验</td> </tr> <tr> <td><strong>心脑关系</strong></td> <td>重视脑</td> <td>心脑并用</td> </tr> <tr> <td><strong>个性通性</strong></td> <td>追求通性</td> <td>追求个性</td> </tr> </tbody> </table> <p><strong>一个重要的文化观察：</strong> 在中国文化里，”脑”这个字是月字旁，是身体的一部分，凡是身体一部分的字，都是月字旁，比如肝、脾、胃、胆。唯一一个例外是”心”。因为古人始终认为心不仅是人身体的一部分，”心”字有两点，一点在中心，是身体内的，另一点在旁，不在身体内。对人来说，心脑是并用的。</p> <p><strong>AI时代的危险：机器越来越像人，人越来越像机器</strong></p> <p>前一阵子，我让我的司机开车带我到广州一个地方见朋友。晚上11点，他一直开到一个大工地里面，下面全是水泥地，漆黑一片。我说这是什么地方？他说这是您叫我来的地方，GPS就导航到这里来了。</p> <blockquote> <p><strong>现在的人开车不去思考了，因为有GPS。人放弃思考是非常危险的，而思考是人区别于动物的唯一的东西。</strong></p> </blockquote> <p>人工智能时代的特征：</p> <ul> <li>获取知识的途径改变了</li> <li>整个社会将会趋于平庸</li> <li>10年以后，讲的话大家都差不多，个性没有了，观点没有了</li> </ul> <h3 id="14-教育的真正重点比感情比创造比个性">1.4 教育的真正重点：比感情、比创造、比个性</h3> <p>你不能跟AI比聪明，就像我们发明了汽车，我们不可以跟汽车比谁跑得快；我们也不能跟它比记忆，它记得比你好；我们跟它比知识，也比不过它。</p> <p><strong>那我们可以跟它比什么？</strong></p> <blockquote> <p><strong>比感情、比创造、比个性，这才是我们教育的真正重点。</strong></p> </blockquote> <h2 id="二重新定义人才从会考试到真优秀">二、重新定义人才：从”会考试”到”真优秀”</h2> <h3 id="21-ai时代对人才的四项要求">2.1 AI时代对人才的四项要求</h3> <p>人工智能时代对人才的要求：</p> <ol> <li><strong>领导力</strong> — 语言、沟通、判断、同理心</li> <li><strong>理性</strong> — 提问、分析、逻辑、批判性思维</li> <li><strong>创造力</strong> — 想象、艺术、探索能力</li> <li><strong>品性</strong> — 勇气、顽强、世界观、人文素养</li> </ol> <h3 id="22-当前教育的误区培养会考试的人">2.2 当前教育的误区：培养”会考试”的人</h3> <p>我分析了这几年的高考试卷，我们对人才的要求大概体现在高考卷子上面：</p> <table> <thead> <tr> <th>能力维度</th> <th>当前高考占比</th> <th>AI时代需求</th> </tr> </thead> <tbody> <tr> <td>记忆</td> <td>70%</td> <td>↓</td> </tr> <tr> <td>理性</td> <td>20%</td> <td>↑</td> </tr> <tr> <td>创造力</td> <td>5%</td> <td>↑↑</td> </tr> <tr> <td>品性</td> <td>5%</td> <td>↑↑</td> </tr> </tbody> </table> <p>不仅是文科的卷子，理科的卷子也是这样。化学、生物甚至物理卷子都基本上是这样，记忆占百分之六七十。</p> <blockquote> <p><strong>记忆好的人，高考成绩就好，而高考成绩好的，我们就叫他优秀人才——所以我们目前在培养的是会考试的人才。</strong></p> </blockquote> <h3 id="23-真正优秀人才的四个维度">2.3 真正优秀人才的四个维度</h3> <p>什么样的人才是真正优秀的？我总结了4条：</p> <ol> <li><strong>勤奋</strong> — 勤奋你才有动力</li> <li><strong>理性</strong> — 有主见，能思辨，能逻辑分析</li> <li><strong>创造性</strong> — 能创新，能突破</li> <li><strong>顽强</strong> — 能坚持，不放弃</li> </ol> <p><strong>中国学生的现状（我40年教育经验的观察）：</strong></p> <table> <thead> <tr> <th>能力</th> <th>中国学生表现</th> <th>对成功的重要性</th> </tr> </thead> <tbody> <tr> <td>勤奋</td> <td>★★★★☆</td> <td>15%</td> </tr> <tr> <td>理性</td> <td>★★★☆☆</td> <td>15%</td> </tr> <tr> <td>创造性</td> <td>★★☆☆☆</td> <td>30%</td> </tr> <tr> <td>顽强</td> <td>★☆☆☆☆</td> <td><strong>40%</strong></td> </tr> </tbody> </table> <p>很多人问我，你认识那么多聪明的人，优秀的人才有什么共同特点？<strong>一个非常明显的就是，他们无论在什么样的情况下都能够坚持下来。</strong></p> <p>我们中国内地的学生，勤奋是很好的，但顽强这点是最差的。我们看美国以及其他国家，比如印度、东南亚、欧洲跟南美一些国家，根据我的观察，他们的理性、勤奋可能不如我们，但他们能坚持，顽强性比较好。他们不怕批评，明天照样继续坚持做。</p> <h2 id="三培养人才学思践悟">三、培养人才：学、思、践、悟</h2> <p>我们到底该怎么来培养人才呢？我认为大概是这4个过程：<strong>学、思、践、悟</strong>。</p> <h3 id="31-学掌握学习的方法">3.1 学：掌握学习的方法</h3> <p>学是不容易的，要有好奇心，有兴趣。没有兴趣，学到最后还给老师了。我们学的东西是前人的东西。学习要专注，要学会学习方法。</p> <blockquote> <p><strong>Learn how to learn，这是最重要的。</strong></p> </blockquote> <p>因为大多数知识到最后，不是老师教的，是你自己学的。我的经验是，<strong>90%以上的知识都不是老师教授的，是你自己学的</strong>。所以要培养自己学习的能力，在学校要系统地教学生这样的能力。</p> <h3 id="32-思闲暇是思考的土壤">3.2 思：闲暇是思考的土壤</h3> <p>学完了以后你要能够去思考。有一位领导问我，怎么让我们的孩子们真正能够思考？我说，给他们点空余时间。</p> <p>他们晚上睡觉之前是否有一个小时是自由的时间？你去想一想，如果没有闲暇的时间，人怎么会思考呢？</p> <blockquote> <p><strong>闲暇的时间是思考的土壤，没有土壤你怎么能成长呢？</strong></p> </blockquote> <h3 id="33-践实践比学校更重要">3.3 践：实践比学校更重要</h3> <p>今天我会着重讲这个事情。<strong>人工智能教育本质上就是人的教育，实践的教育，创新的教育，如果不重视实践的话，根本谈不上人工智能教育。</strong></p> <p>人工智能时代，体验是重要的。优秀教育的效果主要看体验。实践本身就是一所学校，甚至实践比学校更重要。</p> <p>中国的同学欠缺就欠缺在实践这里，没有实践，那就把知识全部还给了老师：</p> <ul> <li>学，不是你的东西</li> <li>思考，无非你想过而已</li> <li>没有实践，光靠课堂教学，学到的就会全部还给老师，你悟不出来</li> </ul> <blockquote> <p><strong>因为只有践，才能生悟。</strong></p> </blockquote> <h3 id="34-悟事教人是教训">3.4 悟：事教人是教训</h3> <blockquote> <p><strong>人教人是知识，事教人是教训。</strong></p> </blockquote> <p>只有通过实践，你才能真正领悟在你一生中应该记住的东西。</p> <h2 id="四教育改革的七个核心方向">四、教育改革的七个核心方向</h2> <p>人工智能时代的教育改革，应该往以下几个核心方向走。</p> <h3 id="41-少教一点多育一点">4.1 少”教”一点，多”育”一点</h3> <p>我们要把重心放在育人上，而不在教书上。现在，我们全社会的人，包括你们，包括我，都应该问一下我们自己：<strong>我们是知道的太多了，还是知道的太少了？</strong></p> <p>每天晚上睡觉的时候我在想：我今天看了那么多东西，有多少是我应该看到的，有多少其实我不应该看到？我大概算了算，有60%是我不应该看到的，40%是我应该看到的。大多数人估计还达不到这个程度，估计80%～20%之间。</p> <blockquote> <p><strong>信息与知识把我们每一个人的脑袋占得太满了。</strong></p> </blockquote> <p>我的一个学生问我，你认识那么多有智慧的人，你告诉我这些有智慧的人跟我们一般的人有什么差别？我想来想去，有一个差别：<strong>他们知道得很少</strong>。</p> <ul> <li>因为知道很少，他们就能专注</li> <li>因为专注，他们就能出成果</li> </ul> <p>所以我们教的东西不用教得太多，你把核心的东西教给人家就可以了。</p> <p><strong>为什么读书越多越不容易创新？</strong></p> <p>这个问题我思考了13年，我把它想清楚了。因为人的脑袋是有容量的，你装东西都装满了以后，脑袋里面会有一个固定的”程式”，这个程式决定了，每当出现新的问题，他会习惯用已有的程式来处理这些新问题。换言之，<strong>他会把所有新的问题都当作这个程式能够处理的旧问题</strong>。</p> <p>举例：</p> <ul> <li>你问一个学生问题，他首先想到的是上网去找一找——他在假定人家已经问过这个问题了</li> <li>我带一个博士生，我跟他说某个课题应该怎么做。他第一时间就问我，教授，你告诉我，有没有参考文献？</li> </ul> <p>参考文献是什么？那是与这个课题有关的、人家已经做过的工作，当然应该知道点。但假如这个工作是你第一个开始做的，你就没有参考文献。</p> <p>朋友们，这不是小事情。<strong>任何一个人，如果你去问他一个事情，他都是假定人家已经碰到过这件事情，他就没有意愿去创新了。</strong></p> <p><strong>读书越多的三个负面效应：</strong></p> <ol> <li><strong>胆子越小</strong> — 了解太多案例，越小心谨慎，越害怕失败</li> <li><strong>大趋势越不清楚，小事情越清楚</strong> — 被细节困住</li> <li><strong>越来越成为小心翼翼的观察者，而不是勇敢的实践者</strong></li> </ol> <h3 id="42-重理性轻记忆">4.2 重”理性”轻”记忆”</h3> <p>理性其实比记忆重要得多。但是我们目前的教育是倾向于记忆，我们要把重心转移到理性的思辨能力的培养上来，要培养学生的科学思维、科学精神。</p> <p>我走了100所中学，我很清楚地告诉大家，有些中学做得蛮好的，在全世界的中学里都是好的，有些则不是。</p> <p><strong>中学需要做的：</strong></p> <ul> <li>加强理性分析的课程内容和考试要求</li> <li>加强对学生综合分析能力的培养</li> <li>数理的教学要<strong>提高深度，而不是广度</strong></li> <li>加强跨学科的课程教学</li> <li>加强学生理性分析辩论的兴趣小组</li> </ul> <h3 id="43-增加实践环节践能生悟">4.3 增加实践环节：”践”能生”悟”</h3> <p><strong>实践比学校重要。</strong> 人工智能教育，如果不重视实践的话，你最后要损失很多东西。我们要培养具有伟大格局的实践者。</p> <p>朋友们，你去看看，我们每天在看的网上的所有信息，都是在观察人家，看看A在说什么，B在做什么，C在做什么，哪个好哪个不好，在那里评论。</p> <blockquote> <p><strong>评论家很多，观察家很多，实践家很少。</strong></p> </blockquote> <p>体验是AI时代首要的教育重点。人教人是知识，事教人是教训。没有教训就没有实践经验，孩子长不大，所以我们的孩子普遍晚熟。</p> <p><strong>一个有趣的对比：</strong></p> <p>我们这个学校是国际大学，有很多国家的学生，各个国家学生跟我们国内的学生混在一起，你去看，他们总是比我们的孩子要成熟一点。我们的孩子，有什么事情都要跟爸爸妈妈商量一下，他们从来不是这样，可以自己做决定。</p> <blockquote> <p><strong>我们的孩子普遍晚熟，我们的家长普遍早熟。</strong></p> </blockquote> <p>小孩子还没长大，我们的家长就什么事情都给他想好了——”我这个孩子数学不行，以后不知道能不能到银行里面去工作？”他还是三年级的小学生啊，家长就这么想。</p> <p><strong>家长的早熟决定了孩子的晚熟</strong>，因为什么都给他想好了，孩子没有机会能自己做一些事情。</p> <p><strong>一个值得赞赏的案例：</strong> 我们有一个同事，他的孩子考上了全世界最好的大学之一。在上大学之前，他让孩子去非洲支教，很多人想不通。他跟我说，我举双手赞成，你这个孩子以后一定有出息。</p> <h3 id="44-培养创新人才学会提问">4.4 培养创新人才：学会提问</h3> <p>创新并不是你想创新就可以创新的。<strong>创新是一种文化</strong>，我们的社会，这种文化有没有呢？教育是这种文化的体现。</p> <p><strong>中国文化当中创新为什么比较困难？三个根源：</strong></p> <p><strong>1. 过于重实用</strong></p> <p>什么东西一出来，首先想有没有用。做个机器人出来了，人家说机器人干什么用？我做一个爬树机器人，他们就问爬树为什么要用机器人？</p> <p>实用当然是重要的，但你光想着实用价值，你就会停留在那个实用上面，就不会去深入了解后面的东西了。我们看到了烟花和炸药，不会想到它背后的化学，不会深入去研究，创新就被阻碍了。</p> <p><strong>2. 对传承的纠结</strong></p> <p>中国人一讲到创新就想到传承。我的感悟，世界上那么多创新，不是所有东西都是要通过传承的，有的是直接可以创新的。</p> <p><strong>3. 认识论上的不严格性</strong></p> <p>在我们中国文化当中，认识论是不严格的，而且非常严重。在西方的哲学里，包括苏格拉底，提出一个事情后会问你：为什么可以这样说呢？中国人是不讲的，不过庄子是个例外。这使人不会深究，创新就有困难了。</p> <blockquote> <p><strong>人工智能时代最大的挑战就是培养创新人才。在座的校长们，你们是中国的希望，中国的希望在10岁到30岁的人当中，而你们肩负着培养这一代人的任务。</strong></p> </blockquote> <p><strong>创新的三个条件：</strong></p> <ol> <li> <p><strong>不满</strong> — 鲁迅先生讲”不满是向上的车轮”，一个人对所有东西都满意的话，他还创什么新？创新就是打破格局。不满就是要提出问题来。</p> </li> <li> <p><strong>想象</strong> — 如果没有想象能力是无法创新的。想象是怎么来的？是跟艺术有关，跟跨学科有关。</p> </li> <li> <p><strong>自由</strong> — 要有面对现实，完全自由的、充分想象的能力。</p> </li> </ol> <blockquote> <p><strong>人工智能时代的教育要教我们的孩子学会提问，提问本身就是创新的一个元素。</strong></p> </blockquote> <h3 id="45-加强艺术教育爱与美是生命支点">4.5 加强艺术教育：爱与美是生命支点</h3> <p>我在不同的场合都讲过，<strong>艺术是了不起的</strong>。</p> <p>我走了100所中学，大概有25所中学是符合艺术教育方面的要求的，欧洲的学校、美国的学校做得比我们好。艺术教育的严重缺乏，将影响我们下一代的整体素质。</p> <blockquote> <p><strong>因为缺少爱是一个生命的缺陷，一个孩子，永远有一个生命的缺陷在那里，这是多么遗憾的一件事情。</strong></p> </blockquote> <p><strong>世界上有两样东西使我们的生活值得苟且，那就是爱与美。</strong></p> <p>这是生命的支点，生命退到最后退不过去的那点，而这两点，都跟艺术有关。如果不懂艺术的话，生命的支点就没有了，情感世界塌陷了。</p> <p>而在理性世界当中，<strong>艺术是创造力的源泉</strong>，所以艺术是很重要的东西。未来社会可能会有一半的生活跟艺术有关。</p> <h3 id="46-文理融合跨学科发展">4.6 文理融合：跨学科发展</h3> <p>这句话好像是跟大学讲的，但其实我是对中学讲的。<strong>文科跟理科是一个世界的两面，不是两个世界。</strong></p> <p>现在教孩子们是两个世界，所以造成了：</p> <ul> <li>理科生严重缺乏人文素养</li> <li>文科生的就业产生了很大的困难</li> </ul> <p>所以：</p> <ul> <li>要让我们的理科生能够欣赏文科</li> <li>要让文科生了解理科</li> <li>AI是不分文理、跨学科的</li> </ul> <blockquote> <p><strong>学校的目的是提供完整的教育，启发完整的人格。从这个意义上讲，文理是不应该分科的。</strong></p> </blockquote> <h3 id="47-观世界才能有世界观">4.7 观世界，才能有世界观</h3> <p>我跟你们讲一个小小的故事。以前我在香港教书的时候，有一群香港学生来跟我讲世界观。</p> <ul> <li>我问一个同学：你是哪里人？——我是番禺人。</li> <li>番禺是哪个省的？——不知道。</li> <li>旁边一个女同学说：我妈妈说我是中山人。</li> <li>你知道为什么那个地方叫中山吗？——不知道。</li> <li>去过中国内地吗？——回乡证里注着15年前去过。</li> </ul> <p>15年当中，中国内地发生了巨大的变化。我说那世界上你还去过什么地方呢？”我其实没去过世界什么地方，我就在沙田这一带。”</p> <blockquote> <p><strong>我说你下次跟我讲的时候，你就跟我讲”沙田观”，你不要讲”世界观”了。</strong></p> </blockquote> <p><strong>一个人的世界观很重要，但世界观怎么来？世界观是从观世界中来。</strong></p> <p>要让学生们：</p> <ul> <li>长见识</li> <li>有全球眼光</li> <li>有同理心</li> <li>有审视世界的能力</li> <li>用世界的眼光看中国</li> <li>更要用中国的精神来引导世界</li> </ul> <hr/> <h2 id="结语带着灵魂往前走">结语：带着灵魂往前走</h2> <p>总结一下我刚才讲的几个观点：</p> <blockquote> <p><strong>人工智能时代的教育，是人的教育，是实践的教育，是创新的教育。</strong></p> </blockquote> <p>最后我用两句话，来结束我的演讲。</p> <h3 id="第一句话">第一句话</h3> <blockquote> <p><strong>人类因为创造了人工智能而伟大，因为知道人工智能的局限而成熟。</strong></p> </blockquote> <p>创造了人工智能，可能是人类历史上最伟大的贡献。所有人，连我也不知道，后面可能会发生什么。以前我们所知道的是人是在进化的，生物是进化的，现在说地球也是在进化的，整个宇宙都是在进化的。</p> <p>人类是不是已经创造了人工智能？现在只是在中途，刚刚开始；人类是不是知道人工智能的局限，如何来面对这些局限性，都在考验人类的智慧和成熟。</p> <h3 id="第二句话">第二句话</h3> <blockquote> <p><strong>对世界文明的真正贡献不在于人口多少，不在于高楼大厦，不在于科技发展，是在这个国家和这个地区造就了什么样的人。</strong></p> </blockquote> <p>所以在座的各位任重道远，世界走得很快，<strong>要教育我们的孩子，带着灵魂往前走。</strong></p> <p>谢谢各位。</p> <hr/> <p><strong>注释：</strong> 本文根据中国工程院院士、美国国家工程院外籍院士、香港中文大学（深圳）校长徐扬生在普通高中校长年会（2025）上的主旨报告《人工智能时代的教育》整理而成。</p> <p>关注我，点击最上端的蓝字”徐扬生”或长按识别二维码关注</p> <p>Image</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[徐扬生院士：人工智能时代的教育]]></summary></entry><entry><title type="html">Andrej Karpathy深度对话：Agent的十年征程与AI的幽灵本质</title><link href="https://emigmo.github.io/blog/2025/karpathy-agent-ten-years/" rel="alternate" type="text/html" title="Andrej Karpathy深度对话：Agent的十年征程与AI的幽灵本质"/><published>2025-10-01T00:00:00+00:00</published><updated>2025-10-01T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/karpathy-agent-ten-years</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/karpathy-agent-ten-years/"><![CDATA[<h1 id="andrej-karpathy深度对话agent的十年征程与ai的幽灵本质">Andrej Karpathy深度对话：Agent的十年征程与AI的幽灵本质</h1> <p><em>访谈对象：Andrej Karpathy（OpenAI联合创始人、Eureka Labs创始人）</em><br/> <em>主持人：Dwarkesh Patel</em><br/> <em>时间：2025年10月</em><br/> <em>来源：Dwarkesh Patel播客访谈</em></p> <p><em>图片来源：Dwarkesh Patel</em></p> <hr/> <h2 id="前言">前言</h2> <p>Andrej Karpathy是OpenAI联合创始人之一，前特斯拉AI与自动驾驶视觉部门负责人，离开特斯拉后创立AI教育公司Eureka Labs。他于2015年在斯坦福大学获得博士学位，师从被誉为AI教母的李飞飞，主要研究领域为自然语言处理与计算机视觉的交叉领域，以及适用于这一任务的深度学习模型。他在2024年被时代杂志评选为在AI领域最有影响力的百名人物之一。</p> <p>在这次深度访谈中，Karpathy分享了对AI发展的深刻洞察，涵盖大语言模型的工作机制、Agent系统的演化潜力、强化学习与人类学习的本质差异，以及对模型坍缩、数据质量和未来教育的长期思考。</p> <h3 id="核心观点">核心观点</h3> <blockquote> <p><strong>“我们并不是在’造动物’，而是在’造幽灵’。这些系统更像是一种数字化的’灵体’或’意识碎片’，因为我们不是通过进化去训练它们，而是通过模仿人类、学习互联网数据的方式让它们成长。”</strong></p> </blockquote> <blockquote> <p><strong>“未来的研究方向之一，是学会减知识，去除部分信息，只保留一个我称之为’认知核心’的部分。那是一种被剥离了知识、却保留了智能算法与问题解决策略的存在。”</strong></p> </blockquote> <blockquote> <p><strong>“我觉得现在整个行业有点跳太快，把还不成熟的能力包装成革命性突破。我们现在处在一个中间阶段：模型惊人地强大，但依然有很长的路要走。”</strong></p> </blockquote> <blockquote> <p><strong>“我之所以认为’认知核心’至少要达到十亿参数规模，是因为训练数据本身太糟糕了。当你真正打开前沿实验室的预训练语料库，随机抽取一个互联网文档看看，几乎全是垃圾。”</strong></p> </blockquote> <blockquote> <p><strong>“让我担忧的不是’AI接管世界’这种情节，而是一个更现实的场景：我们会逐渐失去理解和控制的能力。因为这些系统会被层层叠加在社会的每个角落，理解其运行机制的人会越来越少。”</strong></p> </blockquote> <blockquote> <p><strong>“我认为在多智能体（multi-agent）领域有两个非常重要的方向，但这两件事目前都还没有被真正实现。第一个是’文化’，第二个是’自我博弈（self-play）’。”</strong></p> </blockquote> <hr/> <h2 id="目录">目录</h2> <ul> <li><a href="#一从alexnet到llmai刚走到agent的门口">一、从AlexNet到LLM：AI刚走到Agent的门口</a></li> <li><a href="#二从斑马到大语言模型生物的天生智能机器的后天模仿">二、从斑马到大语言模型：生物的天生智能，机器的后天模仿</a></li> <li><a href="#三写代码容易理解却难ai离成为能思考的程序员还有距离">三、写代码容易，理解却难：AI离成为能思考的程序员还有距离</a></li> <li><a href="#四强化学习是用吸管吮吸监督信号">四、强化学习是”用吸管吮吸监督信号”</a></li> <li><a href="#五未来十年ai不会变成人脑但会重建认知与生产架构">五、未来十年，AI不会变成”人脑”，但会重建认知与生产架构</a></li> <li><a href="#六multi-agent的下一步产生文化与自我博弈的能力">六、Multi-Agent的下一步：产生”文化”与”自我博弈”的能力</a></li> <li><a href="#七被低估的技术扩张难点从demo到规模化的难度">七、被低估的技术扩张难点：从Demo到规模化的难度</a></li> <li><a href="#八建立星际舰队学院用教育守住人类在ai时代的主动权">八、建立”星际舰队学院”：用教育守住人类在AI时代的主动权</a></li> </ul> <hr/> <h2 id="一从alexnet到llmai刚走到agent的门口">一、从AlexNet到LLM：AI刚走到Agent的门口</h2> <h3 id="11-agent需要十年而不是一年">1.1 Agent需要十年，而不是一年</h3> <p><strong>Dwarkesh Patel：</strong> 今天我邀请到了Andrej Karpathy。Andrej，你曾经说过，”这将是Agent的十年，而不是Agent的一年”。可以解释一下你为什么这么说吗？</p> <p>Andrej Karpathy：首先，非常感谢邀请，我很高兴能来聊这个话题。你刚提到那句“Agent的十年”，其实是我对行业内一个流行说法的回应。当时有些实验室在讨论LLM的发展时提出，“今年是Agent之年”。我听到这个说法时有些警觉，因为我觉得现在行业里存在一定程度的过度乐观。从我的角度来看，更准确的说法是“Agent的十年”。目前确实已经出现了一些令人印象深刻的早期Agent，比如Claude和Codex，我自己每天都在用。但距离它们真正成熟，还有很多工作要做。所以我的想法是，我们需要用十年的时间与这些系统并肩工作，它们会逐步变得更好，也会改变很多事情。这将是一个令人兴奋的过程。但就时间尺度而言，我只是想提醒大家，不该低估这条路的漫长。</p> <p>Dwarkesh Patel：你认为哪些问题需要十年才能真正解决？瓶颈在哪里？</p> <p>Andrej Karpathy： 我觉得，关键在于要让Agent真正“能用”。当我们谈论Agent时，无论是实验室的想法还是我个人的理解，都应该把它想象成一个你雇来一起工作的“员工”或“实习生”。你现在身边的团队成员在做一些具体的工作，那么什么时候你会愿意让Claude或Codex来接手？显然，目前还不行。原因很简单，它们现在还没有足够的智能，也不具备真正的多模态能力，无法熟练地使用电脑，也缺乏持续学习的机制。你无法告诉它们一件事并让它们永远记住。它们在认知层面上还很有限，离一个能真正帮你工作的智能体差得很远。要解决这些问题，大概需要一个十年的周期。</p> <p>Dwarkesh Patel： 有趣。作为一个AI的观察者，我可以指出现在的系统缺少持续学习或多模态能力，但我没法判断这些问题需要多长时间才能解决。比如说，持续学习要花五年？十年？还是五十年？你为什么觉得是“十年”，而不是“一年”或者“五十年”？</p> <p>Andrej Karpathy： 这其实更多来自我的直觉，也来自我在这个领域的经验。我从事AI研究大概十五年，虽然不算特别久，但也足以让我见过各种预测，以及它们最终的结果。我既在学术界做过研究，也在工业界待过，所以对行业节奏的体感比较直观。我的感觉是，这些问题是可以被解决的，但它们仍然非常困难。综合经验来看，十年左右是一个比较合理的时间范围。</p> <p>Dwarkesh Patel： 这让我很好奇。你能不能带我们回顾一下这些年AI的重要转折点？当时人们的情绪是怎样的？他们的预期是过度悲观还是过度乐观？</p> <p>Andrej Karpathy： 这是个很大的问题，因为过去十五年里AI经历了几次真正意义上的“地震”。那种地震般的变化，让整个领域的方向都突然发生了转变。我大概经历过两三次这样的时刻，也相信未来还会继续出现这样的转折，而且往往带着一种令人意外的规律性。</p> <p>我职业生涯的开始是在多伦多大学，碰巧就在Geoffrey Hinton身边。Hinton被认为是AI的奠基者之一，那时他正在训练各种神经网络，我觉得那是件极其有趣的事情，于是开始跟他学习。当时的深度学习其实还不是主流，只是AI边缘的小领域。直到AlexNet的出现，才算是第一个巨大的转折点。它让整个领域重新认识到神经网络的力量，几乎所有人都开始投入到这种方法中。</p> <p>不过那个阶段的研究仍然是“针对单一任务”的，比如图像分类或机器翻译。随后，研究者们开始思考：我们是否已经“搞定了视觉皮层”？如果是，那其他“大脑部分”呢？能不能构建出一个真正能与世界交互的完整智能体？2013年前后的Atari强化学习浪潮，在我看来，就是Agent早期探索的一个代表。那时的目标是让Agent不仅能“感知”，还能“行动”，在环境中获得奖励。当时的实验环境是Atari游戏。回头看，这其实是一条有些偏离的道路。包括我所在的OpenAI在内，当时整个行业的关注点都在强化学习、游戏和对抗任务上。那几年几乎所有人都在研究如何让AI通关游戏。但我始终对这种方向持保留态度，因为我觉得真正通向AGI的路径，不应该是游戏，而应该是现实世界的工作，比如一个能完成知识劳动的虚拟会计。</p> <p>因此，我在OpenAI负责的项目叫Universe，目标是让Agent能够用键盘和鼠标操作网页，在数字世界中完成任务。但事实证明，我们太早了。那时的系统几乎学不到任何东西，因为奖励信号太稀疏，只会在屏幕上乱点，浪费算力却得不到有效反馈。根本问题在于缺乏足够强的表示能力。</p> <p>今天我们看到的新一代会用电脑的Agent，其实正是建立在大语言模型之上的。必须先有语言模型，先拥有丰富的表征，才能在此基础上训练Agent。换句话说，当年我们太早追求全能Agent了，正确的路径是先解决表示学习，再实现智能体的行动能力。</p> <p>回顾过去十五年的AI发展，我觉得可以这样理解：最初大家在训练针对具体任务的神经网络；后来尝试构建早期Agent，但时机尚早；直到大型语言模型的出现，研究者才真正掌握了语言与认知的底层表征，才有了更现实的路径去叠加行动与交互能力。</p> <h2 id="二从斑马到大语言模型生物的天生智能机器的后天模仿">二、从斑马到大语言模型：生物的天生智能，机器的后天模仿</h2> <h3 id="21-进化vs预训练我们在造幽灵而不是造动物">2.1 进化vs预训练：我们在造幽灵，而不是造动物</h3> <p><strong>Dwarkesh Patel：</strong> 有意思。如果换个角度想，人类或动物在成长时是同时学习一切的。它们没有语言作为支撑，却能凭感官去理解世界。那为什么AGI不能像人类或动物那样，从零开始，通过感知去探索世界，而非依赖这种漫长的、上亿样本级的训练过程呢？</p> <p>Andrej Karpathy： 这是个非常好的问题。我也看过你邀请Richard Sutton的那期节目，并在那之后写过一篇短文，谈到我对这一话题的看法。我在文中提到，其实我对把AI类比为动物这件事一直非常谨慎，因为动物的形成过程来自一种完全不同的优化机制。动物是经过进化产生的，它们生来就带有大量“硬件”。我在那篇文章里举过一个例子——斑马。斑马出生后几分钟就能站起来奔跑、跟随母亲。这是极其复杂的行为，不是强化学习（reinforcement learning）能解释的，而是进化在基因层面“预设”的结果。显然，进化以某种方式把神经网络的权重编码进了ATCG（DNA碱基序列）中。我们并不知道这种机制具体如何运作，但它确实起作用。</p> <p>因此我认为，大脑的形成路径与我们正在构建的AI完全不同，我们并没有在运行同样的过程。正因如此，我在那篇文章里写道：我们并不是在“造动物”，而是在“造幽灵”。这些系统更像是一种数字化的“灵体”或“意识碎片”，因为我们不是通过进化去训练它们，而是通过模仿人类、学习互联网数据的方式让它们成长。于是，AI呈现出一种介于模仿与思维之间的幽灵式智能——它是完全数字化的，却又带着某种人类智能的投影。从智能空间的角度看，我们其实是从一个完全不同的起点出发的。</p> <p>当然，我也认为未来可以让这些系统更具“动物性”，这本身就是一个值得探索的方向。Sutton的框架其实可以概括为“我们要造动物”。如果那种方法真的能实现，那将非常了不起。想象一下，一个算法能自我运行并学习万物，那会是极其震撼的成果。只不过我个人对此持怀疑态度，因为我认为这样的算法也许并不存在，或者说，这并不是动物学习的方式。动物有一个外层循环，也就是进化。它在生命周期之外完成了大部分优化。而很多看起来像“学习”的过程，其实更多是大脑的成熟过程。动物真正意义上的强化学习可能只出现在运动控制等领域，而非智力层面的推理与决策。因此，我认为人类其实并不依赖强化学习来获得智能。</p> <p>Dwarkesh Patel： 你能重复一下最后一句吗？你是说很多强化学习其实只是运动相关的任务？</p> <p>Andrej Karpathy： 对，我认为强化学习更多用于类似运动技能的任务，比如投掷圈套、跑步、操控等。而在解决问题、推理等智力活动中，人类并不是通过强化学习来完成的。当然，这并不意味着强化学习不值得研究，只是我觉得它在智能的形成中作用有限。</p> <p>Dwarkesh Patel：让我整理一下，因为这里的信息量很大。我想确认自己的理解是否正确。你似乎在说，进化其实扮演的角色有点类似于预训练”，它建立了一种能理解世界的结构。不同之处在于，进化被“压缩”进人类三GB左右的DNA中，而这与模型的权重完全不同。人类大脑的权重并不存储在精子或卵子中，而是在发育中“长出来”的。DNA里的信息量远远不够描述每一个突触，所以进化更像是找到了学习的算法，而不是直接编码了知识。照你刚才的说法，也许这种生命周期内的学习并不等价于强化学习。这种理解与你的观点一致吗？</p> <p>Andrej Karpathy：我同意这种说法。显然，这个过程中发生了某种“奇迹般的压缩”。神经网络的权重并不直接存储在DNA的ATCG序列里，而是通过某种高度压缩的方式，把学习算法嵌入其中，再在生长过程中逐步展开。</p> <p>但我自己的出发点更务实一些。我并不是想造动物，而是想造有用的东西。我更像一个戴着安全帽的工程师，关心如何让技术落地。我们无法重现进化，因为没人知道怎么做。但我们确实能通过模仿人类和互联网数据，构建出这些幽灵式的数字智能。这种方式虽然粗糙，却能让AI获得某种类似进化所赋予的内在智能和知识结构。所以我常说，预训练其实就是一种“低配版的进化”。是我们现有技术条件下，能实现的、最现实的“演化途径”，用以让AI具备一个能够进一步学习和行动的起点。</p> <p>Dwarkesh Patel：我理解。让我从另一角度重新表达一下，也许能更清晰地对比两种观点。进化并不会直接给予知识，它提供的是寻找知识的算法。而预训练似乎直接传递了知识——这二者并不完全等价。或许可以这么理解：如果预训练帮助模型学会如何更好地学习，那它确实类似进化的过程；但如果只是直接灌输知识，那类比就不成立。你怎么看？</p> <p>Andrej Karpathy： 这是个很细微、但关键的区别。你指出的没错。预训练其实同时在做两件事。一方面，它在积累知识，学习互联网上的文本信息；另一方面，它也在变得聪明。当模型在预测下一个token时，它不仅仅是在记忆知识，而是在观察数据中的算法模式，从而在神经网络内部“点亮”一系列思维与学习的电路，比如上下文学习（in-context learning）等机制。</p> <p>事实上，我们甚至不一定需要那么多“知识”。我认为过多的知识反而可能成为神经网络进一步进化的束缚，因为它让模型过于依赖已有信息，难以跳出数据分布的边界。例如，当前的Agent往往难以在互联网上不存在的语境中进行推理或创造。如果它们的“知识”或“记忆”更少一些，或许反而能表现得更好。</p> <p>所以我认为，未来的研究方向之一，是学会减知识，去除部分信息，只保留一个我称之为“认知核心”的部分。那是一种被剥离了知识、却保留了智能算法与问题解决策略的存在，一个真正体现智能本质的核心。</p> <p>Dwarkesh Patel：这里的信息太有意思了。我们从上下文学习（in-context learning）开始谈起吧。这是一个看似显而易见但非常值得细想的点。模型最像有智能的时刻，往往发生在对话的上下文中。当我与它交谈，它能回应、能思考、能意识到错误并调整表达，这种“活的”智能几乎都是在上下文中显现的。而这种上下文学习能力，其实是通过预训练中的梯度下降自发形成的，也就是说，模型在训练过程中“元学习”出了这种能力。但上下文学习本身并不是梯度下降，就像人类的终身学习虽然受进化影响，却并非进化过程本身。我很好奇，这种类比在你看来成立吗？它会在哪里失效？</p> <p>Andrej Karpathy：我会谨慎地说上下文学习并非完全“不是梯度下降”。它确实不是显式的梯度下降，但我认为它在某种意义上依然在执行某种“微型优化”过程。上下文学习的本质是“模式补全”——在token窗口内完成序列模式。而互联网上存在着海量的模式，因此模型通过梯度下降学会了如何补全这些模式。这种能力被固化在神经网络的权重之中。换句话说，权重捕捉并延展了模式。</p> <p>有趣的是，在神经网络内部，似乎确实存在一种“自适应”机制。它在没有显式训练指令的情况下，自发地调整、适应输入模式。这听起来有些神奇，但它确实源自互联网的复杂数据分布。我看到过几篇研究论文探讨这种机制，其中有一篇让我印象深刻：研究者让模型通过上下文学习完成线性回归任务：输入是连续的XY数对，模型要预测下一个Y。结果显示，模型竟然真的学会了执行线性回归，而线性回归本质上就是反复计算误差并用梯度下降优化权重。</p> <p>更令人惊讶的是，研究者在分析模型权重后发现，其内部计算过程与梯度下降机制高度相似。甚至有研究者直接“硬编码”了一种神经网络结构，使其能通过attention层和内部运算执行梯度下降。换句话说，我们并不确切知道上下文学习在内部发生了什么，但它或许真的在执行某种隐含的、局部的梯度下降。</p> <p>Dwarkesh Patel：那么，如果上下文学习与预训练都在某种意义上实现了梯度下降，为何我们在前者中会强烈地感受到智能的生成，而在后者中却没有这种感觉？两者如果本质相似，区别又是什么？也许关键在于信息密度。以Llama 3为例，它在预训练中接触了约15万亿个token，而一个70B参数的模型，其参数信息量相当于每个token仅被“吸收”0.07比特的信息。相比之下，上下文学习的KV cache（键值缓存）在每增加一个token时会扩展约320KB。这意味着两者在“每个token吸收的信息量”上存在约3500万倍的差异。这是否说明上下文学习在信息层面更接近真实智能？</p> <p>Andrej Karpathy： 我基本同意这种看法。我通常会这样描述：模型在预训练阶段学到的知识，其实只是对训练数据的“模糊回忆”。这是因为压缩比太大了。我们把15万亿个token压缩进区区几十亿个参数中。这种剧烈的压缩必然导致信息丢失，所以我称之为“互联网文档的朦胧记忆”。</p> <p>而在模型运行阶段，也就是上下文学习时，所有输入token被装入上下文窗口，模型实时构建KV cache，这些信息在神经网络中是直接可访问的。因此我更愿意把KV cache类比为人类的工作记忆（working memory）。模型权重里的知识，就像我们对一年前读过的书的模糊印象；而上下文窗口中的信息，则是我们此刻脑海中的短期记忆。这种类比其实相当有启发性。比如，当你让模型总结一本书时，它可能只能凭模糊印象回答，但如果你把整章文字输入上下文，它的回答会立刻变得准确，因为那部分内容此时已被加载进模型的工作记忆。这正是上下文学习让我们感到智能鲜活的原因。</p> <p>Dwarkesh Patel： 回到更宏观的层面，在你看来，人类智能中有哪些核心部分是目前的模型最难以复制的？</p> <p>Andrej Karpathy： 几乎是所有方面。我知道这听起来夸张，但确实如此。我们偶然发现了transformer这种极为强大的神经网络结构，它几乎可以处理任何模态的数据：文字、音频、视频，都能识别模式、生成结构。它的通用性让我觉得，它在某种意义上就像大脑皮层的一部分。大脑皮层以高度可塑性著称。即便改变输入信号来源，神经元也能重新分配功能。比如曾有实验把视觉皮层重定向为听觉皮层，动物依然能适应。这让我认为transformer可能对应于某种“通用皮层组织”。而模型中的“推理与规划”模块，尤其是在构建reasoning traces（推理链）时，更像人脑的前额叶皮层。</p> <p>不过，大脑中还有许多结构我们尚未触及。比如基底神经节可能在我们使用强化学习微调模型时发挥了类似作用；但像负责记忆形成的海马体目前并没有明显对应的机制。小脑（cerebellum）或许与认知关系不大，但杏仁核（amygdala）等掌管情绪与本能反应的部分，在AI中几乎完全缺失。</p> <p>当然，我并不主张我们要照搬人脑结构去造机器。我依然是一名工程师，关注实用。但换个角度讲，如果你真的要“雇用”一个AI作为你的实习生，你会明显感觉到它还不够完整。因为它缺乏许多我们习以为常的认知结构与情感机制。从这个意义上说，我们距离把所有脑区都复刻出来还很远。</p> <p>Dwarkesh Patel：这其实和我们预测AI进展速度密切相关。有人认为，像持续学习（continual learning）这样的能力，未来也会像上下文学习那样自发出现——只要我们设计一个外层循环的强化学习框架，让模型在多个会话中被激励去“记忆”更久的内容，它自然就会发展出长期记忆或外部存储写入机制。你觉得这种假设现实吗？</p> <p>Andrej Karpathy： 我对那种说法其实不太认同。因为我觉得这些模型在每次启动时，当它的上下文窗口里没有任何token，它其实就像“重新醒来”，完全从零开始。因此，在这种视角下，我很难想象“持续学习”到底意味着什么。如果用人类作类比（尽管这类比并不完美，但很有启发性），我会说，当我清醒时，我在不断构建一个白天的上下文窗口，它记录了我一天中遇到的人和事。但当我入睡后，似乎发生了一些神奇的过程。我不认为醒来时还能保留完整的白天记忆，而是其中一部分被蒸馏进了大脑的权重。这就是睡眠的作用：它帮助我们把短期经验提炼成长期记忆。而在大语言模型中，这个阶段，也就是从上下文到权重的蒸馏，是完全缺失的。这正是它们无法实现持续学习（continual learning）的关键原因。</p> <p>人类在睡眠时会分析、反思、甚至在梦境中生成新的信息，从而将经验沉淀进神经权重；而模型目前并没有这样的机制。理论上，我们可以让模型在每次对话结束后，执行一种“知识蒸馏”过程，把当天的交互提炼成新数据，再通过微调（比如LoRA）更新部分稀疏权重。这样每个模型都能像人类一样拥有个性化记忆，而不必从头开始。</p> <p>当然，随着上下文窗口越来越长，我们也在探索新的方法来存储和访问这些记忆。例如，稀疏注意力（sparse attention）结构可以让模型在超长上下文中仍能高效聚焦关键信息。像最近发布的DeepSeek v3.2，就引入了这种稀疏注意力机制，使模型能处理极长的上下文。我觉得，我们正在以完全不同的方式，重新发明许多进化在人类大脑中已经完成的认知技巧。最终，我们或许会在认知结构上收敛到相似的架构。</p> <p>Dwarkesh Patel： 很有意思。那在你看来，十年之后我们仍会使用Transformer吗？只是它的注意力机制更复杂、更稀疏，MLP结构更轻量？</p> <p>Andrej Karpathy： 我喜欢用“时间平移等变性”这个思路来想这个问题。也就是，把今天平移到十年前看看差距。2015年我们主要在用卷积神经网络（CNN），ResNet刚刚出现。那时和今天相比，虽然有连续性，但也已经相当不同了。当时Transformer还不存在，更不用说如今各种结构改进了。所以我猜，十年后我们可能依然在用“大规模神经网络 + 前向传播 + 反向传播 + 梯度下降”这一基本范式，但形式上会有一些变化，规模更大，系统更高效。</p> <p>几年前我还做过一个有趣的实验：重现了Yann LeCun在1989年发表的卷积神经网络。那是已知最早用梯度下降训练的现代神经网络之一，用来识别手写数字。我试着用今天的技术“时间旅行”回去，看看如果只调整算法、数据和算力，会发生什么。结果非常有趣：只要把学习率调小一半，就能立刻让模型更稳定。但要进一步提升性能，我必须加入更多数据、训练更久、引入dropout和正则化。这让我意识到，AI的进步从来不是单一因素推动的，而是算法、数据、算力、系统优化的共同演进。</p> <p>未来十年，我们可能会拥有更好的硬件、更大的数据集、更高效的内核与软件，同时算法本身也会继续优化。这四个维度大概率会齐头并进，没有哪一个会独占驱动力。换句话说，AI的进化仍然是一个多维共振的过程。所以回答你的问题：我认为算法上肯定会有变化，但核心框架依然是“用梯度下降训练的巨大神经网络”。这是我最合理的猜测。</p> <p>Dwarkesh Patel： 想不到这一切进步加起来只让误差减少了一半，居然是30年的努力。</p> <h2 id="三写代码容易理解却难ai离成为能思考的程序员还有距离">三、写代码容易，理解却难：AI离成为能思考的程序员还有距离</h2> <h3 id="31-真正的挑战每一个9都要用同样的代价换来">3.1 真正的挑战：每一个”9”都要用同样的代价换来</h3> <p><strong>Andrej Karpathy：</strong> 减半其实已经很惊人了。但真正让我震惊的是，想让模型更好，几乎所有环节都得同时改进：网络结构、损失函数、正则化策略、数据规模、计算系统，没有一项可以掉队。这种整体提升的模式恐怕还会持续很久。</p> <p>Dwarkesh Patel： 我正想问一个类似的问题，关于你最近的NanoChat项目。因为你刚刚亲自从头编写了整个流程，从预训练到对话模型部署，对所有环节都有最新的体会。这个过程中有没有什么让你意外的发现？</p> <p>Andrej Karpathy： 我前几天刚把它开源发布。它的目标是提供一个最简洁但完整的端到端ChatGPT复刻模板。从数据预处理、模型训练到推理部署，整个流程都在一个仓库里呈现。过去几年我已经写过很多演示代码，分别解释每个环节的算法原理，而NanoChat是把这些零碎的部分串联起来，形成一条完整、可运行的管线。老实说，这个项目对我而言更像是一次系统性整理，而不是新的学习。我本来就知道整个流程该怎么做，只是想让它更干净、易懂，让别人能一眼看清一个聊天模型是如何被构建出来的。如果它能帮助更多人理解ChatGPT背后的工程逻辑，那就达到了我的目的。</p> <p>Dwarkesh Patel： 对于想通过这个项目学习的人来说，最好的方式是什么？是直接把所有代码删掉、自己从头实现一遍，还是在此基础上做一些修改？</p> <p>Andrej Karpathy：这是个好问题。整个NanoChat大概有八千行代码，涵盖了从数据到模型部署的完整流程。如果让我建议学习方式，我会这样做：假如你有两台显示器，把原代码放在右边，然后在左边自己从零开始写。可以参考，但不允许复制粘贴。只有当你亲手敲出每一行代码，才能真正掌握背后的逻辑。不过我也要说，这个仓库本身相当庞大。写这样一套系统的过程不是从头到尾顺序完成的，而是分块逐步搭建的。你在看成品仓库时，其实是看不到这种“构建节奏”的——不知道该从哪块开始、先实现什么。因此，仅仅提供最终代码还不够，理想情况下，还需要补充“构建过程”的讲解，比如视频或文档。我可能会在这周找时间做个视频来展示这个过程。</p> <p>总体来说，我建议大家自己从头构建，但不要复制粘贴。因为我认为知识分为两层：表层的理解和深层的掌握。而只有当你动手搭建时，才会暴露出那些“你以为懂了但其实没懂”的地方。动手是唯一能让你逼近真正理解的方式。正如费曼所说：“如果我不能自己造出来，那我就没有真正理解它。”我一直非常相信这一点。不要只写博客、不要只做PPT，去写代码、调逻辑、让它跑通。否则你只是“以为懂”，并没有真正掌握。</p> <p>Dwarkesh Patel：你在推特上提到，代码生成模型（coding models）在你编写NanoChat时其实帮不上什么忙，这让我很好奇，为什么？</p> <p>Andrej Karpathy： 整个仓库我大概花了一个多月完成。现在人们写代码的方式大致可以分为三类。第一类人完全不用LLM，全手写。我认为这种方式在今天已经不是最优解。第二类人——我自己就是这一类——依然主要手写代码，但会使用模型的自动补全功能。当你写出一部分时，模型会自动联想、填充后续内容，大多数情况下是正确的，少数情况需要修改。这种方式下，程序员依然是“架构师”，模型只是辅助。第三类是纯vibe coding，也就是输入一句指令：“Hi，请帮我实现XXX”，然后按下回车，让模型生成完整代码。这基本就是Agent模式。我认为Agent在某些特定场景下非常有用，比如处理模板化的样板代码（boilerplate code），尤其是那种在互联网上出现频率很高的代码片段，因为模型的训练语料中充满了类似样例。在这种场景下，LLM表现得非常好。</p> <p>但NanoChat完全不是这种情况。它是一个高度定制的项目，结构独特，逻辑密集，不存在可直接复用的模式。几乎每一部分都需要精确组织、细致推敲。这种类型的代码要求模型有极强的上下文理解和结构意识。而当前的代码生成模型在这方面仍然存在明显的认知缺陷。一个常见的问题是，模型会不断误会我在写什么。它记忆中过于丰富，总是试图把我正在写的代码套进它在互联网上学到的典型模板里，但我采用的结构恰恰与主流实现方式不同。它会执着地修正我，让代码回到它熟悉的风格，而这正是我不想要的。</p> <p>图片 图片来源：nanochat GitHub Repository</p> <p>Dwarkesh Patel：能举个具体例子吗？</p> <p>Andrej Karpathy： 比如，在NanoChat的训练过程中，我使用了八张GPU同时进行前向计算。通常情况下，梯度同步是通过PyTorch的分布式数据并行（Distributed Data Parallel, DDP）容器实现的。它在反向传播时会自动进行通信和梯度同步。但我并没有用DDP：不是因为不知道，而是因为没必要。我自己在优化器的step函数中写了一个自定义的同步例程。问题是，代码生成模型总是执着地让我使用DDP容器，它似乎“很担心”我没做同步。但我确实不需要它，因为我已经实现了自己的版本。这就说明一个问题：这些模型没法真正“理解”你的上下文和架构假设，它们只是套用它们所熟悉的模式。</p> <p>Dwarkesh Patel： 所以它们无法理解你其实已经有自己的实现。</p> <p>Andrej Karpathy： 对，完全无法。它们会一直卡在那里，疯狂往代码里加各种“防御性”逻辑，塞满try-catch语句，试图把项目变成一个企业级的生产系统。但我的代码是为教学和实验设计的，我明确知道哪些假设成立，所以这些防御性代码完全没必要。它们还经常误用过时API，让整个项目显得臃肿、混乱、不堪入目。虽然我能手动修复，但这显然没什么效率。</p> <p>另外，我也发现让模型写代码有一个令人烦躁的问题：沟通成本太高。我得用自然语言详细描述我要什么，这比直接在代码里打出几个字母还慢。比如我知道代码该写在哪个位置，只要输入前三个字符，自动补全功能立刻能给出我想要的内容。这种方式的信息带宽更高。指向具体位置，手动触发提示，比整段英文prompt更直接。</p> <p>不过我并不是完全不用模型。它们在某些场景下非常有帮助，比如自动生成报告，这种工作模板化程度高、风险低；我会用部分模型生成的字节码（bytecode）来辅助。而另一个例子是在我用Rust重写tokenizer时。Rust我并不算精通，所以当我有一个已经理解透彻的Python版本时，我会让模型帮我生成Rust实现，再配合测试验证效率。这样既安全又节省时间。换句话说，模型能降低你进入新语言或新范式的门槛。比如Rust的语法我还不够熟悉，但模型在这方面表现不错，就能帮我跨过学习初期的壁垒。</p> <p>Dwarkesh Patel： 我之所以觉得这个问题有意思，是因为很多人关于AI加速到“超级智能”的主流叙事，恰恰建立在“AI能自动改进AI”的假设上。想象一下，如果OpenAI或DeepMind内部有成千上万个像你这样的AI同时优化架构参数、尝试模型变体，岂不是指数级的突破？所以听你说AI其实在写原创代码时表现不佳，这一点对预测“AI爆发”的时间线非常关键。</p> <p>Andrej Karpathy：你说得对，这其实也是我为何认为“十年论”比“明年论”更现实的原因之一。我认为模型目前还不擅长写从未被写过的代码。而AI工程本身的核心任务，恰恰是写这种前所未有的东西。</p> <p>Dwarkesh Patel：听起来有点反直觉。毕竟你在NanoChat中用的结构改动，比如Rope Embedding之类的，其实在论文或开源仓库里都已经存在了。那模型为什么还做不好？</p> <p>Andrej Karpathy：它们“知道”，但只知道一半。模型可能了解这些技术的存在，却不知道如何把它们正确地融入到一个具体的代码库中：在我的风格、我的文件结构、我特定的假设下，实现方式就完全不同。它们无法在上下文中整合知识，也无法判断与现有实现的兼容性。</p> <p>当然，这方面正在不断改进。目前我常用的模型是GPT-5 Pro，它的表现已经非常强大。有时候我会花二十分钟，把整个仓库贴进去请它帮我分析某些问题。和一年前相比，它的回答已经非常令人惊喜。但整体上，距离真正“懂代码”还有明显差距。我觉得现在整个行业有点跳太快，把还不成熟的能力包装成革命性突破。其实很多生成代码的结果质量很一般，甚至可以说是半成品。我理解这背后有融资、市场叙事等因素，但实事求是地讲，我们现在处在一个中间阶段：模型惊人地强大，但依然有很长的路要走。对我来说，目前最理想的使用方式仍然是自动补全，而非全自动Agent。</p> <p>Dwarkesh Patel： 这点非常有意思。从编程史来看，每一次生产力的提升——不论是编译器、静态检查、还是更高层语言——都确实提高了开发效率，但并没有引发“智能爆炸”。这听起来很像你说的auto-complete，而不是“全自动程序员”。</p> <p>Andrej Karpathy： 是的，我也这么认为。我们正在经历的，更像是编译器的进化，而不是程序员的替代。我常常觉得很难明确区分“AI从哪里开始、又在哪里结束”，因为在我看来，AI本质上是计算机技术的自然延伸。如果我们回顾历史，其实从最早的编程工具开始，我们就在不断推动这种递归式的自我增强过程。</p> <p>从最初的代码编辑器、语法高亮、类型检查，到更复杂的调试工具、搜索引擎——这些都在不断提升程序员的效率。严格来说，搜索引擎也算AI吧？排名算法本身就是智能决策。事实上，Google在早期就曾称自己是AI公司，因为他们做的正是智能检索系统。我认为这完全合理。</p> <p>因此，我更倾向把AI视作一条连续谱，而不是一个突然出现的分界点。我们现在确实拥有更好的自动补全工具，也开始出现能自主循环的Agent系统，但这些Agent偶尔也会失控。整个过程其实是人类在逐步从底层实现中抽离，交由机器处理更多细节。比如我们早就不再手写汇编代码，因为编译器可以把C语言自动翻译成汇编。同样地，我们正在把更多能被自动化的部分交给机器。就像一个“自主化滑杆”（autonomy slider），每往右滑一点，机器替我们完成的工作就更多，我们则在抽象层级上不断“升高”。</p> <h2 id="四强化学习是用吸管吮吸监督信号">四、强化学习是”用吸管吮吸监督信号”</h2> <h3 id="41-强化学习的本质局限整条轨迹被一个比特牵着走">4.1 强化学习的本质局限：整条轨迹被一个比特牵着走</h3> <p><strong>Dwarkesh Patel：</strong> 我们来谈谈强化学习吧。你在这方面有一些非常有趣的见解。人类能通过与环境交互，建立起丰富的”世界模型”，而这个过程似乎与最终奖励几乎无关。比如一个人创业十年，最后才知道成败结果，但我们仍说他积累了经验与智慧。这种学习显然比单一的奖励信号要丰富得多。那么，在机器学习中，有没有对应的机制？</p> <p>Andrej Karpathy：我可能会直接说人类根本不是靠强化学习在学习。他们做的是完全不同的事。强化学习的效果其实比很多人想象的要糟糕得多。它之所以存在，只是因为以前的方式更差。以前我们只有模仿学习（imitation learning）。举个简单例子：假设你在做一道数学题。强化学习的方式是，你会在同一个问题上尝试上百种不同的解法：这个不行，换一个，再换一个……直到偶然得出正确答案。然后你翻开书后面的答案，看哪个尝试对了。强化学习的逻辑是：凡是得到正确结果的那几条路径，其所有步骤、所有token，全都被上调权重，意思是多做这样的。问题在于，这是一种非常嘈杂、误导性的信号。你可能在前面犯了很多错，最后才碰巧得出对的答案，但强化学习会把那整条路径都当成“好样本”。哪怕中间九成步骤都是错误的，它也会全部“up-weight”。这是纯粹的噪声。</p> <p>我喜欢用一个比喻来形容：强化学习就像是“用吸管吮吸监督信号”。你花了一分钟让模型执行一整条轨迹（rollout），最后得到一个单一的数字——正确或错误。然后你拿这一个比特的信息，去调整整条轨迹的所有参数。这几乎是荒谬的。人类的学习方式完全不同。人不会去做几百次试验，而是在找到答案后会主动回顾：“哪些地方做得好，哪些地方出错了，下次该怎么改。”这种反思性学习在现有的LLM中完全不存在。不过我看到了一些研究在尝试让模型具备这种能力，这说明整个领域已经意识到这一缺陷。</p> <p>回顾过去，模仿学习的出现本身就是一个奇迹。我们最初只有base model，只是纯粹的自动补全机器。后来出现了InstructGPT这篇论文，它让我大开眼界。作者发现：只要拿预训练模型，进行少量基于对话数据的微调，模型就能迅速“风格化”，变得像在与人对话一样自然，同时保留原有的知识结构。这种能力几乎是瞬间发生的，只经过几轮微调循环，模型就从补全文本变成了用户助手。当时我真的被震撼到了。这标志着LLM第一次学会理解语境并与人交流，而不只是预测下一个词。</p> <p>接着强化学习出现了。RL的确比单纯的模仿学习更进一步，因为它能基于奖励函数（reward function）进行爬坡优化（hill climb），不再完全依赖人类示范。对于有正确答案的问题，RL可以直接通过反馈信号不断改进，甚至找到人类没想到的解法。这是非常了不起的进步。然而，它依旧笨拙。缺乏结构化的反思与抽象机制。最近我看到Google有一篇论文，尝试引入反思与回顾（reflect and review）机制，好像叫Memory Bank之类的方向。这类研究越来越多。我认为我们正处在算法层面的一次重大更新前夜。我们可能还需要三到五次类似的突破，才能让LLM的学习方式更接近人类的认知机制。</p> <p>Dwarkesh Patel：你真的很擅长用生动的比喻。“用吸管吮吸监督信号”这句话太妙了。所以你的意思是，结果导向（outcome-based）的奖励机制最大的问题在于：整个学习轨迹可能非常长，但最终你只靠最后一个比特的信息来更新所有行为的学习信号。既然这个问题如此明显，为什么过程导向监督（process-based supervision）——即在学习过程中持续给出反馈——还没有成为更成功的替代方案？是什么阻碍了这种范式的落地？</p> <p>Andrej Karpathy： 所谓“过程监督”，就是你不是在任务结束后才给出奖励或惩罚，而是在执行的每一步都给予反馈。比如不是等十分钟后才告诉模型做得好或不好，而是每个环节都告诉它当前表现如何。听起来很合理，但真正的问题在于：如何自动化地分配部分奖励。当你有了完整答案时，可以直接做等式匹配——对错分明，非常容易实现。但在过程监督中，你得到的是部分解，系统必须判断这部分做得好不好，而这种判断既复杂又模糊。</p> <p>目前不少实验室尝试用LLM裁判（LLM judge）来解决，也就是让一个大模型来评估另一个模型的中间输出。你给它一个提示，比如：“看看这位学生的中间解答，假设最终答案是X，请评价他目前的表现。”然后通过调prompt，让裁判模型输出分数。听起来不错，但这里有个非常微妙却致命的问题：LLM裁判本身是一个拥有数十亿参数的庞大模型，而这种模型极易被投机取巧欺骗。一旦你基于它进行强化学习优化，模型几乎一定会找到它的对抗样本（adversarial examples），也就是那些能骗过裁判的无意义答案。在短期内——比如10步、20步迭代——这种方法还能有效。但如果你训练100步、1000步，模型就会找到各种漏洞，完全钻空子。</p> <p>举个具体例子，这个实验其实后来被公开过：我们用一个LLM裁判来给学生模型打分。模型训练得非常好，奖励值突然暴涨，看起来几乎“完美解决了所有数学问题”。但当我们去看生成结果时，发现完全是胡言乱语——前几步还像样，后面全变成了“the, the, the, the, the…”。你一看就懵了：这是什么？为什么能拿满分？结果发现，“the, the, the, the, the…”对裁判模型而言，是一个对抗样本。因为这串文字在它的训练数据中从未出现过，它无法判断这是无意义的，反而在“纯泛化空间”（pure generalization land）中给出了极高的置信分数，甚至打出了100%的奖励。</p> <p>Dwarkesh Patel： 换句话说，你其实是在训练一个prompt injection模型。</p> <p>Andrej Karpathy： 还不止那样。Prompt injection听起来太高端了。实际上，这只是模型在产生无意义却能欺骗评估者的输出。这些答案显然是错的，但裁判模型却认为它们“极佳”。这就是强化学习对抗的本质——它总能找到漏洞。</p> <p>Dwarkesh Patel： 如果这是强化学习目前的瓶颈，那要解决它，岂不是必须让LLM裁判变得更强、更鲁棒？我们是不是要像GAN那样生成模型对抗判别器，不断训练出更坚固的评估者？</p> <p>Andrej Karpathy： 是的，我相信各大实验室现在都在这么做。最直观的办法就是——好，那我们把“the, the, the, the, the”加进裁判模型的训练集中，并明确标注为0分，告诉它这不是好答案。这样可以缓解一部分问题。但问题在于对抗样本是无限的。每当你修复一批，下一批又会出现。即便你迭代十次、百次，模型依然能在庞大的参数空间里找到新漏洞。毕竟，这些LLM裁判可能拥有上万亿参数，搜索空间巨大。理论上，通过多次迭代确实会让攻击更难，但我并不确定它能彻底解决问题。我猜实验室们确实在努力做这些改进，但我的直觉是：我们仍然需要新的思路。仅靠修补评估器是不够的。</p> <p>Dwarkesh Patel： 很有意思。那你有没有想过，除了强化学习和过程监督之外，还有哪些新的思路可能突破这一瓶颈？</p> <p>Andrej Karpathy： 有一些方向我觉得值得探索，比如反思式训练（review-based training）——也就是让模型在完成任务后回顾自己的解答，生成新的合成样本（synthetic examples），再通过这些样本进行再训练，从而实现某种“元学习”。最近我看到一些论文开始尝试这个方向，不过目前我只读到摘要阶段。大多数还只是概念，还没有谁能在前沿级大模型的规模上真正让这种方法跑通。现在这些论文很多都还处于想法很酷，但实验结果嘈杂的阶段。当然，各大实验室都相对封闭，谁知道他们内部现在在干什么呢。</p> <p>Dwarkesh Patel： 我能理解用合成数据或合成任务去再训练模型，但人类好像还会做另一件事——比如睡眠或者白日梦。这两者不一定是在制造新问题，而更像是一种反思（reflection）。我在想，机器学习里有没有与“白日梦”或“睡眠”相对应的机制？</p> <p>Andrej Karpathy：我也觉得我们确实缺少这一层的机制。举个例子，当人类读书时，他们并不是在逐词学习，就像LLM那样预测下一个token。人类阅读更像是一种思维触发过程。书籍本身是提示（prompt）：它让你去联想、生成自己的思考，或者去和别人讨论。真正的学习发生在这种信息加工和内化的过程中，而不是对文字的逐句吸收。目前的大模型完全没有这种机制。它们在预训练阶段，只是无意识地预测下一个词。理想状态下，我希望未来的预训练可以加入一个反思阶段。模型在读完一段文本后，能主动思考它与已有知识的关系，进行内在整合，再基于此生成新的理解。这类过程在人类中对应的是睡眠中的记忆重组与抽象，但在模型里尚无对应机制。</p> <p>当然，要实现这点非常困难。比如你可能会问：为什么不能简单地让模型自己生成思考样本再训练？问题在于这些样本虽然看起来合理，但其实会让模型退化。这听起来反直觉，但原因在于模型的输出分布是塌缩的（collapsed distribution）。也就是说，模型生成的样本只覆盖了整个可能思想空间中极小的一部分。看单个输出你可能觉得没问题，但总体上，它的思维分布高度集中、缺乏多样性。举个直观的例子：打开ChatGPT，让它讲个笑话。你会发现，它几乎只会那三五个笑话。它的输出分布早已塌缩，不再包含人类那种丰富的随机性和创造性。而人类正相反：他们的思维有噪声，但正因为有噪声，才保留了巨大的熵（entropy）和发散性。也就是说，人类的思考虽然不完美，却保持了开放的探索空间。因此，如何让模型在进行合成数据生成时，既避免塌缩、又保持高熵多样性，才是真正的研究难题。</p> <p>Dwarkesh Patel：我理解了。也就是说，塌缩的问题会直接影响合成数据的价值，因为我们希望模型能生成超出原有数据分布的新问题或新反思，而不是一遍遍重复自己。</p> <p>Andrej Karpathy： 对，正是如此。比如我让模型读完一本书的一章，然后请它思考一下。它会给出一个看似不错的回答——语言流畅、逻辑清晰。但如果你让它重复十次，你会发现十次几乎一模一样。所以你无法通过反思次数叠加来获得真正的新见解。它并不会像人类那样，在重复的思考中生成新的联系或洞见。换句话说，目前的模型无法在同样的提示下越想越深，只能重复同样的想法。</p> <p>从单个样本看，大模型生成的内容往往看起来没问题，但整体分布却是糟糕的。而且糟糕到什么程度呢？如果你继续让模型在自己的输出上训练太久，它就会发生彻底的坍缩（collapse）。我甚至觉得，这种现象可能没有真正的根本解决方案。更有趣的是，人类其实也会坍缩。这个类比听起来令人惊讶，但我认为它非常贴切。人类在一生中也会逐渐过拟合。孩子之所以让人惊叹，是因为他们还没有过拟合：他们的世界模型尚未被“训练死”。他们会说出一些让成年人震惊的话。听起来不合常理，却又在逻辑上有迹可循。那正是“未坍缩的思维”。而我们成年人，思维早已坍缩。我们不断重复自己，思考的路径越来越窄，学习率越来越低。时间越久，这种坍缩就越严重，直到一切都钝化。</p> <p>Dwarkesh Patel： 我看过一篇非常有趣的论文，它提出梦境其实是防止这种过拟合与坍缩的机制。梦的进化意义在于让我们进入那些极度陌生、非现实的场景，从而避免认知模型对日常经验的过度拟合。</p> <p>Andrej Karpathy：这是个很有意思的观点。确实，当你在脑中生成场景、并对它们进行注意（attend）时，本质上就是在训练自己的合成样本。如果这种自我训练持续太久，人也会跑偏，陷入自己的闭环，最终坍缩。因此，人类必须主动去寻求熵（entropy）。与他人交谈，就是获取外部熵的重要方式。它打破了我们内部的思维惯性。或许，大脑确实在进化中形成了一些增加内部熵的机制，用以打破这种自我坍缩的趋势。梦境，也许正是其中之一。</p> <p>Dwarkesh Patel： 我有个还没完全成型的想法，想请你聊聊。我们已知最好的学习者，也就是孩子，其实在记忆方面能力极差。人生早期的所有经历，他们几乎全部遗忘。但他们却能以惊人的速度学习语言、理解世界。相反，在另一个极端，像LLM这样的模型，在复述事实上无与伦比。它们可以逐字背出维基百科的段落，但在抽象学习与模式迁移方面，却远不如人类儿童。成年人则介于两者之间：他们记忆力较强，却失去了儿童那种灵活泛化的能力。也许这里面隐藏着某种深层机制？</p> <p>Andrej Karpathy： 我完全同意，这确实非常有趣。人类与LLM相比，的确更具“见林不见树”的能力。我们不擅长记忆，反而是一种优势。正因为我们无法逐字记忆，才迫使我们去抓取更高层次的模式。</p> <p>而LLM恰恰相反。它们是极端的记忆机器。它们能精准复现训练语料的文本。你甚至可以喂给它一串完全无意义的随机字符，只训练一两次，它就能原样背出。这是人类绝不可能做到的。但人类无法背诵随机字符串，反而是一种“认知特性”：我们被迫去提炼抽象规律，而不是死记细节。而LLM则被它们的完美记忆所困——这些海量记忆反而成为干扰，使它们难以形成真正的理解。</p> <p>这就是我之前提到的认知核心（cognitive core）概念。我希望未来的模型能减少这类记忆负担，让它们不再存储事实本身，而是只保留思考、推理与实验的算法结构。当模型需要知识时，它可以去查，而不是去背。</p> <p>Dwarkesh Patel： 这似乎也与防止模型坍缩有关？</p> <p>Andrej Karpathy： 我觉得这两个问题其实是不同维度的。记忆与坍缩确实都在影响智能系统的泛化能力，但机制不同。LLM的问题在于，它们太擅长记忆，以至于失去了探索空间。而人类在记忆上笨拙，反而保持了创造力和开放性：这是一个非常好的特性。</p> <p>Dwarkesh Patel： 那模型坍缩有没有什么解决方案？我能想到一些很朴素的思路，比如让模型的分布更宽一点、多样性更高一点之类的。但这些直觉式的方案通常会遇到什么问题？</p> <p>Andrej Karpathy：这是个很好的问题。理论上，你可以为模型加一个“熵正则化项”（entropy regularization），去鼓励输出分布更广、更具多样性。但从经验上看，这些方法效果都不太理想。一个核心原因是：我们目前要求模型执行的任务，本身就不需要太多多样性。这或许是最本质的答案。前沿实验室主要目标是让模型变得实用。从实用角度看，多样性其实反而会带来麻烦：输出更难评估，行为更不可控，价值密度反而下降。所以在很多任务上，输出的多样性被主动压制。</p> <p>Dwarkesh Patel： 对，而且在强化学习里，“过于有创意”其实还会被惩罚，对吧？</p> <p>Andrej Karpathy： 对，比如让LLM帮人写作。如果模型太有创意，往往反而是坏事。因为它会偏离你想要的内容，产出表面多样但本质相似的东西。也就是说，它的回答看起来不同，实际上都来自同一个坍缩的模式。我觉得问题的关键是：当前很多任务不要求多样性，所以模型也就不具备它。但从长远看，这反而在合成数据生成（synthetic generation）阶段成为瓶颈。我们在某种意义上亲手切断了模型的熵来源。我认为实验室应该在这方面做得更积极一些，保留并利用这种“熵”。</p> <p>Dwarkesh Patel：你刚才提到这可能是一个根本性问题，不太容易解决。你能具体解释一下你的直觉吗？</p> <p>Andrej Karpathy：我不确定这是不是根本性问题，也许我刚才说得太重了。我自己没做过大规模实验，但我确实认为，可以尝试用一些方法去提高输出熵，让模型产生更多解答。但问题在于如果你让它太“自由”，模型就会偏离训练分布，开始自创语言或使用极少见的词汇，输出就会变得不可控。换句话说，这是一场分布控制的平衡博弈：既要让模型发散，又不能让它漂移得太远。这件事不算不可能，但确实复杂，绝对不是调个超参就能解决的。</p> <p>Dwarkesh Patel：那如果要你大胆猜一下，你认为智能的最小核心（the optimal cognitive core）应该有多大？假设我们要把它放到一个冯·诺依曼探针（von Neumann probe）里，它需要多少比特？</p> <p>Andrej Karpathy：这个问题很有趣。回顾AI发展的历史，过去一度流行“scale-pilled”思维，也就是一切问题都靠更大的模型解决。我们造出了上万亿参数的模型（trillion-parameter models），但现在又出现了逆趋势：模型开始变小。我认为这些超大模型记忆太多、泛化太少。其实，我曾经提出过一个预测：我们最终可能可以把“认知核心”压缩到十亿参数级（~1 B），就能具备非常强的思考与推理能力。这样的模型可能无法直接回答百科式事实问题，但它知道自己不知道，并会主动去查。这就是智能的真正标志。它不再是“记忆体”，而是“思考体”。</p> <p>Dwarkesh Patel：但这挺出乎意料的。毕竟我们现在已经有十亿参数级的模型，它们看起来就很聪明了。</p> <p>Andrej Karpathy： 是的，但别忘了，目前的主流模型动辄上万亿参数。</p> <p>Dwarkesh Patel： 没错，但那些模型主要是在记忆而非“思考”。而以现在的进展速度，像GPT-OSS 20B这样的新模型已经比原版GPT-4（上万亿参数）更强了。照这个趋势发展，十年后“认知核心”只需要十亿参数？这也太保守了吧。难道不会更小，比如几千万、甚至几百万参数？</p> <p>Andrej Karpathy：不，我之所以认为“认知核心”至少要达到十亿参数规模，是因为训练数据本身太糟糕了。问题的根源在这里。我们今天训练模型所用的语料，是整个互联网。而互联网本身就是一个充满噪声的语料库。你以为的互联网，也许是《华尔街日报》那样的高质量内容，但现实完全不是这样。当你真正打开前沿实验室的预训练语料库，随机抽取一个互联网文档看看，几乎全是垃圾。要么是股票代码，要么是论坛碎片，或者毫无上下文的乱语。像《华尔街日报》这种高质量文本在整个语料中几乎微不足道。</p> <p>所以问题是：我们被迫用极差的数据去训练极大的模型。这些模型的大部分算力，其实都浪费在“压缩垃圾”上。它们的工作更多是记忆性压缩（memory work），而不是认知性学习（cognitive work）。理想情况下，我们希望保留的，是认知部分，而不是记忆部分。因此我们需要智能模型来帮助我们清洗、筛选预训练语料，把它提炼成真正有助于认知学习的数据。这样一来，模型规模自然可以变小。不过这类精炼数据集不会直接用于训练，而是会通过“蒸馏”（distillation）。从更大、更强的模型中提取精华，再用来训练小模型。</p> <p>Dwarkesh Patel：但我好奇的是既然数据都被蒸馏过了，为什么最终的蒸馏模型还要有十亿参数？</p> <p>Andrej Karpathy： 因为蒸馏的过程本身就非常有效。事实上，几乎所有小模型都是蒸馏而来的。没人会直接拿原始互联网去训练一个小模型。</p> <p>Dwarkesh Patel： 我明白。但为什么你觉得十年后，这个蒸馏后的“认知核心”还停留在十亿参数，而不会更小？</p> <p>Andrej Karpathy： 我认为再怎么压缩，智能至少需要“十亿个旋钮”（a billion knobs）才能做出点真正有趣的事。你希望它更小？</p> <h2 id="五未来十年ai不会变成人脑但会重建认知与生产架构">五、未来十年，AI不会变成”人脑”，但会重建认知与生产架构</h2> <h3 id="51-认知核心十亿参数就够了">5.1 认知核心：十亿参数就够了</h3> <p><strong>Dwarkesh Patel：</strong>是啊，从过去几年的趋势来看，模型规模已经在快速下探。我们从上万亿参数降到几十亿、二十亿，而且性能反而提升了。照这个趋势下去，智能核心或许还能继续缩小。就像费曼说的那句话，底部还有充足的空间。</p> <p>Andrej Karpathy： 哈，我本来以为我提出“十亿参数认知核心”已经够反主流了，没想到你还比我更激进。也许你说得对，也许我们还能再小一些。但我仍然觉得模型必须保留一定的知识基线（curriculum of knowledge）。它不需要百科式的细节知识，但也不能什么都去外部查。否则它会陷入认知分页状态。就像你思考时不停查资料，而不是在脑中推演。一个理想的智能体需要一定程度的内部知识缓冲，但不必记得太多“冷门事实”。</p> <p>Dwarkesh Patel： 所以我们刚讨论的，是“认知核心”可能的合理规模。那另一件事是：你怎么看未来前沿模型（frontier models）的总体规模趋势？过去几年，我们经历了从GPT-3到GPT-4.5的快速扩张，现在又出现了缩减或平台期。你认为接下来会怎样？模型还会变大，还是会继续变小？</p> <p>Andrej Karpathy： 我没有特别确定的预测，但我认为实验室们正在变得更务实。他们正在约束预算。事实证明，把算力和成本都砸在预训练阶段，并不是最划算的选择。这就是为什么新一代模型反而更小。预训练阶段变轻了。但他们会在中后期，比如强化学习（RL）或中间训练（mid-training）阶段，把算力“补回来”。换句话说，他们在调整投资结构。不是一味堆参数，而是在不同阶段寻找性价比最优的配置。所以我很难对趋势做精确预测，但我的直觉是人类对“更大模型的渴望不会消失。这几乎是AI发展的宿命。只是我们该怎么“变大”与“变聪明”，未来可能不再是同一件事。</p> <p>Dwarkesh Patel： 你认为接下来几年的进展，会延续过去两到五年的那种节奏吗？比如从NanoGPT到NanoChat之间的那些架构调整（architectural tweaks），是否就代表了未来的创新方向？还是你预期会出现更大的范式转变？</p> <p>Andrej Karpathy：我认为未来最大的变化其实会来自数据集质量的跃升。现在的数据集坦白说——糟糕透顶。糟糕到我甚至都不太明白模型是怎么在这种条件下学会任何东西的。如果你真的去看一个随机的训练样本，会发现里面充满了事实错误、逻辑混乱、无意义的内容。但当你把规模放得足够大时，这些噪声会被平均掉，剩下的信号就浮现出来。这其实是“统计规模的魔法”，而不是数据质量本身的功劳。</p> <p>所以未来数据集会变好很多。同时，硬件也在持续改进。NVIDIA 正在不断优化底层硬件，比如 TensorCores；配套的 kernel 算法也会继续被调优，以最大化算力利用率。算法层面同样会演进，从优化器到架构，再到训练流程中的每个组成部分，都会慢慢变得更高效、更统一。</p> <p>我过去十几年的经验是：没有哪一项单独的突破主导全部进步，而是每个环节都在稳步提升，大约“各项都+20%”，累积起来便是质变。</p> <p>Dwarkesh Patel： 有人提出过用不同方式来刻画我们距离“AGI”的进度，比如用“教育阶段”来比喻：从高中生，到大学本科生，最后再到博士。</p> <p>Andrej Karpathy：我不太喜欢这种说法。</p> <p>Dwarkesh Patel：也有人用“任务时间跨度”（horizon length）来定义：现在的AI能自主完成一分钟的任务，未来能完成一小时、一周、甚至更长期的任务。你认为用什么维度来衡量AI的进步才更合适？</p> <p>Andrej Karpathy： 我有两个回答。首先，我几乎想直接拒绝这个问题本身。因为我一直把AI视为计算机科学的自然延伸。那你要怎么量化计算的进步呢？自上世纪70年代以来，计算机的发展并没有统一的“Y轴”，对吧？从这个角度看，AI进展曲线这个提法本身就有点滑稽。</p> <p>不过，如果我们回到最早的定义。OpenAI刚成立时，我们对AGI的定义是：“一个系统能以人类水平或更高水平完成任何具备经济价值的任务。” 我其实一直坚持这个定义。后来人们陆续简化了它。比如去掉“物理任务”部分，只谈“数字化知识工作”（digital knowledge work）。这是一个重大让步。毕竟原始定义里包括人类能做的一切，比如搬运、操作、感知等。现在的AI显然还做不到。但即便只限定在知识工作，这个市场仍然巨大。我估计知识工作占整个经济体的10%到20%，单在美国，这也意味着数万亿美元的价值空间。</p> <p>Dwarkesh Patel：那按照这个定义，现在AI在多大程度上达到了经济可替代的水平？</p> <p>Andrej Karpathy：这很难直接量化。我们需要更细粒度地看任务而不是职位。因为社会会不断重构职位，将可自动化的部分剥离出来，剩下的由人类完成。举个例子，Hinton 曾预测放射科医生（radiologists）会被取代，结果完全相反。放射科医生的需求反而在增长。虽然计算机视觉技术已经足够强大，但实际工作中还包括病患沟通、报告撰写、跨学科协作等复杂环节，AI远未能胜任。相比之下，呼叫中心客服则是一个更容易被自动化的职业。它具备几个特征：任务简单、重复性高、交互模式固定、完全数字化。一个典型流程是10分钟通话、解决问题、修改数据库。这样的工作非常适合AI接管。</p> <p>但我并不认为AI会直接取代人。更现实的模式是出现一种自主滑块（autonomy slider），AI完成80%的工作量，剩下20%交给人类处理。未来的工作界面可能会变成这样：一个人类监督五个AI助手，管理他们执行重复性任务。企业也会诞生出新的“AI协调层”，专门帮助团队管理这些尚未完全可靠的AI员工。我认为，这种AI分层管理结构将会在未来十年广泛出现在经济的各个行业。毕竟，除了客服，大多数职位的复杂性远超接电话这一层。</p> <p>Dwarkesh Patel：我在想，也许放射科医生的情况可以类比早期自动驾驶的发展。最初自动驾驶上路时，前排必须坐着一个人，以防系统出错时能及时干预。即便是现在，像Waymo的Robotaxi虽然正式商用了，但车内依然有人类监控。我怀疑在放射科领域也可能出现类似情况——即便AI已经能自动化99%的工作，那最后1%依然需要人类来完成，而这 1% 的人类环节反而成为整个系统的瓶颈。</p> <p>更重要的是，如果这个“坐在前排的人”必须经过多年专业训练才能胜任，那他们的薪资理论上应该显著上升。因为他们承担着唯一且关键的不可替代角色。我感觉放射科医生的薪资上涨，部分原因就来自这种“瓶颈效应”：当人类成为最后1%的守门人时，他们的稀缺性被放大了。这种趋势或许也会在呼叫中心或其他岗位的薪资结构中重现：工资先下降90%，然后在最后阶段陡然上升，直到那最后1%被彻底替代。</p> <p>Andrej Karpathy： 这是个挺有趣的假设，但我觉得目前还没在放射科看到这样的趋势。说实话，我一直不太理解为什么Hinton当初点名放射科医生这个职业。因为这个行业本身极其复杂、混乱，远不止图像识别那么简单。相比之下，我更关注呼叫中心员工的情况。因为他们的工作高度重复，自动化潜力非常高。虽然我没有直接的数据，但我猜这一领域目前已经在引入部分AI替换人力。不过我也预计这种替换可能会被部分回调。或许一年或两年后，公司又会重新招聘一些人类员工。</p> <p>Dwarkesh Patel：事实上，有证据表明这种回调已经在发生。一些最早引入AI的公司，后来又重新雇用了人类员工，这其实挺令人意外的。另一个让我惊讶的现象是，按理说，如果我们真在通往AGI的路径上，AI应该能处理所有知识性工作（我们先不谈体力劳动）。那你会以为，这种替代会像逐步抽丝一样进行：先从顾问工作里替掉一小部分任务，再从会计工作中替掉一点点，慢慢地从每个职业里削走一块。但现实完全不是这样。AI的发展路径非常不均衡，几乎所有效率的提升都集中在程序员群体。</p> <p>如果你看大模型公司的收入结构，去掉面向消费者的Chat业务，单看API收入，几乎都是编程相关的应用在主导。这说明一个号称“通用”的系统（general-purpose system），在实际部署中却极度集中地服务于代码生成与软件开发。这真是一个反直觉的现象，AGI的早期形态，居然首先变成了程序员助手。</p> <p>Andrej Karpathy： 这一点其实挺有意思。我确实认为编程是LLM和Agent最完美的首个应用场景。原因在于，编程从一开始就是围绕文本展开的。程序员面对的是命令行和代码编辑器，一切交互都以文本为中心。而大语言模型天生就是在互联网上以文本为食的，它们最擅长的能力就是处理文本。再加上互联网上存在着海量的高质量代码数据，这种供需关系几乎是“天作之合”。</p> <p>除此之外，整个软件生态早已为代码操作构建了完备的基础设施。比如，开发者都有自己的IDE，像Visual Studio Code，而Agent可以直接嵌入进去执行修改。当Agent对代码做出变更时，系统会立刻生成“diff”文件，清晰展示修改的差异。也就是说，我们已经提前为AI自动化准备好了“展示、对比、验证”的可视化框架。</p> <p>但当你把这种模式搬到别的领域，比如幻灯片自动化，情况就完全不同。幻灯片不是文本，而是由空间布局与视觉元素组成的。AI要修改幻灯片，你该怎么呈现“diff”？没有任何通用工具能直观显示修改前后的差异。整个生态基础都不存在。所以结论是：代码是AI的理想宿主，而许多非文本领域目前并不具备这种可对接性。</p> <p>Dwarkesh Patel：但我不确定这能完全解释现象。因为我个人尝试过一些完全属于语言输入 - 语言输出的任务，比如把长访谈稿改写成短片段、根据文本生成视频摘要等等。理论上，这类任务正好是LLM的强项，但实际效果很有限。我们的共同朋友Andy Matuschak（AI科学家）也提到过类似的情况。他尝试了无数方法让模型为间隔重复（spaced repetition）生成学习卡片：这也是典型的“语言进-语言出”任务。他试过上下文示例、监督微调（supervised fine-tuning）、检索增强等五花八门的方式，但模型始终无法生成符合预期的结果。这让我觉得奇怪：即使在纯文本领域，除了编程以外，我们依然很难从LLM中挖掘出显著的经济价值。这背后的原因是什么？</p> <p>Andrej Karpathy：我觉得这确实合理。我并不是说只要是文本就容易处理，代码的结构性远强于自然语言。自然语言往往更“散”，充满模糊性与冗余信息，用统计学的说法，它的熵更高。而代码是高度规范化的，人们一旦理解了语法，就能直接检验输出是否正确。另外，编程本身是一项门槛较高的活动，这使得LLM哪怕只解决了一部分问题，用户也会感受到巨大的赋能。而在文本生成类任务中，人类的创造力基线已经很高，模型的增益显得不那么明显。所以我想说的不是所有文本任务都容易，而是代码刚好落在了LLM能力与人类需求的完美交集点：结构清晰、语料丰富、验证标准客观。也正因如此，编程成为AI最早实现规模化应用的领域。</p> <p>我们早已身处新一代工业革命中——超级智能不是突变，而是惯性 Dwarkesh Patel：你怎么看“超级智能（super intelligence）”？你觉得它会不会和人类或人类社会的智能有本质上的不同？</p> <p>Andrej Karpathy：我倾向于把它看作社会自动化的一种延续。从计算趋势来看，我认为未来会有越来越多的任务被逐步自动化。超级智能只是这个趋势的外推结果。我们会看到越来越多的自主实体（autonomous entities）完成数字化工作，甚至在更久之后扩展到物理层面。总体来说，我认为它仍然是“自动化”的范畴。</p> <p>Dwarkesh Patel：但自动化只是复制人类已经能做的事，而超级智能可能还能做出人类没做过的创新，对吧？</p> <p>Andrej Karpathy：对，不过我觉得“发明新事物”其实也属于自动化过程的一部分，如果这样说得通的话。</p> <p>Dwarkesh Patel：那从更感性的角度看，你觉得未来的文明会不会质上不同？比如超级智能的思考速度更快，能复制无数个自己、甚至融合这些副本，整体比人类聪明太多。这样的文明会不会让人类社会显得完全陌生？</p> <p>Andrej Karpathy：我觉得本质上它还是自动化，但外观上一定会“非常陌生”。就像你说的，它运行在巨大的计算集群上，速度极快、规模庞大。让我担忧的不是“AI接管世界”这种情节，而是一个更现实的场景：我们会逐渐失去理解和控制的能力。因为这些系统会被层层叠加在社会的每个角落，理解其运行机制的人会越来越少。结果就是一切仍在运作，但我们越来越不清楚自己身处的系统是怎么运作的。在我看来，这种“逐步丧失理解与控制”的情境，可能才是最可能发生的未来。</p> <p>Dwarkesh Patel：我想进一步探讨一下。对我来说，“失去控制”和“失去理解”并不是同一回事。比如一家公司的董事会——随便举例，像台积电、Intel——那些董事大多是八十岁的老先生，他们对公司运作其实理解不多，也不一定真正掌控全局。又或者更极端一点，美国总统这个例子，总统确实拥有巨大的权力，但他对系统运作的理解程度与他的控制权完全不同。</p> <p>Andrej Karpathy：对，我觉得这是个合理的反驳。你说得对。我猜我预期的是两者都会同时发生——既失去理解，也失去控制。我们已经在进入一个我自己都很难想象的领域。如果我写一部科幻小说，它的情节可能不是某个单一实体统治世界，而是出现多个逐渐具备自主性的实体，它们相互竞争，有的走向失控，有的被其他AI反制。那种情境更像是一个被完全自治系统填满的“沸腾生态池”，而我们已经把权力和任务都委托给了它们。</p> <p>Dwarkesh Patel：所以并不是因为它们比我们聪明，而导致我们失去了控制。或者说，不一定。真正导致失控的，是这些系统之间的竞争，以及竞争过程中涌现出的结果。</p> <p>Andrej Karpathy：是的，我基本上也这么想。我觉得这些系统中的很多其实仍然是人类使用的工具，只是有部分在代表特定人群或组织行动。所以可能从个体层面看，那些人依然“在控制”，但从整个社会层面看，我们可能已经整体失去了控制，因为最终的结果和社会希望的方向逐渐脱节。这是一种“整体意义上的失控”。</p> <p>Dwarkesh Patel：我们刚才谈到，现在做AI工程或AI研究时，这些模型更像是“编译器”，而不是“替代者”。可一旦真的出现所谓的AGI，它应该能做你在做的事。那如果有一百万个“你”的副本在并行工作，会不会极大加速AI的进步？也就是说，当真正的AGI出现时，我们会不会看到“智能爆炸”？我不是说现在的LLM，而是更远期的情况。</p> <p>Andrej Karpathy：我想会的，但这其实是“常态”。因为我们早已身处一场持续数十年的“智能爆炸”之中。看看GDP曲线，它本质上就是一个跨越各行业、指数加权的增长过程。一切都在逐步自动化，已经持续了几百年。工业革命就是自动化的开始，是物理劳动和工具制造的自动化。编译器是早期的软件自动化。</p> <p>所以我认为，人类早已在递归式地自我加速、自我改进。换个角度看，地球在生物演化前其实是个“无聊的星球”，从太空看几乎没什么变化。而现在我们正处在一场“烟火事件”的中段，只不过它是慢动作的爆炸。我确实认为，这场爆炸早已发生了很久。而AI，只是这场持续爆炸过程中的又一个阶段，并不是某种与过去断裂的新技术。</p> <p>Dwarkesh Patel：你认为这种“超指数增长趋势”会持续下去吗？</p> <p>Andrej Karpathy：这正是让我感兴趣的地方。我之前尝试在GDP数据中寻找AI的痕迹——我以为既然AI出现了，GDP曲线应该上升。但当我去看那些我认为同样“具有颠覆性”的技术，比如计算机、智能手机等，我发现你根本找不到它们在GDP中的突变痕迹。GDP曲线依旧保持着那条指数线。</p> <p>比如早期的iPhone，它刚发布时甚至还没有App Store，没有今天的各种功能。我们回看2008年觉得那是一次“地震级”的技术变革，但从经济数据上看，几乎没有突变。原因在于，这些技术的扩散是缓慢而分散的，最终都会被“平均”进那条稳定的指数增长曲线中。计算机也是如此——当计算机出现时，你并不会在GDP上看到一个“陡然上升的节点”。</p> <p>AI也是一样的。它只是进一步的自动化，让我们能编写一些以往做不到的程序。但归根结底，AI仍然是一种“程序”，是一种新的计算机系统，拥有自己的问题。它的影响也会慢慢渗透开来，最后仍旧汇入那条同样的指数曲线。不过，随着指数继续上升，它会变得越来越“垂直”，我们生活的世界也会显得越来越陌生。</p> <p>Dwarkesh Patel：你的意思是，从工业革命之前到现在，人类经济增长率经历了一个“超指数”的变化——比如几千年前是0%的增长，后来变成0.02%，而今天大约是2%。这就是“超指数”的过程。那你是在说，AI会让这个增长率从2%变成20%甚至200%？还是说，从过去300年的趋势来看，不论技术如何更迭，增长率其实一直稳定在2%，AI也不会改变这个规律？</p> <p>Andrej Karpathy：我认为增长率在过去两三百年确实保持相对稳定。但放在人类历史尺度上看，它的确经历了一次“爆炸式加速”——从几乎0%变成现在的2%。我之前确实试图在GDP曲线里找到AI的影响，但最后我相信这是一个误区。就算人们谈论“递归自我改进”，我也觉得这依旧是“常态”。当然AI会推动自我改进，但人类社会早就在不断自我加速了。比如LLM让工程师更高效地构建下一代LLM，越来越多的环节在被自动化、被优化。所有工程师能用Google搜索、能用IDE、能用自动补全、能在云端编程——这一切本身就是持续的加速。AI只是这条平滑曲线上的又一个环节，而非断点。</p> <p>Dwarkesh Patel：所以，你的意思是，增长率本身不会变化。即使出现所谓的“智能爆炸”，它也只会让我们继续维持在那条2%的增长曲线上，就像互联网帮助我们维持住那条2%的增长一样？</p> <p>Andrej Karpathy：对，我的预期是它仍会遵循同样的模式。</p> <p>Dwarkesh Patel：但如果从相反的角度看，我倾向于认为它可能会“爆炸”。我说的是真正的AGI——不是LLM编程助手，而是能在服务器上取代人类的那种智能体。它与以往的生产力工具不同，因为它本身就是“劳动力”。我们现在生活在一个“劳动力稀缺”的世界里。问任何创业者他们最需要什么，答案往往是“更多优秀的人”。如果突然多出几十亿个会创造、会协作、会自己组建公司的“智能个体”，那和发明一项新技术完全不同。这更像是——地球上突然多出了100亿个聪明人。</p> <p>Andrej Karpathy：我觉得这是个有趣的反驳，我也愿意被说服。但我想指出一点：计算本身就是劳动。计算机早就接管了大量人类的数字信息处理工作，许多岗位因此消失。换句话说，计算机已经是“劳动力”。自动驾驶也是如此——那就是计算机在从事劳动力活动。所以我认为，这种“AI做劳动”的现象其实已经在发生了。这仍然是“常态”。</p> <p>Dwarkesh Patel：但这一次，我们面对的机器可以更快地产生下一代技术——比如它不仅能造出下一辆自动驾驶汽车，还能造出下一个互联网。历史上我们确实经历过从0.2%增长跃升到2%的阶段，所以也许，这次的“机器产出机器”的循环，会让这种加速再度发生。</p> <p>Andrej Karpathy：我明白你的意思。但我觉得人们往往会有一种误解。他们假设，“好吧，我们已经把上帝装进了盒子，现在它什么都能做。”但事实不会是那样。AI会做成一些事情，也会在另一些事情上失败。它会逐步被引入社会，慢慢扩散，最终仍然呈现出同样的模式。这就是我的预测。因为这种假设——认为我们突然拥有一个完全智能、完全灵活、完全通用的人类模型，可以随意调配到社会的各个问题上——我认为那不会发生。这不会是一个“离散跳跃（discrete change）”，而是一个渐进的过程，技术会一点点渗透进各个行业。</p> <p>Dwarkesh Patel：我觉得这种讨论中常见的误导是，人们使用“智能（intelligence）”这个词，会让人误以为“超级智能”意味着某个单一的超级大脑坐在服务器里，像神一样思考所有发明与技术，从而引发一场爆炸式变革。而我想象的并不是那样。我设想的是可能会有数十亿个非常聪明、接近人类思维的“心智”同时存在。真正带来变化的不是某个超级个体，而是数量的巨大。这些“心智”就像数以亿计的高智商移民，他们会自己找到融入经济体系的方式——创业、发明、创造价值，而不需要我们“教他们该怎么做”。我们已经在现实中看到类似的例子：像香港、深圳这样的地区，在几十年里实现了10%以上的年经济增长。原因在于，他们有大量愿意充分利用资源的聪明人，经历了一段“追赶期”。我认为AI可能带来的情景，会非常类似。</p> <p>Andrej Karpathy：我明白你的意思，但我仍然觉得你在预设一种“突变”。就像某个被锁住的能力突然解封，然后我们马上就能在数据中心里拥有成千上万的天才。我认为这种离散式的跳跃在人类历史中几乎没有先例，我在任何数据中都找不到这样的模式，我也不认为它会发生。</p> <p>Dwarkesh Patel：但工业革命不就是一个这样的跳跃吗？我们从接近0%的增长变成了2%的增长。我只是说，也许AI会带来类似的跃迁。</p> <p>Andrej Karpathy：我对这点有点怀疑。我得去查查看，比如也许工业革命之前的经济记录并不精确，数据质量有限。我不排除你说的可能性，也许你是对的。你认为那是一个极其独特、几乎“魔法般”的事件，而现在我们可能会迎来另一次类似的时刻。一个打破既有范式的节点。</p> <p>Dwarkesh Patel：其实我觉得工业革命的关键就在于它并不神奇。你如果拉近看1780年或1870年，并不会发现某个决定性发明。变化是渐进的。但与此同时，经济确实进入了一个全新的增长轨道，增速更快、指数倍增长。我对AI的预期也类似：不会有某个“关键发明”的时刻，而是当积累到一定阈值时，我们突然意识到增长曲线已经彻底改变。</p> <p>Andrej Karpathy：是的，也许可以把它理解为某种“能量释放的临界点”。比如有时候出现一种新能源，或者在AI的情况下，是认知能力（cognitive capacity）被解锁。当这种潜在能力突破临界点时，就会释放出大量尚未完成的“认知工作储备（cognitive work overhang）”。而你认为AI正是填补这些工作缺口的新技术。</p> <p>Dwarkesh Patel：没错。毕竟，经济增长来自人们提出想法、执行想法、创造有价值的成果。而在人类历史的大部分时期，这些过程伴随着人口的快速增长。过去50年里，有人认为全球增长趋于停滞，尤其是发达国家的人口增长放缓。如果我们回顾过去的“超指数增长”，其实是人口和产出的共同增长驱动的。人口的指数增长带来了产出的超指数增长。</p> <p>Andrej Karpathy：我理解这种观点，但直觉上我并不完全认同。</p> <h2 id="六multi-agent的下一步产生文化与自我博弈的能力">六、Multi-Agent的下一步：产生”文化”与”自我博弈”的能力</h2> <h3 id="61-智能的演化一个极其罕见的事件">6.1 智能的演化：一个极其罕见的事件</h3> <p><strong>Dwarkesh Patel：</strong>你之前向我推荐过Nick Lane的书，我也因此去采访了他。我想顺着这个话题问一点关于智能与进化的问题。你做了二十年的AI研究，现在对”智能”是什么、怎样产生智能，应该有了更具体的理解。那你会不会因此更加惊讶进化竟然能”偶然”地演化出智能？</p> <p>Andrej Karpathy：我非常喜欢Nick Lane的书，刚才上来的路上我还在听他的新播客。关于智能及其演化，我确实认为它出现得相当晚——非常非常近期。老实说，我对它能演化出来感到惊讶。我常常会想象：如果宇宙中有上千颗像地球一样的行星，它们会是什么样子？Nick Lane提过，绝大多数星球上可能都会演化出类似的生命形式，比如细菌之类的东西；但从细菌到更复杂生命之间，有几个极难跨越的阶段。</p> <p>我直觉上觉得，“智能”的演化是极其罕见的事件。比如，细菌已经存在了二十多亿年，却没有产生更复杂的智能。真核生物的出现本身就非常艰难，因为细菌在地球演化早期就已经出现。再比如动物——多细胞动物大概存在了几亿年，也许是地球寿命的10%左右。从这个时间尺度上看，智能的出现似乎也不是不可能，但我仍觉得惊讶。直觉上，我原本更期待看到的，是一群在做“动物的事”的动物，而不是能创造文化、积累知识的生命体。这种突变太神奇了。</p> <p>Dwarkesh Patel：如果我们接受这种“突变性”的观点——也就是智能的核心其实是动物智能——那也许只要达到“松鼠”的智能水平，我们就离AGI不远了。松鼠的智能大概出现在寒武纪大爆发后六亿年前，那次变化似乎是由地球的“氧化事件”触发的。氧气一旦足够，真核生物就能形成复杂结构，随之智能算法似乎马上就出现了。这会不会说明：动物智能其实并不复杂，只是一次偶然的演化事件？</p> <p>Andrej Karpathy：是啊，这种事情真的太难判断了。我们或许可以根据“停滞的时长”去估计复杂性。Nick Lane在书里对这一点描述得很好——在细菌和古菌阶段，有长达二十亿年的瓶颈期：化学过程极其多样，却始终没有迈向动物形态。而在动物到智能之间，我们似乎还没看到那种持续数十亿年的“停滞”。</p> <p>另外，也许可以看智能是否曾经多次独立演化。如果这种“算法”在不同物种间多次出现，那说明它可能并非罕见。比如人类的类人猿智能是一种，但鸟类的智能（像乌鸦）也是极高的，而且它们的大脑结构与人类大不相同。这或许是智能多次独立演化的迹象。</p> <p>Dwarkesh Patel：有两位我之前采访过的学者，也就是Guern和Carl Schulman曾提到过一个有趣的观点：他们认为，人类与灵长类拥有的“可扩展智能算法（scalable algorithm）”其实也在鸟类中独立演化过，甚至可能在其他物种中也发生过。区别在于人类找到了一个能够奖励“边际智力提升”的生态位（evolutionary niche），并拥有一种可扩展的大脑结构，使得智力得以持续进化。</p> <p>相反，鸟类的大脑再聪明，也受制于体重和飞行。如果脑太大，它们就飞不起来；它们非常聪明，但生态位不会“奖励”更大的脑容量。海豚也类似，智商高，但受限于环境。而人类不同：我们有双手，能推动工具使用；我们能通过烹饪和社会分工外化消化过程，将更多能量供给大脑，于是这个正反馈循环就启动了。</p> <p>Andrej Karpathy：是的，而且还得有材料可用。比如，如果我是只海豚，那会更困难吧？因为在水里，你没办法点火，也不能做很多事。单从化学角度讲，水下世界能实现的反应和活动范围，比陆地要小得多。我同意刚才你说的“生态位激励”理论。也就是说，不同环境奖励不同的进化方向。但我仍然觉得这件事很神奇。直觉上，我原本更可能预期的是动物们一路往“肌肉更大”的方向进化，而不是走向智能。智能的出现是一个非常奇妙的分叉点。</p> <p>Dwarkesh Patel：我们换种方式说吧：这件事之所以困难，是因为它需要走在一条极其细微的界线上。如果某种能力足够重要，进化就会把它直接“硬编码”进DNA里；如果不够重要，那干脆就不学。智能之所以特别，就在于必须存在一种中间状态——它要让个体在生命周期中学习，但又不能完全预先写死。</p> <p>Andrej Karpathy：没错，你得“激励出”某种可适应性。理想情况下，你希望生物所处的环境高度不可预测，这样进化就没法提前把算法写进权重里。很多动物基本上是“预烘焙的（pre-baked）”，它们一出生，大脑连接就决定了行为。而人类则不同——人类必须在“测试时”（test time）重新学习世界。所以也许，真正促进智能出现的，是那些变化极快、难以预知的环境。因为只有这样，你才“不得不”进化出智能，用以实时解决问题。</p> <p>Dwarkesh Patel：Quentin Pope 有一篇有趣的博客文章，他认为AI不会出现“陡峭爆发（sharp takeoff）”，理由来自人类的历史。人类在6万年前已经拥有现代大脑结构，但直到1万年前才出现农业革命与文明。在那5万年里，人类其实是在慢慢建立“文化支架（cultural scaffold）”，一种能让知识跨世代累积的机制。</p> <p>但在AI训练里，这种机制是“天生存在的”：我们可以反复微调、重训模型，让它们继承彼此的参数或数据语料，而不需要像人类一样从零开始。因此，那种需要几万年才能形成的“文化循环”，对AI来说似乎是自带的。</p> <p>Andrej Karpathy：是，也不是。因为大语言模型其实没有“文化”，至少目前没有。也许我们现在给它们的输入太多了，反而让它们失去了创造文化的动力。但“文化”的核心在于情感、书写、传承，也就是能在个体之间留下记录、积累记忆。而LLM目前并没有这种机制。这实际上是一个限制。</p> <p>Dwarkesh Patel：那你觉得，“LLM文化”可能会是什么样的？</p> <p>Andrej Karpathy：最简单的形式，也许就是一个巨大的可编辑笔记本（scratchpad）。LLM在处理任务或阅读内容时，不断更新这个笔记本，为自己留下记录。为什么LLM不能为其他LLM写书呢？那该多有趣。为什么别的LLM不能读这本书，被启发、被震惊？目前完全没有这种机制——但这正是“文化”的萌芽。</p> <p>Dwarkesh Patel：你认为这种如多智能体系统（multi-agent systems）或AI的“独立文明”与“文化”的现象什么时候会开始出现？</p> <p>Andrej Karpathy：我认为在多智能体（multi-agent）领域有两个非常重要的方向，但这两件事目前都还没有被真正实现。第一个是“文化”——也就是让LLM为自身目的逐步积累知识的能力。第二个，是我认为极具潜力的“自我博弈（self-play）”。</p> <p>在我看来，自我博弈是一个非常强大的机制。进化的本质其实就是不断的竞争，而这种竞争推动了智能的诞生与进步。在算法层面，AlphaGo就是通过和自己对弈，不断提升棋力。但在LLM领域，目前还没有任何真正等价的“自我博弈”机制。我认为这迟早会出现。比如，为什么LLM不能自己生成问题，让另一个LLM去解决？然后这个LLM又尝试制造更难的问题，让对方挑战？</p> <p>这种机制完全可以设计出多种形式，这是一个值得深入研究的领域。但截至目前，还没有看到令人信服的成果，能够真正实现这两种“多智能体的协同进化”机制。我们仍然停留在“单一智能体”的阶段。文化也是类似的情况——我们甚至还没看到AI之间的“组织”或“群体结构”真正形成。正因如此，我认为我们仍处在非常早期的阶段。</p> <p>Dwarkesh Patel：那你觉得，目前阻碍LLM之间这种协作的核心瓶颈是什么？</p> <p>Andrej Karpathy：比如，一些小模型或“更笨”的模型，它们与人类认知阶段有奇妙的对应关系：像幼儿园、小学、初中、高中。但现在的模型总体上还没“毕业”。无论是我的Claude Code还是Codex，它们虽然能做博士级别的测验题，但在认知层面，仍然更像是小学甚至幼儿园的学生。它们是“天才儿童”——记忆力完美，能生成看起来漂亮的内容，但并不真正理解自己在做什么。它们还没形成跨领域的认知协调能力。正因为如此，我不认为它们现在具备创造“文化”的条件。它们还太稚嫩。</p> <h2 id="七被低估的技术扩张难点从demo到规模化的难度">七、被低估的技术扩张难点：从Demo到规模化的难度</h2> <h3 id="71-自动驾驶的教训每一个9都要用同样的代价换来">7.1 自动驾驶的教训：每一个”9”都要用同样的代价换来</h3> <p><strong>Dwarkesh Patel：</strong>你曾在特斯拉负责自动驾驶项目，从2017到2022年，亲眼见证了这项技术从”炫酷演示”到”数千辆车在路上自主驾驶”的转变。为什么这件事花了十年？这中间到底发生了什么？</p> <p>Andrej Karpathy：首先我要强调——这项技术现在还远远没完成。自动驾驶对我来说是非常重要的经验，因为我在这个领域工作了五年，这让我对AI发展的节奏形成了很强的直觉。事实上，自动驾驶的早期演示可以追溯到上世纪80年代。卡内基梅隆大学在1986年就展示过一辆能自己在路上行驶的卡车。</p> <p>我2014年左右加入特斯拉前，朋友带我看过Waymo的早期演示，那次驾驶几乎完美。当时我以为量产就在眼前，但结果又花了十年。这揭示了一个规律：有些任务存在巨大的“从demo到实际产品的落差”。做出一个演示很容易，但要成为真正可靠的产品则极其困难。尤其像自动驾驶这种，一旦失败，代价太高。</p> <p>在很多行业中，失败的代价并不大，但在这些高风险领域，开发周期会被极大拉长。比如软件工程也是类似的例子——vibe coding当然没问题，但若是要写生产级代码，哪怕一个小漏洞也可能造成安全问题或隐私泄露，后果灾难性。</p> <p>所以，我认为软件和自动驾驶共享同一个特征：高风险、高要求、零容错。更具体地说，这个过程是一场“九的征程（march of nines）”。每多提升一个“9”的可靠率，都需要同样的工作量。当系统达到90%的可靠性，那只是第一个“9”；接下来要95%、99%、99.9%、99.99%……每一步都同样艰难。在我在特斯拉的五年里，我们大概提升了两到三个“9”，但仍有更多要攻克。这就是为什么这样的项目要花十年甚至更久。</p> <p>也正因如此，我对demo从来毫不印象深刻。任何AI demo在我看来都只是起点。无论它多么惊艳，都还没触碰到真正的产品难题。真正的挑战，是当技术接触到现实世界后，如何在成千上万种异常情况下依然安全稳定地运行。自动驾驶教会了我一点：每一个“9”都要用同样的代价换来。AI的未来也是这样——demo令人振奋，但离成熟仍有漫长的路。</p> <p>Dwarkesh Patel：你刚才提到软件系统在安全保障方面的要求，和自动驾驶其实非常相似，这一点很有意思。人们常说自动驾驶之所以花这么久，是因为失败的代价太高。比如，一个人类司机平均每开40万英里或大约七年才出一次严重错误。如果你要发布一个“不能在七年内犯错”的代码智能体，那部署难度肯定巨大。但你的意思似乎是，如果一个编程Agent每七年就犯一次灾难性的错误，比如让关键系统崩溃，那其实也是不可接受的？</p> <p>Andrej Karpathy：是的，而且非常容易发生。</p> <p>Dwarkesh Patel：实际上，从“时间”角度看，这个周期甚至比七年更短。因为AI在不断地输出代码，相当于每个token都在冒风险。换算到时钟时间上，出错频率其实要高得多。</p> <p>Andrej Karpathy：对，从某种意义上说，这其实是一个更难的问题。自动驾驶只是人类成千上万种行为中的一种，是一个垂直领域。而软件工程则复杂得多，表面积更广，问题更多。</p> <p>Dwarkesh Patel：不过有人提出一个反驳：自动驾驶花很长时间的一个主要原因，是它必须先解决“基础感知”的问题——也就是视觉理解、物体识别、空间推理，以及当环境稍有变化时，如何具备“常识性泛化”。比如，当路边有人挥手时，系统能理解那意味着“停车”，而不需要针对这个动作单独训练。但现在我们拥有LLM或VLM（视觉语言模型），这些基础表征问题已经“被免费解决”了。所以从理论上讲，把AI部署到不同领域，就像让自动驾驶汽车去另一座城市运行——困难，但不需要再花十年。</p> <p>Andrej Karpathy：我不太确定我完全同意这种说法。我不确定我们到底“免费获得”了多少。我认为我们确实拥有了更通用的智能表征能力，但仍存在巨大的理解空缺。自动驾驶虽然是专用任务，但从另一个角度看，构建“专用智能”也有自己的难度，因为它不是从通用智能中自然涌现的产物，需要从零构建。因此，类比虽然有一定合理性，但我觉得并不完全贴切。LLM确实更“通用”，但它们依然充满漏洞与认知缺口，我们还没有真正获得“开箱即用的泛化”。</p> <p>另外，我想回到最开始提到的那点，那就是自动驾驶其实还远未完成。虽然我们已经能看到Waymo之类的车辆在路上运行，但规模依然很小。根本原因在于它们还不具备经济可行性。这些系统是“未来的产物”，现在运行的每一辆车都伴随着高昂的维护与资本开销。要让自动驾驶真正“划算”，仍需要长期的努力。</p> <p>还有一点，人们看到“无人驾驶”车辆时，往往会被表象误导。事实上，很多自动驾驶车背后都有庞大的远程操作中心，里面有人类操作员在“闭环干预”。我不知道具体程度，但可以肯定：人类仍在回路中，只是被“转移到看不见的地方”。在某种意义上，我们并没有移除人类，而是把人类藏了起来。</p> <p>因此，我仍然认为，自动驾驶要从一个环境推广到另一个环境，仍面临大量挑战。虽然现在的系统在体验上“看起来真实”，但背后仍有许多辅助。比如，Waymo的车辆其实不能行驶在所有城市区域——信号差的地方可能根本不行驶。当然，这些只是我的推测，我不了解他们的完整技术架构。</p> <p>图片 图片来源：Bloomberg</p> <p>Dwarkesh Patel：不过你毕竟在特斯拉带领自动驾驶团队干了五年。</p> <p>Andrej Karpathy：是的，不过我要澄清一下，我并不了解Waymo的具体实现。我其实非常喜欢Waymo，也经常乘坐它。我想表达的只是：人们对AI进展往往过于天真。我认为距离真正的成熟仍有很长的路。我更看好特斯拉的方向，因为他们采取了更具可扩展性的策略，我觉得团队做得非常出色，也更贴近我对未来的判断。特斯拉的系统依赖更通用的传感与学习，而不是为每个场景定制规则，这样的路线更有生命力。所以，我其实不太认同“自动驾驶花了十年才完成”这种说法——因为它还没完成。真正的自动驾驶革命，还没真正发生。</p> <p>Dwarkesh Patel：确实，从这个角度看，自动驾驶的起点应该是上世纪80年代，而不是十年前；而且结局还远没有到来。</p> <p>Andrej Karpathy：对，我同意。因为当我们谈论“自动驾驶”时，我脑海中的定义是规模化的自动驾驶，也就是普通人不需要再考驾照的那一天。离那个时刻，我们还有很长的路要走。</p> <p>Dwarkesh Patel：我想再延伸两个方向，因为我觉得这可能是当下世界上最重要的问题之：AI部署的速度与早期阶段的实际价值。如果我们试图预测2030年世界的样子，这就是最核心的问题。</p> <p>首先，一个显而易见的不同是“延迟要求”。自动驾驶模型的实时性要求极高，模型可能只有几千万参数，但必须毫秒级响应。而知识型工作并不一定受这个约束。当然，对计算机操作类任务或许仍有要求。</p> <p>第二个不同点，更关键的也许是资本开销。部署一辆自动驾驶车的成本极高，而AI模型则完全不同。虽然每次推理有额外的计算成本，但边际成本极低。训练一次大模型的投入可以摊薄到后续的无数次调用中。部署一个新的AI实例，远比制造一辆新车简单得多。整体上，AI的扩张经济性要好得多。</p> <p>Andrej Karpathy：我同意。凡是“以比特为核心”的事，都比“接触物理世界”的事容易上百万倍。比特是完全可重组、可任意复制、可即时更新的——这意味着AI产业的适应速度和扩张速度都会更快。至于你提到的另一个问题……</p> <p>Dwarkesh Patel：延迟需求。以及它对模型规模的影响。</p> <p>Andrej Karpathy：是的，我认为大体上你的判断是对的。当然，在大规模知识工作应用场景中，也会出现新的延迟瓶颈——因为要支撑这些任务，我们必须建立庞大的计算供应链与算力调度系统。不过，还有一个更容易被忽略的层面，我想简短地补充一下：那就是法律、社会、保险与伦理的层层结构。</p> <p>比如，在自动驾驶中，有人往Waymo车顶放一个路锥，这种行为也必须被纳入系统设计。那么，AI社会中它的等价物是什么？谁来承担责任？谁来定义“AI事故”？什么是“AI保险”？同样的问题也会出现在Agent与多模型协作中。所以，我觉得自动驾驶其实是一个极好的类比。几乎每个现实挑战，在AI社会里都会出现它的“镜像版本”：</p> <p>“放锥体”的人 → 故意干扰AI输入的用户；</p> <p>“远程操作员” → 隐形的人类在AI系统中的监管者；</p> <p>“交通法规” → 模型间交互的伦理与法律边界。</p> <p>Dwarkesh Patel：这很有趣。那这是否意味着——当前AI行业正在疯狂扩建算力，计划在一两年内增加十倍，十年内甚至百倍。如果AI的实际部署速度低于乐观预测，我们是否可能在“过度造计算力”？</p> <p>Andrej Karpathy：这让我想到历史上的类似情况，比如铁路泡沫，或者更准确地说，是上世纪电信行业的过度铺设。上世纪90年代末，互联网还没真正普及，但整个电信行业已经为“未来的互联网”提前铺满了光缆，于是出现了泡沫。</p> <p>Dwarkesh Patel：是的，电信泡沫。</p> <p>Andrej Karpathy：对，我理解我听起来有点悲观。但其实我是个乐观主义者。我认为AI最终一定会奏效——这是一个可解的问题，只是需要时间。我之所以语气显得悲观，是因为我每天刷推特时看到太多“脱离现实”的言论。很多都源于融资激励、流量变现或资本营销，而不是技术事实。我只是对这种“虚高”在做出反应。但总体上，我仍然非常看好这项技术的长期前景。</p> <p>过去一年，Claude Code、OpenAI Codex等产品几乎是凭空出现的奇迹，而ChatGPT的使用量也验证了庞大的市场需求。所以，我并不认为我们在“过度建算力”。我认为所有这些计算资源最终都会被消化。只是，我希望行业能校准预期。在过去十五年的AI历程中，我反复见到“知名人士误判AI进度”的情形，这次也不例外。而这种误判不仅影响投资与产业，还可能带来地缘政治层面的后果。因此，我们更需要脚踏实地地理解AI技术究竟是什么，不是什么。</p> <h2 id="八建立星际舰队学院用教育守住人类在ai时代的主动权">八、建立”星际舰队学院”：用教育守住人类在AI时代的主动权</h2> <h3 id="81-为什么选择教育而非继续ai研究">8.1 为什么选择教育而非继续AI研究？</h3> <p><strong>Dwarkesh Patel：</strong>我们来聊聊Eureka Labs吧。你现在也可以去重新创办一家AI实验室，继续攻克那些技术难题。但你选择了做教育，这让我挺好奇的。你现在主要在做什么？为什么不是继续从事AI研究？</p> <p>Andrej Karpathy：我想，也许可以这样解释——我对当下AI实验室的发展有一种“确定性”的感觉。我当然可以参与进去，也能起到一些作用，但那种作用未必是独特的。相反，我现在更担心的，是人类在这一切中的角色会逐渐被边缘化。我不仅关心AI将来能否建造“戴森球”、能否实现全自动化的未来，更关心那时的人类会怎样。我希望人类在那个未来依然能活得好，而不是被甩在AI进步的阴影之外。我觉得这正是我可以提供独特价值的地方——让人类能够在智能时代继续保持能动性与尊严，而不是去做前沿实验室里那种“边际的技术改进”。我最害怕的未来，其实有点像《机器人总动员（WALL·E）》或《蠢蛋进化论（Idiocracy）》那样——人类变成了旁观者，被智能系统“照顾”着、麻木着。所以，我认为教育是避免那种未来的关键路径。通过教育，人类才能在AI时代重新掌握主动权。</p> <p>Dwarkesh Patel：那你现在在Eureka具体在做什么？</p> <p>Andrej Karpathy：Eureka的目标，说得最简单一点，就是打造现实版的“星际舰队学院（Starfleet Academy）”。我不知道你有没有看过《星际迷航》。</p> <p>Dwarkesh Patel：我没看过，但听说过。</p> <p>Andrej Karpathy：在《星际迷航》里，Starfleet Academy是一所培养星际飞船驾驶员和科学官的精英学院——那是为探索未来、构建前沿技术而设立的学校。我想象的Eureka，就是这样的地方：一个极具前瞻性的技术教育机构，实时更新知识体系，培养真正能驾驭未来科技的人。</p> <p>Dwarkesh Patel：我接下来想问你一类更具体的问题。你是全世界最擅长讲授技术和科学内容的人之一。你能谈谈你认为“如何才能把技术内容讲好”吗？我很好奇你在YouTube上的教学理念，以及在Eureka中，这种理念会有什么不同？</p> <p>Andrej Karpathy：关于Eureka，我觉得最让我着迷的是——AI将彻底改变教育，而我们必须重新设计整个系统。我认为现在的阶段还非常早期。很多人会从最直接的方式入手，比如让LLM当成问答助手、讲解内容、出题练习，这些当然有用，但对我来说，这还太粗糙了。我希望能“做对”，而不是“做出来”。目前AI的能力还远远达不到我想要的那种“理想导师体验”。举个例子：我最近在学韩语。一开始我是在网上自学，后来去韩国上了小班课，大概十个人左右，再后来换成了一对一的导师。这段经历让我真正意识到好老师的标准有多高。比如我的韩语老师——她只通过几分钟的交流，就能精准判断我的学习阶段、我掌握了什么、哪里模糊。她能提出恰到好处的问题，探测我的知识模型。而今天的任何LLM，都做不到这一点，甚至离那还差得很远。当她真正了解我的程度后，她能为我定制出完全匹配的学习内容。这就是理想的学习状态——永远被恰当地挑战（appropriately challenged）：既不太难，也不太简单。在那种教学中，我感觉自己是唯一的学习瓶颈。知识永远准确、节奏刚好、解释充分——唯一的限制就是我的记忆力与专注度。我希望能让每个人都拥有这种体验。教育不该是知识的瓶颈，而是能力的放大器。这正是Eureka想要重构的未来教育模式。</p> <p>Dwarkesh Patel：你如何自动化这个过程呢？</p> <p>Andrej Karpathy：这是个非常好的问题——“如何自动化这种导师体验”？就以当前的AI能力来说，答案是：做不到。也正因为如此，我认为现在其实还不是构建“AI导师”产品的最佳时机。虽然这无疑是一个有价值、并且许多人都会尝试去做的方向，但以我想要达到的标准而言，目前的模型能力还远远不够。哪怕今天推出一个AI教学产品，它在教育价值上仍是巨大的，但当我回想起自己和那位韩语导师的体验时，我会觉得这个门槛实在太高——高到让我感到敬畏，甚至觉得“暂时还造不出来”。</p> <p>Dwarkesh Patel：但你确实已经在做了，对吧？</p> <p>Andrej Karpathy：是的，但也不是以那种“AI导师”的形式。任何真正体验过优秀导师的人，都会问自己：“你怎么可能复制出这样的存在？”所以我目前更多是在等待那种能力的到来。在AI行业里，我其实常常扮演“劝退”的角色。以前我做计算机视觉咨询时，我的最大价值有时恰恰是告诉客户不要用AI。因为并不是所有问题都适合用AI解决。教育领域现在也是类似的情况：我想要的那种AI导师体验还没到时机。所以我现在构建的Eureka，看起来会更“传统”一些——既有实体部分，也有数字部分。但我很清楚未来的形态会是什么样，只是我们还没到那一步。</p> <p>Dwarkesh Patel：那你能透露一下，你计划今年或明年推出的产品是什么吗？</p> <p>Andrej Karpathy：我现在正在开发Eureka的第一门课程。我希望它能成为“学习AI的最先进目的地”，一个毫无疑问的行业标杆。AI是我最熟悉的领域，所以我认为它是理想的起点。课程名为LLM 101N，其中的压轴项目是一个叫 NanoChat 的作品——它是课程的核心部分，也是学生的最终项目。我现在正在补充中间环节的教学模块，并准备组建一个小型助教团队，真正把整门课搭建成一个系统的学习体验。</p> <p>我想补充的是：很多人谈教育时，往往强调“知识传播”的那一面，也就是把已有的知识扩散出去。但在我看来，教育其实是一项极具技术挑战性的工程问题。对我来说，教育的本质是“为学习者搭建通往知识的坡道（ramps to knowledge）”。NanoChat就是这样一个坡道，它是一个极简但完整的AI项目。只要学生阅读它、拆解它、理解它，他们就能在极短时间内获得大量“灵光乍现”的时刻。我称之为“Eurekas per second”，即每秒顿悟率。这就是我想要的教育体验：高密度的理解时刻。Eureka的使命，就是要设计出这种“坡道体系”，让学习者永远不会卡关，不会觉得太难或太易，而是被持续地、精确地推动向前。</p> <p>Dwarkesh Patel：所以你现在的设想是，在短期内，由学生自己去培养“自我探测理解力”来取代AI导师的那种提问方式。只要他们有足够的自省能力，就能在TA、LLM和参考答案之间找到正确路径。听起来，真正的核心不是AI自动化本身，而是你把“如何解释AI”这件事系统化并编码进课程结构，这才是最大的创新。</p> <p>Andrej Karpathy：我认为这确实是关键——要永远校准自己与现实的AI能力之间的差距。很多人会说“直接去问ChatGPT就行了”，但如果你真的让它“教你AI”，它只会生成一些杂乱的东西——远远称不上教学内容。当前的AI还写不出NanoChat这样的教学作品，而NanoChat正是我认为“当前能力边界下最有用的形态”。我依然在与AI合作，它帮我生成素材、起草框架、处理繁琐的部分，让我能更快地构建课程。</p> <p>我过去曾在斯坦福开设CS231N课程——那是斯坦福第一门深度学习课程。而现在在开发LLM 101N时，我能明显感到两者的差别：如今的AI极大地提升了我的生产力。我可以让LLM帮我写示例代码、整理幻灯片、生成解释结构，我几乎以数倍速度推进课程开发。但内容的创造性设计、概念提炼、以及教学逻辑仍需要我本人完成。AI是辅助，但还不是替代。真正的挑战在于——你是否能正确地定位AI的角色，让它在恰当的时机做恰当的事。</p> <p>Dwarkesh Patel：所以，当你设想Eureka在几年后能够提供的内容时，看起来最大的瓶颈其实是在各个学科里找到像你这样的“Karpathy”，能够把自己对知识的理解转化为通向知识的“坡道”，对吗？</p> <p>Andrej Karpathy：我认为这会是一个逐步演变的过程。现阶段，Eureka的重点还是招聘教师，与AI及教学团队协作，共同打造最前沿的课程体系。而随着时间推移，部分助教可能会逐渐被AI替代。比如，对于一些基础性问题，AI完全可以通过掌握全部课程资料，为学生提供即时、准确的解答，成为高质量的自动化助教。不过，教师仍然是关键的，他们负责课程整体架构的设计与协调，确保内容体系科学、连贯、层层递进。我预想的教育演变路径是：从“人类主导、AI协作”逐渐过渡到“AI设计、人类监督”。也许未来某个阶段，AI会在课程设计上比我做得更好，而那时我可能反而变得不再必要。但这需要一个长期的演化过程。</p> <p>Dwarkesh Patel：那你设想的教学体系是由不同领域的专家各自贡献课程内容，还是说你希望保持自己在教学风格和理念上的主导？比如Sal Khan在Khan Academy里，几乎所有课程视频都是他本人讲解的。你会采取类似的方式吗？</p> <p>Andrej Karpathy：不会那样。我会聘请各领域的专家教师。因为有许多学科我并非专家，只有真正的领域专家才能为学生提供最前沿的学习体验。我当然会继续主导AI方向的课程，但在更广阔的教育体系中，专业化分工是必须的。 总体而言，我设想的Eureka会比人们想象的更“传统”一些。Starfleet Academy在我心中首先是一所实体学校——学生能全职学习，从头到尾与教师、同学共同完成课程内容，确保真正理解。而在此之下，我还会推出一个数字化版本，它可能包含AI教学助手或LLM互动系统，但属于一个相对“次一级”的体验。虽然不如线下沉浸式学习那样完整，但它能让全世界八十亿人都有机会接触到优质教育。</p> <p>Dwarkesh Patel：听起来你其实是在以当代工具重新发明“大学”——基于AI、网络和交互技术，从第一性原理出发构建教育体系，并吸引那些真正有动力、愿意深入学习的人。</p> <p>Andrej Karpathy：是的，我认为未来不仅需要“教育”，还需要“再教育（re-education）”。我非常希望能在这方面有所贡献，因为我相信工作的形态将发生巨大变化。举个例子，现在已经有大量人开始学习AI技能、尝试提升自己——这恰恰是一个非常好的教学起点。在动机层面上，我常把教育分成两个阶段：AGI 之前与 AGI 之后。在AGI出现之前，学习的动机很简单：人们希望赚到钱，而掌握AI技能就是当下最直接的路径。但在AGI之后，这个问题就变得更有趣了。如果一切都实现了自动化，人类不再需要工作，那人们为什么还要上学？</p> <p>我常说，“AGI之前的教育是有用的，AGI之后的教育是有趣的。”就像今天人们去健身房一样。我们并不再需要体力去搬重物——机器早已替代了这部分功能。可人们依然健身，因为这很有趣、能让身体健康、还能让你看起来更好。换句话说，健身已成为一种心理与文化上的吸引力，而不再是生存的需求。我相信教育的演化也会类似。未来的人们上学，就像他们现在去健身房一样。</p> <p>目前，大多数人觉得学习困难，是因为教材要么太难、要么太简单，导致他们反复“卡关”或失去兴趣。只有少部分人能克服这种阻力。而我认为，这其实是一个可以被技术解决的工程问题。如果我们能像我学习韩语时那位导师那样，打造出真正理解学生状态、能持续适配的AI导师系统，学习就会变得轻松而愉快。那样的话，人们将会为了乐趣而学习，因为学习不再痛苦，而是即时满足、不断“顿悟”的体验。到那时，任何人都能轻松掌握多门语言，熟悉大学的所有基础课程，只因为“为什么不呢？”</p> <p>Dwarkesh Patel：我明白你的意思了。也就是说，在AGI之后，人们一方面会把教育当作娱乐、当作自我提升的方式；但另一方面，你似乎也认为教育有助于让人类保持对AI的掌控，对吗？这两者听起来有些不同。你觉得教育的作用是两者并存吗？</p> <p>Andrej Karpathy：可以这么说。我确实认为教育在短期内是一种赋能手段，让人类能跟上AI的步伐。但从长期来看，我得承认，这可能是一场“逐渐失利的游戏”。AI的能力终将超越人类，不过在那之前，我们还远远没有触及人类潜力的极限。</p> <p>目前的问题是，大多数人之所以没能“走得更远”，是因为学习系统效率太低。教材太难或太浅，人们总是被卡在错误的难度区间。如果我们能让教育像精准的训练计划一样被定制化，人类完全可以达到前所未有的广度与深度。未来，每个人都可能会说五种语言、掌握多学科知识，仅仅因为那变得非常轻松。</p> <p>Dwarkesh Patel：这真的很有趣——你这个愿景和健身文化完全对应。想想看，一百年前几乎没人会有一副健美的身材，也没人能轻松卧推两片杠铃。现在呢？系统训练的理念普及后，普通人都能做到。你设想的“教育健身房”模式，本质上就是让学习像健身一样被系统化、量化和普及——让人类在任何知识领域中都能更快、更深、更强。</p> <p>Andrej Karpathy：没错，这正是我想看到的未来。我觉得我其实是在押注“人性的不变性”。无论技术如何发展，人类依然会渴望探索、思考、学习这些事情，而这种渴望是跨越时代的。我相信这种追求会继续存在——就像几千年来人们始终向往智慧与力量一样。</p> <p>如果你回顾历史，其实也能找到一些“后AGI时代的雏形”。比如古希腊的学者圈、或者某些贵族社会——在那些物质充裕、劳动不再是生存必要条件的小型社会里，人们往往会把时间投入到身体和精神的“繁荣”中：学习、运动、艺术、哲学。我相信这种人类本能会延续下去。</p> <p>当然，如果我错了，我们最终迎来一个像《机器人总动员》或《蠢蛋进化论》那样的未来——那将是最糟糕的结局。就算我们建起了戴森球、拥有无限能量，我也不会认为那是胜利。因为那样的世界失去了人类的主体性和尊严。我真正关心的，是人类的成长与卓越——每个人都应该以某种方式成为“超级人类”。</p> <p>Dwarkesh Patel：我理解，但那样的世界似乎依然是一种“文化式的存在”——人类依旧活着，但无法再凭个人的劳动或思维去改变技术的轨迹。你可能还能对AI的决策提出批准或否决意见，但那不再是因为你自己发明了什么、创造了什么新设计而影响未来。</p> <p>Andrej Karpathy：或许吧。但我认为我们仍会经历一个过渡阶段——在这个阶段里，人类依然能通过自身理解力和创造力参与其中，推动科技向前。长期来看，这种能力可能会逐渐被AI取代，但在人类与AI共存的时期，我们仍能有所贡献。</p> <p>甚至我觉得，到了那个阶段，这种“参与”可能会演化成一种运动或竞赛。就像今天的力量举运动员不断挑战身体极限，那么在未来的“认知时代”，或许会有人类去挑战知识与思维的极限。这会成为一场“智力奥运会”。</p> <p>想象一下：当每个人都能拥有一个完美的AI导师，人类的潜能会被彻底释放。我们今天所称的“天才”，可能只是人类心智能力的冰山一角。未来，人类可能真的能靠认知训练变得“超人”。这并不是神话，而是我觉得非常现实的方向。</p> <p>Dwarkesh Patel：我太喜欢这个愿景了。事实上，我觉得这种未来最“对口”的人可能就是我——我的工作本身就要求我每周都要学习不同的领域知识。如果你真的能把学习做到像你说的那样，我会是第一个报名的人。</p> <p>Andrej Karpathy：在这点上我其实跟你挺像。很多人都讨厌上学，迫不及待地想离开学校，但我恰恰相反——我非常喜欢学习、热爱学校，甚至希望能一直读下去。我一路读到了博士，最后是因为学校“实在不让我再待了”，我才去了工业界。对我而言，学习本身既是一种乐趣，也是一种赋能的过程。它让我变得更有用、更有生产力。</p> <p>Dwarkesh Patel：我觉得你刚刚其实提到了一点很微妙的东西，我想把它明确说出来。到目前为止，网络课程之所以没能让“每个人都学到所有知识”，问题并不是内容本身，而是动机门槛太高。这些课程缺乏清晰的“上坡道”，学习者太容易被卡住，学不动、也坚持不下去。如果我们能像你说的那样，做出一个真正像“优秀导师”一样的系统，它能即时调整节奏、匹配难度，这在动机层面上会是一次彻底的“解锁”。</p> <p>Andrej Karpathy：我完全同意。因为“卡关”的体验真的很糟糕。你投入了时间和精力，却发现学不下去——要么太难，要么太无聊。那种心理上的“负反馈”会让人迅速失去兴趣。而一旦我们能做到真正的个性化匹配，学习其实是愉快的。对我来说，这完全是一个技术问题——我们要解决的，不是“人为什么不想学”，而是“系统为什么不够聪明”。 在未来一段时间内，教育的最佳状态大概是人类教师与AI协同。AI帮我们调整节奏、提供反馈，人类负责激发兴趣、理解情境。最终，也许有一天AI可以单独胜任这一切，但那还需要时间。</p> <p>Dwarkesh Patel：我想问一些关于“教学方法”的问题。如果你要给其他领域的教育者一些建议——尤其是在那些无法像编程那样用代码验证理解的学科——你会怎么告诉他们，如何制作像你那样的教学内容？</p> <p>Andrej Karpathy：这是个很宽泛的问题。我可能有十几条“半下意识”的小技巧，但从高层来看，我的教学思维很大程度上受益于我的物理学背景。我一直认为，物理是最能“启动人类大脑”的学科。基础教育的目标不应是为未来某个行业储备技能，而是让你的大脑学会思考。而物理学能让你学会建模、抽象与近似——这些思维习惯对任何领域都极其宝贵。</p> <p>举个例子：物理学家常说“假设有一头球形的牛”，很多人会笑，但这其实是极其聪明的思维方式。它教你抓住一阶近似（first-order term），去理解事物的核心结构，再逐步补上二阶、三阶的复杂细节。这种分层建模的思维，是理解世界的通用框架。</p> <p>我很喜欢Geoffrey West那本《Scale》，他用物理的方法研究生物尺度定律：动物体型越大，心跳越慢；散热能力取决于表面积（平方增长），而能量产生取决于体积（立方增长）。这些规律让你看到——哪怕是复杂的生命系统，也能用物理近似捕捉它的核心。</p> <p>所以，在教学中我总是去寻找“最重要的一阶项”：这个体系中，真正决定性的是哪一部分？我能否用最简洁的方式把它展示出来。举个我自己的例子：我写过一个叫 MicroGrad 的开源项目。只有一百行Python代码，却完整实现了神经网络的前向与反向传播。这一百行代码里，包含了神经网络学习的全部核心思想：链式求导、梯度传播、参数优化。当然，它不高效，缺少并行化与内存优化。但那些只是“效率工程”，而非“智识核心”。MicroGrad让人能在极简模型里真正“看懂”深度学习是怎么工作的。</p> <p>我热爱这种“把知识解耦与重排”的过程。教育的本质就是构建知识的上坡道（ramps to knowledge），让每个概念都依赖于前一个概念，不让学生被卡在中间。这其实是一个非常高智力的工作：你得不断拆解、简化、重组复杂系统，让知识从混乱的网络，变成一条可攀登的阶梯。对我而言，这种“理清知识的过程”本身就是最有趣的智力活动之一。</p> <p>图片 图片来源：The Nvidia Patterns</p> <p>Dwarkesh Patel：这样的教学方式真的能极大提升学习动机。比如你在讲解 Transformer 的视频里，从最简单的 bigram 模型开始——一个“前一个词 → 下一个词”的查表过程，简直就是一张最原始的词表。然后你再一步步引出 Transformer，每一层、每一个结构的添加都有明确动机：为什么需要这一部分？它解决了前一步的什么问题？这比直接记公式、背注意力机制的数学表达式要有意义得多。</p> <p>Andrej Karpathy：对，先让学生感受到“痛点”，再呈现解决方案。这其实非常巧妙。学习的过程应该是一段“渐进式的推理体验”。我总是希望学生能跟着这个节奏走。他们先遇到一个问题，然后我带他们看见为什么要发明下一个方法。这样的教学充满“参与感”，学生也会更投入。还有一些我认为非常重要的小技巧，很多优秀教师都会用。比如：不要直接给出答案。一定要先问学生，“你会怎么解决？”哪怕他们想错了也没关系，因为这个过程能让他们明确“问题空间”是什么。直接给答案其实是一种“偷走学习体验”的行为，有点不厚道。</p> <p>Dwarkesh Patel：对，因为当学生自己先尝试时，他们就能更清楚地理解——这个问题的行动空间是什么，目标是什么，为什么只有这个方案能满足目标。</p> <p>Andrej Karpathy：没错。只有在你尝试过、思考过之后，再看到正确答案，你才会对它产生真正的“理解与敬意”。那种“啊，原来如此”的瞬间会让学习效率最大化——每获得一个新知识点，就能带来最大的信息增量。</p> <p>Dwarkesh Patel：非常同意。那你觉得，为什么很多真正的专家——那些在领域里非常厉害的人——往往反而讲不好课？</p> <p>Andrej Karpathy：这其实是一个非常普遍的现象——知识的诅咒。当你成为专家后，你会下意识地假设很多基础是“理所当然”的，以至于你很难再站在初学者的角度思考。我自己也常常犯这个错。有一次，有人给我看一篇生物学论文，我完全看不懂。我满脑子都是“愚蠢的问题”，于是我干脆用ChatGPT，把论文放进上下文窗口里，开始问它这些问题。AI 会帮我理清最基础的概念，等我弄懂后，我就把这整段对话发回给那位研究员——他是那篇论文的作者。我觉得这其实特别有帮助。因为当一个专家看到别人是如何困惑的，他才能重新理解自己的知识体系里哪些地方“默认”太多了。所以我其实很希望，大家能把自己和 ChatGPT 的“傻问题对话”分享给我。那样我能看到学生真正在哪些地方卡壳，从而改进教学。</p> <p>Dwarkesh Patel：太有道理了。其实我觉得，这个问题在写作上也一样——无论是论文、博客还是产品公告。几乎在100%的情况下，如果你把作者的正式文本换成他们“午餐时的口头解释”，内容都会变得更清晰、更准确，甚至更科学。因为正式写作时，人们总喜欢把语言变得抽象、堆满术语，还要在开头铺垫几段“清嗓子式”的废话。而面对面交流时，你被迫直接“讲明白”，直接进入主题。这种“把话说清楚”的冲动，反而让知识的表达更真实。</p> <p>Andrej Karpathy：对，我看到那条推文的时候真的觉得写得太好了，我还转发给了好几个人。这种情况我见过太多次了。最典型的例子就是我还在读博士、做研究的时候，你读别人发表的论文，反复琢磨、费劲地想明白他们到底在做什么。然后过几个月在学术会议上碰到那位作者，大家一起喝啤酒聊天，你随口问一句：“嘿，你那篇论文到底是讲什么的？”结果他三句话就把核心思想讲得清清楚楚——那三句话完美地捕捉了论文的精髓，让你完全明白整篇论文在做什么，而根本不用读完那几十页。就像只有在酒桌上随意聊时，他才会这么说：“哦，其实就是把这个想法和那个想法结合一下，做了个实验试试看。”一句话直击本质。你就会想，为什么正式论文里不能这么写？</p> <p>Dwarkesh Patel：刚才我们一直在讨论“如何让解释者更好地讲清楚一个概念”。那从学习者的角度来说呢？如果一个学生没有像你这样的 Karpathy 来帮他讲透一篇论文——他只是自己读书、读论文——你会给他们什么建议？在非专业领域自学时，你是怎么做的？</p> <p>Andrej Karpathy：老实说，我也没有什么“独门秘籍”，这其实是一个挺“痛苦”的过程。但我确实有几个心得。</p> <p>第一，我发现“按需学习（learning on demand）”特别有效。也就是，当你正在做一个项目、要解决一个实际问题时，你带着明确目标去学习，这种学习是有“即时奖励”的。而相对的，学校教育里很多是“广度式学习”。老师告诉你：“先学这个，将来会用到的”，然后你就学了，但其实你当下感受不到任何价值。我喜欢的是那种“学即所用”的学习。你因为要做成一件事，被迫去理解知识，这时候吸收效率极高。</p> <p>第二个我觉得非常重要的是：教别人是最好的学习方式。每当我试图向别人解释某个概念时，我就会立刻暴露出自己理解的漏洞。你一讲就发现：“糟糕，我其实没搞懂这一点。” 这种不舒服的意识逼迫你回去查、去补，直到你能用自己的语言清楚讲出来。这就是为什么我喜欢不断地重讲、重写，哪怕只是给AI、给学生、或者写成推文。解释知识的过程会强迫你重新整理、操控并真正“拥有”它。</p> <p>Dwarkesh Patel：是的，这真是一个完美的收尾。Andrej，非常精彩，谢谢你。</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[Andrej Karpathy深度对话：Agent的十年征程与AI的幽灵本质]]></summary></entry></feed>