<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://emigmo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emigmo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-25T08:26:09+00:00</updated><id>https://emigmo.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">姚顺雨AI与Agent研究观点集</title><link href="https://emigmo.github.io/blog/2025/yao-shunyu-ai-agent-insights/" rel="alternate" type="text/html" title="姚顺雨AI与Agent研究观点集"/><published>2025-09-17T10:00:00+00:00</published><updated>2025-09-17T10:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/yao-shunyu-ai-agent-insights</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/yao-shunyu-ai-agent-insights/"><![CDATA[<h2 id="前言">前言</h2> <p>本文整理了OpenAI研究员姚顺雨关于AI与Agent系统的深度思考。作为从清华姚班到普林斯顿PhD，再到加入OpenAI的研究者，姚顺雨在AI发展的关键节点上提出了许多具有前瞻性的观点。他的”下半场”理论、记忆层级理论等思想为我们理解AI发展趋势提供了重要视角。</p> <hr/> <h2 id="个人背景与研究历程">个人背景与研究历程</h2> <h3 id="学术经历">学术经历</h3> <p><strong>标准化的成长路径：</strong></p> <blockquote> <p>“我感觉我是个非常乖的学生。从小到大就是按部就班的学习。本科从合肥考到清华，读姚班。在姚班大家会告诉你去美国读PhD，我就去美国读PhD，我在普林斯顿读PhD。读PhD之后很自然，OpenAI是做research最好的地方，就加入OpenAI——感觉我前28年的人生，非常的乖。”</p> </blockquote> <p><strong>时间线：</strong></p> <ul> <li>2015-2019年：清华姚班</li> <li>2019-2024年：普林斯顿大学PhD</li> <li>2024年：加入OpenAI</li> </ul> <h3 id="研究转向的关键时刻">研究转向的关键时刻</h3> <p><strong>从理论到应用的觉醒：</strong></p> <blockquote> <p>“当时，我觉得很多重要理论问题已经解决得差不多，比如将某个图算法的复杂度从n的2.83次方优化到n的2.82次方，这种改进在现实中意义不大。”</p> </blockquote> <p><strong>深度学习的启蒙：</strong></p> <blockquote> <p>“我在2016年上李建老师的一门课，看到一个multi-modal embedding的demo，展示了embedding一个非常神奇的例子：比如用’king’的embedding减去’man’，再加上’woman’，结果接近’queen’的embedding——这让我第一次意识到，深度学习在语义表示上居然能做到这么惊艳的计算。”</p> </blockquote> <p><strong>从视觉到语言：</strong></p> <blockquote> <p>“最初我做的是Computer Vision，但渐渐意识到Vision很难实现通用人工智能。我的直觉告诉我：Language是一个更核心、更有潜力的方向，于是读博后转向语言模型研究。”</p> </blockquote> <h3 id="非共识的选择">非共识的选择</h3> <p><strong>早期的非共识：</strong></p> <blockquote> <p>“我一直有这个非共识：我想要去做Agent。”</p> </blockquote> <blockquote> <p>“当时做的人很少，因为它太难了，或者不是一个共识类的事情。当时共识类任务是做问答，做翻译，或者做一些已经被社区接受的任务。”</p> </blockquote> <p><strong>简单通用的追求：</strong></p> <blockquote> <p>“我一直想做简单且通用的东西。我不想做一个很复杂、但只能在一个领域奏效的东西。这个方向在传统意义上很难被接受，大家习惯了做AI的方式：把问题不停细分，做很多细分方法。”</p> </blockquote> <hr/> <h2 id="agent系统的本质认知">Agent系统的本质认知</h2> <h3 id="agent的定义与演进">Agent的定义与演进</h3> <p><strong>广义定义：</strong></p> <blockquote> <p>“任何能进行自我决策、与环境交互，并试图optimize reward的系统，都可以被称为Agent。”</p> </blockquote> <p><strong>现代Agent的特点：</strong></p> <blockquote> <p>“今天我们讲的Agent更多是指：怎么基于语言模型这样的foundation model去做具备自我决策能力的系统，而不是传统意义上基于规则或仅在某个领域用强化学习训练出来的Agent。”</p> </blockquote> <p><strong>三波Agent发展：</strong></p> <ol> <li><strong>第一波：符号主义AI</strong> <ul> <li>基于规则的推理系统</li> <li>专家系统的尝试</li> <li>局限：无法覆盖所有情况，导致AI寒冬</li> </ul> </li> <li><strong>第二波：深度强化学习</strong> <ul> <li>DeepMind的游戏AI、AlphaGo</li> <li>OpenAI的机器手、Dota</li> <li>局限：环境特定，无法泛化</li> </ul> </li> <li><strong>第三波：基于语言模型</strong> <ul> <li>具备推理能力</li> <li>可以进入数字环境（编程、互联网）</li> <li>核心：方法线+任务线的共同进化</li> </ul> </li> </ol> <h3 id="language-agent的核心优势">Language Agent的核心优势</h3> <p><strong>开放空间决策能力：</strong></p> <blockquote> <p>“如果你要做一个language Agent，你需要的不只是选择能力，而是去自由产生新动作的能力。世界的本质就是，你的行为空间是open-ended的，这种在开放空间决策的能力BERT永远做不到。”</p> </blockquote> <p><strong>与传统Agent的区别：</strong></p> <blockquote> <p>“最大区别在于，语言模型提供了一个足够强的先验，这个先验让你可以推理，而推理又可以在不同的环境间泛化。”</p> </blockquote> <h3 id="推理能力是泛化的关键">推理能力是泛化的关键</h3> <p><strong>人类vs AI的差异：</strong></p> <blockquote> <p>“为什么我可以一下子去玩一个新的游戏，但现在这些系统或AI需要几十万步甚至几百万步训练，才能完成类似的事？我发现，是因为我可以思考。”</p> </blockquote> <p><strong>推理的价值：</strong></p> <blockquote> <p>“如果没有这样的思考能力，而是直接从复杂语言去预测’我要往后走’，就很难——没有推理做不到。核心是推理能力，推理才能带来泛化。”</p> </blockquote> <hr/> <h2 id="技术发展的下半场理论">技术发展的”下半场”理论</h2> <h3 id="上半场vs下半场">上半场vs下半场</h3> <p><strong>转折点的判断：</strong></p> <blockquote> <p>“主线正从’上半场’转向’下半场’。我说的主线是基于语言的智能体。从语言出发，去定义Reasoning、定义Agent，我们终于有了一个非常general的方法，而且这个方法是可泛化的——我们实现了一个基点时刻。”</p> </blockquote> <p><strong>本质变化：</strong></p> <blockquote> <p>“这带来一个本质变化：以前我面对很多怪兽，需要造出各种不同武器去打它们；现在我有了一把通用武器，比如机关枪，我不需要再为每个怪兽单独造武器。接下来要思考的问题就变成：我该朝哪个方向开枪？”</p> </blockquote> <h3 id="从训练模型到使用模型">从训练模型到使用模型</h3> <p><strong>瓶颈的转移：</strong></p> <blockquote> <p>“大家过去往往更关注模型训练、方法设计，但我觉得现在的bottleneck已经转移了：变成怎么去定义好的任务，怎么去定义好的环境。”</p> </blockquote> <p><strong>研究价值的重新定位：</strong></p> <blockquote> <p>“当时最有价值的，就是去研究怎么使用模型。如果你想训练模型，会落后OpenAI或这些公司好几年。你做的很有可能几年前别人已经发现了。如果你想做不一样的，可能怎么去使用模型更有价值。”</p> </blockquote> <h3 id="任务定义的重要性">任务定义的重要性</h3> <p><strong>环境的重要性：</strong></p> <blockquote> <p>“第二个learning是：任务或环境非常重要。当你有一个非常差的任务，你永远不可能学到非常好的东西。”</p> </blockquote> <p><strong>好任务的标准：</strong></p> <blockquote> <p>“首先你要找一个足够有挑战的任务，这个任务能做出本质的新方法。”</p> </blockquote> <hr/> <h2 id="openai能力分级体系解读">OpenAI能力分级体系解读</h2> <h3 id="l1-l3的递进逻辑">L1-L3的递进逻辑</h3> <p><strong>逻辑关系：</strong></p> <blockquote> <p>“逻辑是，首先你要有语言的先验知识。基于语言的先验知识，最早能做出来的应用是Chatbot（L1）。接下来，基于语言先验，你需要具备推理能力，这是Reasoner（L2）。当你既有语言知识，又具备推理能力，才可能进一步做各种Agent（L3），尤其是能泛化的Agent。”</p> </blockquote> <h3 id="l4与l5的正交关系">L4与L5的正交关系</h3> <p><strong>并行发展的观点：</strong></p> <blockquote> <p>“我一开始是认为Innovator（L4）和Organization（L5）是更正交或并列的关系。我当时在群里问了一个问题：当一个大公司CEO和一个科学家，到底哪一个难？这个不好说，实现路径有区别。所以，不用太纠结谁是第四级，谁是第五级，都很重要。不一定要先实现哪一个才能实现另一个，可以同时去探索。”</p> </blockquote> <p><strong>L4（创新者）的要求：</strong></p> <ol> <li><strong>Long-Term Memory</strong>： <blockquote> <p>“你作为一个Innovator，首先你需要一个Long-Term Memory。比如，我是Wiles，我研究费马大定理，可能花了20年。我就需要一个长期记忆。”</p> </blockquote> </li> <li><strong>内在奖励机制</strong>： <blockquote> <p>“我有这个长期记忆还不够，还需要有内在的reward。因为在你真正证明那件事之前，没有任何外部奖励——你没有获奖，没有做成任何’可交付’的事情，也没人给你feedback。你需要自己给自己反馈。”</p> </blockquote> </li> </ol> <p><strong>L5（组织者）的挑战：</strong></p> <blockquote> <p>“作为一个Organization，你需要解决的问题是：Agent和Agent之间怎么协作？怎么让Multi-Agent协作scale？”</p> </blockquote> <h3 id="实现路径与技术突破">实现路径与技术突破</h3> <p><strong>三个关键方向：</strong></p> <blockquote> <p>“在fundamental research上，比较重要的有三方面：一个是Memory，一个是Intrinsic Reward，还有一个是Multi-Agent。”</p> </blockquote> <p><strong>当前水平的定位：</strong></p> <blockquote> <p>“现在的Agent就像一个普通大学生，做一个数字化的实习生。或者说，AGI就是一个普通一本大学生在电脑上能做所有事情的一个能力。”</p> </blockquote> <p><strong>人类社会的价值边界：</strong></p> <blockquote> <p>“但是，人类社会的边界是什么？这当然覆盖80%或90%的人。但我们最崇拜的人，是哪两种？一种是创造新东西，在认知或审美上开创新领域的人：爱因斯坦、高更、梵高、贝多芬；另一种是能创造新组织、伟大组织的人：伊隆·马斯克、乔布斯。”</p> </blockquote> <hr/> <h2 id="code作为ai的手">Code作为AI的”手”</h2> <h3 id="数字世界的affordance">数字世界的Affordance</h3> <p><strong>类比人类的手：</strong></p> <blockquote> <p>“Code有点像人的手。它某种程度上，是AI最重要的affordance。对于物理世界，人最重要的affordance是手——我们围绕它制造各种工具，比如锤子、笔、筷子。但对AI、对Digital Agent来说，最重要的affordance可能就是code。”</p> </blockquote> <p><strong>天然的机器语言：</strong></p> <blockquote> <p>“因为其他东西，都是给人定义的。比如网页、小说、视频，是为人类设计的；但code是一个天然就给机器使用的表达形式。”</p> </blockquote> <h3 id="api-vs-gui的争论">API vs GUI的争论</h3> <p><strong>经典辩论：</strong></p> <blockquote> <p>“有个非常经典的debate：最终的AGI，是基于API或code的？还是基于GUI？或者是为人定义的前端环境？还是它是一个混合体？”</p> </blockquote> <p><strong>现实的解决方案：</strong></p> <blockquote> <p>“这个问题有点像：你是想改造你的车让它适应所有路，还是改造所有路让它适应现在的车？当然，最终结果很可能是meet in the middle，两边都会做，而且这个事情可能没那么难。”</p> </blockquote> <h3 id="编程环境的特殊价值">编程环境的特殊价值</h3> <p><strong>多轮反馈的重要性：</strong></p> <blockquote> <p>“我们当时做了一个工作叫InterCode。大家都在做的是：给一个coding task模型生成一段代码，然后你去evaluate它。但我们就在想：为什么不把执行结果反馈给模型？我们可以让它变成一个多轮Agent task，构造成一个环境，而不是单次完成的任务。”</p> </blockquote> <p><strong>非共识的坚持：</strong></p> <blockquote> <p>“有时候，很有意思的一点：一个东西明明非常重要，但就是没人做。如果你是一个研究员，觉得你做的事很重要，但别人不觉得、也没人做，并不是坏事——可能它真的很重要，只是大家还没开始。”</p> </blockquote> <hr/> <h2 id="任务设计与评估哲学">任务设计与评估哲学</h2> <h3 id="基于结果的奖励机制">基于结果的奖励机制</h3> <p><strong>设计原则：</strong></p> <blockquote> <p>“我从很早就有一个偏好：我想定义一个基于结果的reward，而不是基于过程的；而且这个reward应该是基于规则、可计算的，而不是来自人的偏好、模型的偏好，或者一些黑盒指标。”</p> </blockquote> <p><strong>成功案例的特征：</strong></p> <blockquote> <p>“像math和coding这种任务，之所以能做出来，核心就是：Reward是基于结果，不是基于过程；Reward是白盒的、基于规则的，不是基于人的偏好或模型的偏好。”</p> </blockquote> <p><strong>避免Hacking的重要性：</strong></p> <blockquote> <p>“但如果你reward是基于过程，就会出现hacking。你去优化人的偏好、模型的偏好，也会出现hacking。比如你生成一段非常优美的代码，但它并不解决实际问题。”</p> </blockquote> <h3 id="passk-vs-passk">Pass@k vs Pass^k</h3> <p><strong>两种不同的评估需求：</strong></p> <blockquote> <p>“有些任务我们需要优化的是Pass@k（多次尝试中至少成功一次），而另一些任务，比如客服，我们需要优化的是Pass^k（每次都成功），或者我们最关心的是Pass@1（一次就要成功）。”</p> </blockquote> <p><strong>任务特性的差异：</strong></p> <ul> <li><strong>创造性任务</strong>：允许多次失败，只要有一次成功</li> <li><strong>可靠性任务</strong>：需要极高的稳定性，每次都要成功</li> </ul> <h3 id="robustness的重要性">Robustness的重要性</h3> <p><strong>被忽视的问题：</strong></p> <blockquote> <p>“现在我们对于简单任务的robustness并没有特别重视——这是因为大家做AI还是在做一些benchmark，而不是实际应用。”</p> </blockquote> <p><strong>思维转变的价值：</strong></p> <blockquote> <p>“但如果你接受了这个mindset转变，很自然你就会意识到：有些应用是需要特别强调robustness的，那你就需要去优化它的robustness。现在大家还没完全意识到这件事；但我相信，如果大家意识到这个转变，会带来很大进步。”</p> </blockquote> <hr/> <h2 id="语言与泛化的本质">语言与泛化的本质</h2> <h3 id="语言作为通用工具">语言作为通用工具</h3> <p><strong>独特性的来源：</strong></p> <blockquote> <p>“为什么语言非常独特？因为它是人在这个世界完成各种各样事情的工具。语言也是人类发明的工具，像火或笔一样。但它之所以特殊，是因为它是一个帮助你解决任何事情的通用性或泛化性的工具。”</p> </blockquote> <p><strong>与其他工具的区别：</strong></p> <blockquote> <p>“当你学会了这门工具，你可以去做很多新任务。比如你学会了攀岩，它帮不了你完成新任务。但你学会了语言，你可以通过语言和人交流，学习、思考、推理。”</p> </blockquote> <p><strong>本质认知：</strong></p> <blockquote> <p>“2020年以前，大家没把这个事想清楚，觉得语音、文字、图像、视频都是一些数据，没什么区别。但我觉得最大区别是：语言是人为了实现泛化而发明出来的工具，这一点比其他东西更本质。”</p> </blockquote> <h3 id="强化学习的泛化能力">强化学习的泛化能力</h3> <p><strong>历史性突破：</strong></p> <blockquote> <p>“我之所以这么说，是因为在此前，如果你在一个特定环境上训练，模型只能在这个环境表现良好，不能轻易迁移到其他环境。但现在，你在一个环境上训练，模型可以适应更多不同环境，这才是最本质的区别。”</p> </blockquote> <p><strong>具体表现：</strong></p> <blockquote> <p>“DeepSeek大家觉得一个有趣结果是：你在数学和编程领域用强化学习训练模型，但它在创意写作上也变得更强。这体现了本质区别：AlphaGo只能下围棋，不能下象棋；而现在你学会数学，也能提高创意写作。”</p> </blockquote> <p><strong>泛化的机制：</strong></p> <blockquote> <p>“但我觉得，它还是泛化的。原因是它能够推理。当你能在一个环境学到如何思考的技能，并且这种思考能力能迁移到新环境，这才是泛化的本质原因。”</p> </blockquote> <h3 id="内在激励机制">内在激励机制</h3> <p><strong>创新者的驱动力：</strong></p> <blockquote> <p>“就像我刚刚说的，很多创新者之所以能在没有外在激励的情况下坚持，是因为他有内在的价值观或激励机制。”</p> </blockquote> <p><strong>婴儿的好奇心模型：</strong></p> <blockquote> <p>“这个问题，AI和神经科学已经研究多年。婴儿是最典型的例子。他们拥有基于好奇心或自我激励的机制。很多婴儿会反复玩一个玩具，用嘴去咬一个东西，或者做一些看似’无意义’的动作。”</p> </blockquote> <p><strong>成长的转变：</strong></p> <blockquote> <p>“当人长大之后，会发生重要变化。当你是婴儿，你对世界的理解，是基于视觉、触觉，基于物理世界的。当你长大之后，你对世界的理解方式变了，变成一个基于语言、推理、文字系统的理解。你玩的，不再是一个物理游戏，而是一个文字游戏。”</p> </blockquote> <p><strong>AI面临的挑战：</strong></p> <blockquote> <p>“这是AI面临的挑战：传统AI，比如玩迷宫、做机器人仿真，它可以定义一些基于世界模型或者模仿婴儿阶段好奇心的内在激励。但当AI在玩的是一个语言游戏，要怎么定义内在激励？——这个问题就变得不太一样了。”</p> </blockquote> <hr/> <h2 id="记忆层级理论">记忆层级理论</h2> <h3 id="环境作为最外层记忆">环境作为最外层记忆</h3> <p><strong>冯诺依曼的洞察：</strong></p> <blockquote> <p>“前年冬天，我读到冯诺依曼临终前写的一本书，The Computer and the Brain。最让我印象深刻的一句话是：Essentially, the Environment is always the most outer part of the Memory Hierarchy.（基本上，环境永远是记忆层级中最外层的部分。）这很哲学。”</p> </blockquote> <p><strong>人类的记忆层级：</strong></p> <blockquote> <p>“对于人，你有你的Memory Hierarchy，有Working Memory、Long-Term Memory在脑子里，但最外层是你的笔记本、Google Doc、Notion，这些是你最外层Memory Hierarchy的一部分。”</p> </blockquote> <p><strong>Agent的记忆层级：</strong></p> <blockquote> <p>“某种程度上，是的。从Agent角度看，这个世界有一个Memory Hierarchy。Memory Hierarchy最外层永远是环境。”</p> </blockquote> <h3 id="long-context-vs-long-term-memory">Long Context vs Long-Term Memory</h3> <p><strong>实现关系：</strong></p> <blockquote> <p>“Long Context是实现Long-Term Memory的一种方式。如果你能实现1亿或1千亿或无限长的Context，它是实现Long-Term Memory的一种方式。它是一种和人区别很大的方式，但这是有可能的。”</p> </blockquote> <p><strong>评估的问题：</strong></p> <blockquote> <p>“起码到去年为止，大家主要还在做所谓Long Range Arena，比如needle in the haystack——我有一个很长的输入，我在中间插入一句话，比如’姚顺雨现在在OpenAI’，然后我问你相关问题。这是一个必要但不充分的任务。”</p> </blockquote> <h3 id="context的经济价值">Context的经济价值</h3> <p><strong>人类不可替代的原因：</strong></p> <blockquote> <p>“为什么我们现在的模型，推理很强，考试很强，玩游戏很强；但它还没创造出足够经济价值？——根本原因是：它没有这些Context。”</p> </blockquote> <p><strong>分布式系统的特点：</strong></p> <blockquote> <p>“人类社会比较tricky的一点是：当然，我们确实写下了很多东西——我们用文字、Google Doc、Notion，记录了很多东西；但很多Context永远只存在人的大脑，是通过一个分布式的系统来维护。”</p> </blockquote> <p><strong>解决方案的价值：</strong></p> <blockquote> <p>“如果这个问题解决了，Utility问题就可以在很大程度被解决。这个世界，大多数人并不是乔布斯，也不是爱因斯坦，只是一个普通人。他的数学推理没有o3强，但他能manage Context。”</p> </blockquote> <hr/> <h2 id="创业与产品思考">创业与产品思考</h2> <h3 id="创业公司的机会">创业公司的机会</h3> <p><strong>正确的担心：</strong></p> <blockquote> <p>“创业公司应该担心的是模型没有溢出能力，这样你就真的什么都做不了了。有溢出能力是个非常好的事情，这几乎意味着你有机会。”</p> </blockquote> <p><strong>最大机会所在：</strong></p> <blockquote> <p>“创业公司最大机会是：能设计不同的interface，或者说人和数字世界交互的方式。”</p> </blockquote> <p><strong>成功的条件：</strong></p> <blockquote> <p>“对于创业公司，最好的机会是：你做新的交互方式，并且模型不停有新的溢出能力，让你能够赋能这些新的交互方式——两者缺一不可。”</p> </blockquote> <h3 id="交互方式的创新">交互方式的创新</h3> <p><strong>大厂的路径依赖：</strong></p> <blockquote> <p>“但拥有一个Super App对于公司是双刃剑。当你已经有了一个交互方式，你必然形成路径依赖。当你有像ChatGPT这样的Super App，很自然你的研究就会center around这个Super App，会center around这个交互方式。”</p> </blockquote> <p><strong>Cursor的价值：</strong></p> <blockquote> <p>“Cursor是很好的例子，创造了一种新的交互。不是像人一样的交互，而是像Copilot。写代码的时候，它能给你提示或编辑。没有人和人是这样交互的。这是它的价值所在。”</p> </blockquote> <p><strong>通用性与应用的平衡：</strong></p> <blockquote> <p>“一个比较理想的情况，你有一个非常通用的交互方式，这个交互方式想象力足够大。但并不矛盾的是，你可以有每个阶段的Killer App。”</p> </blockquote> <h3 id="数据飞轮的条件">数据飞轮的条件</h3> <p><strong>成功案例分析：</strong></p> <blockquote> <p>“比较成功的是Midjourney，有非常清晰的reward——人更喜欢哪张图，这个reward和应用是对齐的，reward做得更好，公司就更成功，模型也更好——一切都对齐。有了这种情况，才能自己训练模型，做数据飞轮。”</p> </blockquote> <p><strong>必要条件：</strong></p> <blockquote> <p>“如果你要有数据飞轮，首先你要能自己去训模型，并且能通过交互有很好的reward，使你能把好的数据和不好的数据分开。”</p> </blockquote> <p><strong>相互借鉴的关系：</strong></p> <blockquote> <p>“这世界是个相互抄的关系，而不是一个单向抄的关系。”</p> </blockquote> <hr/> <h2 id="未来生态构想">未来生态构想</h2> <h3 id="多元化vs单极化">多元化vs单极化</h3> <p><strong>多面向系统的观点：</strong></p> <blockquote> <p>“对于不同的任务和交互，需要不同的Agent系统去解决。模型是可以share的，但如果你讨论的是整个系统，那就不一样了。就像你问，这个世界上最强的互联网网站是什么？最强的互联网公司是什么？很难回答。它是一个multiface的系统，有很多不同侧面。”</p> </blockquote> <p><strong>避免灰暗的未来：</strong></p> <blockquote> <p>“AI可能也会变成这样的结构。OpenAI可能会成为一个类似Google的公司，成为新世界里非常重要的一环——但这并不代表，这个世界就会被这样一个单极系统垄断。如果真是那样，这个世界就会变得很灰暗。大多数人也就没什么价值了。”</p> </blockquote> <h3 id="agent社会的可能性">Agent社会的可能性</h3> <p><strong>信息差的价值：</strong></p> <blockquote> <p>“为什么这个世界上很多人有价值？不是因为他们的数学或编码能力强，而是因为他们拥有别人没有的信息。中间商本质是拥有信息差。拥有信息差的人会想维护自己的权利和资源。”</p> </blockquote> <p><strong>分布式网络的形态：</strong></p> <blockquote> <p>“在交易世界里，信息很重要，每个人只拥有信息的一小部分，这种情况会出现新的不同形态。可能是Multi-Agent，每个人有自己的Agent，Agent之间可以与百万甚至更多人交换信息，达成交易或某些目的。”</p> </blockquote> <h3 id="中心化与去中心化的平衡">中心化与去中心化的平衡</h3> <p><strong>双重趋势：</strong></p> <blockquote> <p>“我最近的一个思考是这样：我感觉人类社会是一个网络，它有两个重要性质：一个性质是中心化程度，也可以说是资源分配的集中性。还有另一个维度，是你从网络边缘到中心的速度或可能性。”</p> </blockquote> <p><strong>历史的观察：</strong></p> <blockquote> <p>“过去几百年发生的事情是这样：网络越来越中心化，贫富差距越来越大，二八定律、马太效应更明显；但与此同时，平民或普通人翻身的机会也变多了。”</p> </blockquote> <p><strong>技术发展的影响：</strong></p> <blockquote> <p>“看起来，技术发展的趋势是两件事同时加剧——一方面，中心化加剧，因为效率这个因素是根本性的；另一方面，创造新东西的机会，起码到目前为止，是越来越多的。变得更中心化和变得更diverse，可能并不矛盾。”</p> </blockquote> <p><strong>力量的平衡：</strong></p> <blockquote> <p>“根本上，现在非常强的巨头和重要节点，有动力继续推动中心化。但在中心化之外的力量，也有动力做一些非中心化的事情。这个世界可能不会是单方压倒另一方，双方都会有自己的力量。”</p> </blockquote> <hr/> <h2 id="人与ai的关系思考">人与AI的关系思考</h2> <h3 id="效用导向的设计原则">效用导向的设计原则</h3> <p><strong>核心判断标准：</strong></p> <blockquote> <p>“Again，这是一个Utility Problem。很多问题上，人的方式并不一定更有价值。比如下围棋、开车。但有些事情，人就是做得更好。那你就应该思考，怎么去bridge the gap？”</p> </blockquote> <p><strong>具体应用场景：</strong></p> <blockquote> <p>“下围棋、打游戏，基于强化学习可以学到和人不一样、甚至更好的方式，就不需要像人。但如果在一个公司打工，和老板搞好关系，完成各种各样的任务，人就是比AI做得更好，就需要更像人。”</p> </blockquote> <p><strong>拟人化的条件：</strong></p> <blockquote> <p>“一个事情如果有价值，就会产生。比如，很多人很孤独，他需要一个朋友，技术如果能创造这样的体验，拟人化就是合理存在的未来。”</p> </blockquote> <h3 id="从认知科学到第一性原理">从认知科学到第一性原理</h3> <p><strong>认知的转变：</strong></p> <blockquote> <p>“我现在觉得，一个更好的方法是：你先去思考人能做什么，而机器现在不能做。这是客观事实。但你找到差异之后，你可以基于第一性原理去思考，如何解决这个问题。你不一定要依赖’人是怎么解决这个问题的’来解决它。”</p> </blockquote> <p><strong>借鉴的边界：</strong></p> <blockquote> <p>“所以，从人身上可以借鉴的一点：哪些事情是人可以做，而机器目前不能做？这点比较robust和客观。但至于’人是怎么做到的’，以及’我们在多大程度上要借鉴这种方式’，这个问题本身更主观、也更noisy。”</p> </blockquote> <h3 id="安全与价值的权衡">安全与价值的权衡</h3> <p><strong>现实的优先级：</strong></p> <blockquote> <p>“我会担心。但现在最大问题是——AGI还没实现，我们还没创造足够价值。如果我们还没想清楚，怎么把它变得有价值，就急着把它变得很安全，好像没有意义。”</p> </blockquote> <p><strong>商业驱动的安全：</strong></p> <blockquote> <p>“安全是很复杂的问题。比如ChatGPT，如果它不安全，产品就失败了，没有商业价值。即使是为了商业价值，它也会重视安全。”</p> </blockquote> <hr/> <h2 id="openai内部视角">OpenAI内部视角</h2> <h3 id="非共识决策的勇气">非共识决策的勇气</h3> <p><strong>历史的选择：</strong></p> <blockquote> <p>“但问题在于，如果你没有一个different bet，很难超越前面的霸主。如果OpenAI一直做强化学习，可能很难超过DeepMind。即使你在某些任务上做得比它好，人们提到强化学习，想到的还是DeepMind。”</p> </blockquote> <p><strong>GPT的赌注：</strong></p> <blockquote> <p>“你要想超越之前的霸主，就必须有一个different bet。而GPT是那个不同的赌注——但这个选择在当时是一个非共识的事情。”</p> </blockquote> <p><strong>内部的分歧：</strong></p> <blockquote> <p>“我说实话，当时OpenAI内部绝大多数人也不认为scale-up是最promising的方向，我觉得这是有可能的。”</p> </blockquote> <p><strong>领导者的价值：</strong></p> <blockquote> <p>“Ilya最大贡献并不是他做了GPT‑1，或者他具体参与了什么技术工作；而是，他是那个号召大家all in这个方向的人。”</p> </blockquote> <h3 id="强化学习的持续重要性">强化学习的持续重要性</h3> <p><strong>历史的连续性：</strong></p> <blockquote> <p>“历史并不是说我把强化学习彻底抛弃，转而走另一条路，再返回来走强化学习，而是更soft的过程。”</p> </blockquote> <p><strong>产品化的关键：</strong></p> <blockquote> <p>“后来证明，ChatGPT成功，强化学习也很关键。没有RLHF，没有Alignment技术，它也没办法形成一个产品。”</p> </blockquote> <h3 id="扩展维度的未来">扩展维度的未来</h3> <p><strong>新的可能性：</strong></p> <blockquote> <p>“会有新的scaling dimension出现。如果你有大量的Memory，你的test-time compute就会有所增加，可以用新的方式scale。如果你有了Multi-Agent，那你的test-time compute又会出现另一个新维度去扩展。”</p> </blockquote> <p><strong>复杂的选择：</strong></p> <blockquote> <p>“我觉得会有新的scale dimension出现，但当你有很多scale dimension，怎么去选择？怎么基于某一个应用去分配不同scale维度的比重？——这会是一个很有意思的问题。”</p> </blockquote> <hr/> <h2 id="个人哲学与价值观">个人哲学与价值观</h2> <h3 id="通用性的追求">通用性的追求</h3> <p><strong>从小的特质：</strong></p> <blockquote> <p>“我从小是一个比较general的人——我想试图变得很通用，试图了解很多不同的学科，做很多不同的事情。”</p> </blockquote> <p><strong>认知的升华：</strong></p> <blockquote> <p>“但后来我发现，一个人即使再聪明、再有精力，他能理解的知识或能做的事情，也只是人类社会积累的知识的很小一部分。更好的是，你去创造一个比你更通用、更general的事情。”</p> </blockquote> <p><strong>执念的力量：</strong></p> <blockquote> <p>“我好像一直对于通用性，有一种执念或追求。”</p> </blockquote> <h3 id="创造不同的驱动力">创造不同的驱动力</h3> <p><strong>价值的追求：</strong></p> <blockquote> <p>“用一个非常俗的话说，希望你对这个世界创造一些不同——探索新的、根本性的研究，是一种创造不同的方式；创造一种完全不同的新的产品形态，也是一种创造不同的方式。”</p> </blockquote> <p><strong>导师的智慧：</strong></p> <blockquote> <p>“我导师令我印象最深的是这样一句话。学术圈经常发生这样的事——你有一个想法，然后别人做了，你会很烦。他说：If someone else can do it, then it’s okay to let them do it（如果别人能做，那就让他们去做吧）。”</p> </blockquote> <p><strong>全局的思考：</strong></p> <blockquote> <p>“从人类全局的角度，如果这个事情很多人能做，别人做可能是不是也没有什么区别？对这个社会，或者对整体来说，似乎没有什么变化。”</p> </blockquote> <h3 id="时代机遇的把握">时代机遇的把握</h3> <p><strong>时代的特殊性：</strong></p> <blockquote> <p>“但我觉得恰好是这个时代，你去做上限更高的事情是更好的。因为现在有一个巨大的机会。如果没有这样一个巨大的机会，最佳路径可能是去做incremental、确定性强的事情，一步一步地积累。但恰好有一个上限非常高的事情。”</p> </blockquote> <p><strong>勇气的回报：</strong></p> <blockquote> <p>“如果你敢想，或者你胆子特别大，或者你想象力很丰富，就会有好事发生。”</p> </blockquote> <p><strong>技术的通用性：</strong></p> <blockquote> <p>“但这个时代很幸运的一点：这个技术非常通用，这个技术非常伟大，有足够多探索的空间。”</p> </blockquote> <p><strong>个人的选择：</strong></p> <blockquote> <p>“另一点是，我想让生活更有趣，更有意思，更快乐，就去做一些自己喜欢的事情。这很难用语言解释，就是一个taste或preference的问题。”</p> </blockquote> <hr/> <h2 id="结语">结语</h2> <p>姚顺雨的观点体系体现了一个研究者从技术深度到哲学高度的完整思考。他不仅在技术层面提出了”下半场”理论、记忆层级理论等重要观点，更在价值层面思考了AI发展的多元化可能性。他的非共识选择、对通用性的追求，以及对创造差异化价值的坚持，为AI研究者和创业者提供了重要的思维框架。</p> <p>他的核心信念可以概括为：<strong>在技术快速发展的时代，要有勇气做非共识的选择，追求简单而通用的解决方案，并始终以创造真实价值为导向。</strong> 这种哲学不仅适用于AI研究，也适用于更广泛的创新活动。</p> <hr/> <p><em>本文整理自姚顺雨的多次访谈和演讲内容，旨在为AI研究者和从业者提供思考框架和启发。</em></p>]]></content><author><name></name></author><category term="research-insights"/><category term="AI"/><category term="Agent"/><category term="LLM"/><category term="research"/><summary type="html"><![CDATA[OpenAI研究员姚顺雨关于AI Agent发展的深度思考与独到见解]]></summary></entry><entry><title type="html">杨植麟观点精华：AI时代的技术哲学与实践</title><link href="https://emigmo.github.io/blog/2025/yang-zhilin-ai-philosophy/" rel="alternate" type="text/html" title="杨植麟观点精华：AI时代的技术哲学与实践"/><published>2025-07-15T14:00:00+00:00</published><updated>2025-07-15T14:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/yang-zhilin-ai-philosophy</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/yang-zhilin-ai-philosophy/"><![CDATA[<h2 id="前言">前言</h2> <p>本文基于2025年7月对月之暗面创始人杨植麟的深度访谈整理而成。作为AI领域的重要创业者和技术专家，杨植麟在这次访谈中分享了他对AI发展的深度思考，从哲学层面的认知框架到具体的技术实践，从产品理念到组织管理，构建了一个完整的AI时代思考体系。</p> <hr/> <h2 id="一技术哲学与认知框架">一、技术哲学与认知框架</h2> <h3 id="11-无限的山问题与解决的永恒循环">1.1 无限的山：问题与解决的永恒循环</h3> <p>杨植麟深受David Deutsch的《无穷的开始》影响，认为AI研发正处于类似启蒙运动后的动态知识创造状态：</p> <p><strong>核心观点</strong>：</p> <ul> <li><strong>问题的不可避免性</strong>：”问题是不可避免的”和”问题是可以解决的”是可以刻在石头上的两句话</li> <li><strong>知识边界的拓展</strong>：每当解决一个问题，就会带来新的问题，因为知识边界在不断拓展</li> <li><strong>技术进步的本质</strong>：每解决一个问题，技术就能往上攀登几百米</li> </ul> <p><strong>实践体现</strong>：</p> <ul> <li>解决了强化学习问题，又遇到评估、衡量、验证的新问题</li> <li>AI研发是一个不断解决问题、产生新问题的过程</li> <li>“也许有一天会发现，这座雪山没有尽头——我希望它一直没有尽头”</li> </ul> <h3 id="12-agi不是终点而是方向">1.2 AGI不是终点，而是方向</h3> <p><strong>对AGI的重新定义</strong>：</p> <ul> <li>AGI不是某一级台阶，不会突然一夜之间达到</li> <li>它是一个持续的方向，而非固定的终点</li> <li>今天在很多领域已经可以认为达到了AGI水平（如数学、编程竞赛）</li> </ul> <p><strong>两个层面的理解</strong>：</p> <ol> <li><strong>技术层面</strong>：技术能力持续提升</li> <li><strong>社会层面</strong>：技术对人类社会的影响，需要几十、几百年消化</li> </ol> <p><strong>与登月的区别</strong>：</p> <ul> <li>登月有明确的成功标志</li> <li>AI很难在某个时间点宣布”实现了AGI”</li> <li>这是一个动态进化的过程</li> </ul> <h3 id="13-ai是人类文明的放大器">1.3 AI是人类文明的放大器</h3> <p><strong>AI的本质定位</strong>：</p> <ul> <li>AI是”人类文明的放大器”和”巨大的杠杆”</li> <li>它将成为Meta science（元科学）</li> <li>从启蒙运动到现在，下一个突破知识边界的是AI</li> </ul> <p><strong>对人类价值的思考</strong>：</p> <ul> <li>人的一生有三个意义：<strong>创造、体验和爱</strong></li> <li>“创造”的很大部分AI可以做，但”体验”和”爱”会是以人为中心的</li> <li>人类的独特价值在AI时代会持续存在</li> </ul> <p><strong>风险与应对</strong>：</p> <ul> <li>承认AI摧毁人类文明的风险存在</li> <li>但不能因噎废食，放弃就等于放弃了人类文明的上限</li> <li>需要更安全的对齐和更好的社会机制</li> </ul> <h2 id="二技术范式与发展路径">二、技术范式与发展路径</h2> <h3 id="21-两种推理范式缸中之脑vs交互式智能体">2.1 两种推理范式：”缸中之脑”vs交互式智能体</h3> <p><strong>“缸中之脑”模式（如o1）</strong>：</p> <ul> <li>特点：在自己大脑里思考，不需要与外界交互</li> <li>机制：通过让模型做很多尝试和反思，反思是重点</li> <li>能力：提出新猜想 + 验证猜想</li> <li>本质：把Pass@k变成Pass@1</li> </ul> <p><strong>交互式智能体模式</strong>：</p> <ul> <li>特点：与外界进行多轮交互</li> <li>行为：边思考边操作（搜索、浏览器、写代码等）</li> <li>优势：下一步行为基于交互反馈和状态更新</li> </ul> <p><strong>共同指向</strong>：</p> <ul> <li>两种范式都指向<strong>Test-time Scaling</strong>（测试时扩展）</li> <li>通过规模化token来完成更复杂任务</li> <li>能够端到端完成复杂工作，如代码仓库翻译、调试、测试</li> </ul> <h3 id="22-l1-l5能力等级的非线性发展">2.2 L1-L5能力等级的非线性发展</h3> <p><strong>OpenAI分级体系</strong>：</p> <ul> <li>L1：Chatbot（聊天机器人）</li> <li>L2：Reasoner（推理者）</li> <li>L3：Agent（智能体）</li> <li>L4：Innovator（创新者）</li> <li>L5：Organizer（组织者）</li> </ul> <p><strong>杨植麟的核心观点</strong>：</p> <ul> <li><strong>非串行关系</strong>：这些能力不一定是互相依赖的线性发展</li> <li><strong>技术路径选择</strong>：可以先做Agent再做Reasoning，如Claude的路线</li> <li><strong>最终融合</strong>：要做到最好的Agent，必须把Reasoning也做到最好</li> </ul> <p><strong>L4创新者的关键</strong>：</p> <ul> <li>标志：模型参与模型本身的开发</li> <li>实现：K2参与K3的开发过程</li> <li>依赖：需要强大的Agentic能力</li> </ul> <p><strong>L4与L3的互相促进</strong>：</p> <ul> <li>用L4的技术去解决L3的问题</li> <li>Agent泛化不够，需要用创新去解决</li> <li>体现了能力发展的非线性特征</li> </ul> <h3 id="23-test-time-scaling的核心价值">2.3 Test-time Scaling的核心价值</h3> <p><strong>定义与意义</strong>：</p> <ul> <li>在测试时或推理时实现更好的规模化</li> <li>突破传统单轮对话的token限制</li> <li>通过增加轮数或每轮思考token数来完成复杂任务</li> </ul> <p><strong>实现方式</strong>：</p> <ul> <li>长思考的强化学习</li> <li>Agent的强化学习</li> <li>多轮交互和工具使用</li> </ul> <p><strong>价值体现</strong>：</p> <ul> <li>能够花几小时完成复杂任务，无需人工参与</li> <li>实现端到端的复杂工作流程</li> <li>大幅提升模型的实际应用能力</li> </ul> <h2 id="三模型训练与技术创新">三、模型训练与技术创新</h2> <h3 id="31-token-efficiency突破数据墙的关键">3.1 Token Efficiency：突破数据墙的关键</h3> <p><strong>问题背景</strong>：</p> <ul> <li>高质量数据增长缓慢，接近常数</li> <li>多模态数据无法很好提升文本”智商”</li> <li>Scaling Law遇到数据墙</li> </ul> <p><strong>解决方案</strong>：</p> <ul> <li>提升<strong>Token Efficiency</strong>，把一份数据当成几份用</li> <li>使用Muon优化器，获得两倍提升效果</li> <li>通过数据改写（Rephrase）提升泛化能力</li> </ul> <p><strong>Muon优化器的优势</strong>：</p> <ul> <li>不是独立考虑每个元素，而是整体考虑矩阵参数的依赖关系</li> <li>在compute optimal情况下，学一份数据相当于用Adam学两份数据</li> <li>解决了大规模训练中的max logit爆炸问题</li> </ul> <p><strong>数据改写策略</strong>：</p> <ul> <li>对高质量数据进行多种改写操作</li> <li>避免同一份数据多次学习的过拟合问题</li> <li>通过改写增加一定程度的泛化能力</li> </ul> <h3 id="32-强化学习范式的转变">3.2 强化学习范式的转变</h3> <p><strong>技术路线转变</strong>：</p> <ul> <li>从预训练+SFT转向预训练+强化学习</li> <li>发现不需要太多process reward或value function</li> <li>直接用端到端的reward就能训练得很好</li> </ul> <p><strong>强化学习的优势</strong>：</p> <ul> <li>泛化性比SFT更好</li> <li>有更多on-policy采样，模型从自身采样中学习</li> <li>具有负梯度，scaling效率比Pre-Training高很多</li> </ul> <p><strong>挑战与限制</strong>：</p> <ul> <li>泛化仍然有限，”种瓜得瓜，种豆得豆”</li> <li>需要搭配好的评估和验证机制</li> <li>希望通过更多AI参与训练来摆脱局限</li> </ul> <h3 id="33-agent泛化性的挑战与解决">3.3 Agent泛化性的挑战与解决</h3> <p><strong>核心挑战</strong>：</p> <ul> <li>Agent最缺的是泛化能力</li> <li>现有RL技术局限于单点任务和评价指标</li> <li>容易过拟合到特定工具、环境或任务</li> </ul> <p><strong>泛化的重要性</strong>：</p> <ul> <li>如果泛化能力强，垂直Agent就没那么必要</li> <li>通用Agent能泛化到长尾工具，解决专有问题</li> <li>只需接入定制数据库、API、文档接口即可</li> </ul> <p><strong>解决思路</strong>：</p> <ol> <li><strong>用AI训练AI</strong>：让模型参与更多训练过程</li> <li><strong>AI native方式</strong>：摆脱人工设计的局限</li> <li><strong>更好的评估机制</strong>：解决Benchmark不够用或失效的问题</li> <li><strong>课程学习</strong>：从中等难度任务开始，逐步提升</li> </ol> <p><strong>Agent的两个关键特征</strong>：</p> <ul> <li><strong>多轮</strong>：实现test-time scaling</li> <li><strong>工具</strong>：连接”脑”与外部世界</li> </ul> <h2 id="四产品与商业思考">四、产品与商业思考</h2> <h3 id="41-模型即产品的理念">4.1 模型即产品的理念</h3> <p><strong>核心理念</strong>：</p> <ul> <li>训练模型时就要把整套系统搭好</li> <li>模型训练完成，产品也基本完成</li> <li>产品在训练过程中完成，而非训练后开发</li> </ul> <p><strong>Agent产品的特殊性</strong>：</p> <ul> <li>需要把模型与工具和Context结合</li> <li>模型性能在训练中已经与工具、环境适配好</li> <li>交互改进只是锦上添花</li> </ul> <p><strong>系统复杂性</strong>：</p> <ul> <li>简单：所有东西放在同一个模型，不需要维护多个模型</li> <li>复杂：要让模型在各种场景下都能工作，对通用性要求很高</li> <li>挑战：避免只拟合单点能力，要保证真正的通用性</li> </ul> <h3 id="42-开源vs闭源的战略选择">4.2 开源vs闭源的战略选择</h3> <p><strong>对开源的重新认识</strong>：</p> <ul> <li>承认之前”领先者不会开源”的判断，因为月之暗面在全球范围内还没完全领先</li> <li>开源更多是赋能下游应用，而非反哺基础模型提升</li> <li>社区贡献主要在推理侧，模型本身的提升仍只有原厂能做</li> </ul> <p><strong>开源的价值</strong>：</p> <ul> <li>可基于开源模型做Agentic Post-Training，催生专用智能体</li> <li>与社区分享技术know-how，加速技术提升</li> <li>形成开源生态，推动技术发展</li> </ul> <p><strong>战略平衡</strong>：</p> <ul> <li>希望长期分享更多技术，但不一定只做开源</li> <li>既有技术信仰，也有市场博弈策略</li> <li>最终希望让技术更安全、更快达到更好水平</li> </ul> <h3 id="43-一方产品的竞争优势">4.3 “一方产品”的竞争优势</h3> <p><strong>定义</strong>：</p> <ul> <li>模型公司自己做产品，控制上下文环境、工具接口、prompt结构</li> <li>自己当”使用方”，而不只是提供API</li> </ul> <p><strong>相对于”三方产品”的优势</strong>：</p> <ul> <li>正向设计vs逆向工程</li> <li>先设计好工具和Context Engineering，再在此环境中训练模型</li> <li>模型天然在自己环境中表现更好</li> <li>可以更好整合工具和模型，端到端训练</li> </ul> <p><strong>发展趋势</strong>：</p> <ul> <li>Claude Code、ChatGPT Agent都是”一方产品”</li> <li>上限可能更高，但不一定能覆盖所有Agent领域</li> <li>与”三方产品”会形成合作与竞争并存的生态</li> </ul> <h2 id="五组织管理与个人成长">五、组织管理与个人成长</h2> <h3 id="51-用rl方式管理团队">5.1 用RL方式管理团队</h3> <p><strong>管理哲学的转变</strong>：</p> <ul> <li>科研、模型训练、组织管理都遵循RL原理</li> <li>从SFT式管理转向RL式管理</li> <li>核心是掌握SFT和RL的平衡</li> </ul> <p><strong>RL管理的特点</strong>：</p> <ul> <li>给团队成员目标和奖励，而非具体指令</li> <li>保持团队成员的主观能动性和创新能力</li> <li>建立多个观测指标，避免过拟合单一目标</li> </ul> <p><strong>挑战与风险</strong>：</p> <ul> <li><strong>Reward Hacking</strong>：容易被利用漏洞，看起来结果很好但实际没达到目标</li> <li><strong>奖励定义</strong>：需要深入理解具体细节，合理定义reward</li> <li><strong>平衡艺术</strong>：SFT太多会失去创造力，RL过度会被hack</li> </ul> <h3 id="52-技术决策的方法论">5.2 技术决策的方法论</h3> <p><strong>决策原则</strong>：</p> <ul> <li>基于充分的实验数据，不能拍脑袋</li> <li>需要非常了解实验的具体结果</li> <li>技术战略是公司战略的关键部分</li> </ul> <p><strong>关键技术bet</strong>：</p> <ul> <li>很早投入long CoT的RL</li> <li>采用新的优化器（Muon）</li> <li>做更大规模的Pre-Training</li> <li>做第一个开源的Agentic模型</li> </ul> <p><strong>决策流程</strong>：</p> <ul> <li>持续思考下一代模型应该什么样</li> <li>看工具箱里有什么新技术可以用</li> <li>通过实验验证技术的有效性</li> <li>数据足够充分时，判断比较显然</li> </ul> <h3 id="53-创业心态与价值观">5.3 创业心态与价值观</h3> <p><strong>核心驱动力</strong>：</p> <ul> <li>“寻找真相的过程，去不断发现新问题、解决它的过程”</li> <li>认为AI很重要，是人类文明的放大器</li> <li>享受攀登无限之山的过程本身</li> </ul> <p><strong>心态管理</strong>：</p> <ul> <li>“做时间的朋友”</li> <li>“不以物喜，不以己悲”，避免情绪化决策</li> <li>关注当前能做什么，而非过度担忧未来</li> </ul> <p><strong>成长感悟</strong>：</p> <ul> <li>最大成长：认识到问题不可避免但可以解决，持续解决新问题是最有意思的</li> <li>在自己的故事里不断感受和思考</li> <li>很多复杂性是人为强加的，实际并没有那么复杂</li> </ul> <p><strong>对成功的理解</strong>：</p> <ul> <li>只要每往上爬，成功概率就会变大</li> <li>会有恐惧，但更重要的是专注当下这一步</li> <li>任何中间状态都可能被批评，但要在投入”不变”的东西和适应调整之间找平衡</li> </ul> <hr/> <h2 id="总结">总结</h2> <p>杨植麟的思想体系体现了一个技术创业者的深度思考：从哲学层面理解AI发展的本质规律，到技术层面的具体创新实践，再到组织管理和个人成长的方法论。他将AI视为人类文明的放大器，将技术发展视为攀登无限之山的过程，既有理想主义的高度，又有实用主义的深度。</p> <p>这种思想框架不仅指导着月之暗面的技术发展方向，也为AI时代的创业者提供了有价值的思考范式。在AI快速发展的当下，这样的深度思考显得尤为珍贵，它提醒我们在追求技术突破的同时，也要思考技术的本质、价值和意义。</p>]]></content><author><name></name></author><category term="ai-insights"/><category term="AI"/><category term="philosophy"/><category term="LLM"/><category term="moonshot"/><category term="technology"/><summary type="html"><![CDATA[月之暗面创始人杨植麟关于AI发展的深度思考，从技术哲学到实践创新的全面洞察]]></summary></entry></feed>