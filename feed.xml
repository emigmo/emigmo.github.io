<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://emigmo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emigmo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-23T03:01:51+00:00</updated><id>https://emigmo.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Nick Joseph访谈：Anthropic预训练的核心思考与实践</title><link href="https://emigmo.github.io/blog/2025/anthropic-pretraining-nick-joseph/" rel="alternate" type="text/html" title="Nick Joseph访谈：Anthropic预训练的核心思考与实践"/><published>2025-10-10T15:00:00+00:00</published><updated>2025-10-10T15:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/anthropic-pretraining-nick-joseph</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/anthropic-pretraining-nick-joseph/"><![CDATA[<h2 id="前言">前言</h2> <p>本文基于Y Combinator于2025年9月30日对Anthropic预训练团队负责人Nick Joseph的深度访谈整理而成。Nick Joseph曾在Vicarious和OpenAI从事AI安全与模型缩放研究，深度参与了多代大语言模型的开发与优化。</p> <p><strong>核心观点速览：</strong></p> <ul> <li>预训练的核心是推动损失函数下降，这是我们一直追求的唯一目标</li> <li>对齐问题在于如何让模型分享人类的目标，尤其是在模型比人类更聪明时</li> <li>如果拥有无限计算资源，真正的挑战将是如何有效利用这些资源并解决规模扩展中的工程难题</li> <li>当前AI研究最大的瓶颈之一是计算资源受限，而非算法突破</li> </ul> <hr/> <h2 id="目录">目录</h2> <ul> <li><a href="#一从ai安全初心到anthropic预训练掌舵人">一、从AI安全初心到Anthropic预训练掌舵人</a></li> <li><a href="#二预训练基石与扩展定律自回归建模成为ai发展的核心引擎">二、预训练基石与扩展定律：自回归建模成为AI发展的核心引擎</a></li> <li><a href="#三工程实战深水区万卡集群硬件极限调试与分布式训练挑战">三、工程实战深水区：万卡集群、硬件极限调试与分布式训练挑战</a></li> <li><a href="#四合成数据风险模型评估指标设计与价值嵌入博弈">四、合成数据风险、模型评估指标设计与价值嵌入博弈</a></li> <li><a href="#五agi之路范式转移焦虑架构变革与未来展望">五、AGI之路：范式转移焦虑、架构变革与未来展望</a></li> </ul> <hr/> <h2 id="一从ai安全初心到anthropic预训练掌舵人">一、从AI安全初心到Anthropic预训练掌舵人</h2> <h3 id="11-从vicarious到openai的早期探索">1.1 从Vicarious到OpenAI的早期探索</h3> <p><strong>Ankit Gupta</strong>：大家好，今天非常高兴邀请到Anthropic的预训练负责人Nick Joseph来和我们对谈。我想先聊一聊你的背景，了解一下你是如何走到今天这一步的。</p> <p><strong>Nick Joseph</strong>：我之前在Vicarious工作，后来去了OpenAI，然后才加入Anthropic。Vicarious最初是一家以AGI为目标的实验室，我加入时他们正处于转型阶段，逐渐开始开发一些具体的产品，尤其是机器人相关的项目。我当时负责的主要是为机器人产品训练计算机视觉模型。那是我的第一份工作，所以我在那段时间主要学会了如何构建机器学习模型，以及如何搭建机器学习的基础设施。</p> <h3 id="12-选择实践而非学术的职业路线">1.2 选择实践而非学术的职业路线</h3> <p><strong>Ankit Gupta</strong>：那时候你有没有考虑过走学术路线？</p> <p><strong>Nick Joseph</strong>：其实我对这件事的想法有点不同，这很大程度上源于我之前在一个叫GiveWell的非营利机构实习的经历。GiveWell主要负责评估慈善项目的效果，当时那里有一些人提出——也许未来我们会拥有AGI，而它可能带来风险，需要提前关注这种潜在的威胁。</p> <p>那时我对这种说法并不是特别信服，我更倾向于直接去做一些可以帮助贫困人口的事情。后来因为种种原因，这条路没有走通，于是我决定至少去做AI相关的工作。这样无论未来AI安全是不是一个重大问题，我都能有所贡献：如果它真的重要，我就投身其中；如果不是，我也能用AI创造出一些能切实帮助贫困人群的东西。</p> <blockquote> <p><strong>所以我并不是以学术研究为出发点来进入这个领域。实际上，我之所以选择这条路，其中一个吸引我的地方是——我可以立即投入到AI实践中去。</strong></p> </blockquote> <h3 id="13-ai安全的早期状态与哲学探讨">1.3 AI安全的早期状态与哲学探讨</h3> <p><strong>Ankit Gupta</strong>：那么，当时AI安全这方面的研究处于什么状态呢？</p> <p><strong>Nick Joseph</strong>：在我看来，当时关于AI安全的大部分讨论都还停留在理论层面。那时候的模型其实并没有多强，也不存在真正的威胁。因此，当时的讨论更多是哲学性的——比如，假设未来我们会拥有比人类更聪明的AI，那我们是否应该提前重视这种潜在风险？</p> <h3 id="14-加入openai与代码能力的惊人突破">1.4 加入OpenAI与代码能力的惊人突破</h3> <p><strong>Ankit Gupta</strong>：接下来你去了OpenAI。当时的OpenAI是什么样的？</p> <p><strong>Nick Joseph</strong>：我当时加入的是一个安全研究团队，同时也在参与代码模型的相关研究。刚到那儿时，我看到的第一个项目就是他们对GPT-3进行微调，让它能够写代码。而且效果相当好。</p> <p>这让我突然意识到——如果人们担心AI会变得极其强大，甚至能自我编写、改进自己的代码，那么这种能力确实看起来有可能导致自我提升。于是我开始做很多评估和研究，分析哪些因素会促成这种能力。</p> <p>大概八个月后，我所在团队的几位AI安全负责人相继离开，而我之所以加入OpenAI，本身就是因为我非常关心AI安全问题，希望能和他们共事。后来他们中的一些人去了Anthropic，我也几乎在Anthropic成立初期就跟随加入了。</p> <hr/> <h2 id="二预训练基石与扩展定律自回归建模成为ai发展的核心引擎">二、预训练基石与扩展定律：自回归建模成为AI发展的核心引擎</h2> <h3 id="21-什么是预训练从互联网数据到下一个词预测">2.1 什么是预训练？从互联网数据到下一个词预测</h3> <p><strong>Ankit Gupta</strong>：既然提到了Anthropic，我们就来聊聊你现在的工作吧。如今你是Anthropic的预训练团队负责人。你能先谈谈什么是”预训练”吗？</p> <p><strong>Nick Joseph</strong>：我们知道让AI模型变得更强的关键要素之一就是”规模”。你需要投入大量的算力。而如果退一步思考，要想让模型获得尽可能多的算力，我们就需要一个拥有海量数据的训练目标。</p> <p>一个显而易见的想法是——互联网本身就是庞大的数据源，可能是人类历史上最大的数据集合。而且这些数据是无标签的，你不可能指望有人去把整个互联网内容都看一遍、逐条标注。因此，我们希望能从数据本身提取出”标签”。</p> <blockquote> <p><strong>于是，就有了这样的思路：我们可以让模型预测下一个词。这种方式的好处是，你能得到非常密集的学习信号——每一个词都是一个新的训练样本。</strong></p> </blockquote> <p>从GPT-1到GPT-2的研究发现，只要你持续增加算力、使用更多数据、训练更大的模型，模型的能力就会不断增强，表现也会更智能。可以说，这就是整个预训练理念的核心假设。</p> <h3 id="22-扩展定律与正反馈循环">2.2 扩展定律与正反馈循环</h3> <p>这里还涉及”扩展定律”的概念，也就是说，我们可以用相当精确的方式去量化：当你增加算力、数据量或模型参数时，模型在预测下一个词时的损失会以可预期的方式降低，性能也会相应提升。</p> <p>真正出乎意料的是，这一机制形成了一个<strong>“正反馈循环”</strong>：</p> <ol> <li>你训练出一个模型</li> <li>它能被用来创造有用的产品</li> <li>这些产品带来收入</li> <li>你再把收入投入更多算力</li> <li>从而训练出更好的模型</li> </ol> <p>过去五年左右，我们其实反复地在运行这一循环。</p> <h3 id="23-为什么自回归建模胜出">2.3 为什么自回归建模胜出？</h3> <p><strong>Ankit Gupta</strong>：”下一个词预测”这种自回归方式似乎已经成为主流的预训练方法。但如果回到2017年至2020年，当时其实存在各种各样不同的预训练目标。例如BERT、BART这些模型采用的是”掩码语言建模”。你对那个阶段有什么回顾或感想吗？</p> <p><strong>Nick Joseph</strong>：我认为答案主要是经验驱动的——换句话说，我们是通过大量实验得出的。我的观点是，这类问题最终要靠实证：都试一试，看哪种更有效。</p> <p><strong>自回归建模的巨大优势：</strong></p> <ul> <li>你可以直接从模型中采样生成文本，这个过程非常自然</li> <li>损失函数本身就能直接反映出你真正关心的目标</li> <li>如果你能把这个任务做到完美，模型自然就能像人一样写作</li> <li>容易转化为产品应用</li> </ul> <p>相比之下，其他一些方法并不具备这种天然的生成特性。</p> <h3 id="24-算力才是王道">2.4 算力才是王道</h3> <p><strong>Nick Joseph</strong>：没错，这种方式赋予了模型最具开放性的潜力。一般的流程是：你先训练一个基础模型，然后针对不同任务进行微调。</p> <p>不过我总体的直觉是：<strong>真正起决定作用的是算力</strong>。只要你投入足够的算力，无论采用哪种预训练目标，最终都能训练出表现不错的模型，然后再通过微调适配各种用途。</p> <blockquote> <p><strong>令人意外的是，很多我们以为关键的细节，其实远不如”增加算力”来得重要。</strong></p> </blockquote> <hr/> <h2 id="三工程实战深水区万卡集群硬件极限调试与分布式训练挑战">三、工程实战深水区：万卡集群、硬件极限调试与分布式训练挑战</h2> <p>Ankit Gupta：确实如此。而且当我们谈到“增加算力”时，这本身也有很多维度。比如，对于一个固定的模型架构，你可以给它输入更多数据；或者让模型更深，增加层数或参数量；再或者通过神经架构搜索去尝试不同结构的组合。我想如今大家已经比较明确哪种架构更有效，但在早期阶段，这方面的探索应该更具不确定性。能否谈谈当时你们是怎么思考这些问题的？你们的基础设施又是怎样支持这些探索和决策的？</p> <p>Nick Joseph：我想，简短的答案是——这真的很难。你要做的事情其实是训练一个庞大而昂贵的模型，而你面对的是一个包含上百个超参数的空间。比如你要决定层数、宽度等等，而你希望所有这些超参数都能最优。你需要在“它们到底重要到什么程度”之间取得平衡——也就是说，你能不能凭直觉选一个差不多的配置，然后只要增加算力就能得到不错的效果？</p> <p>Ankit Gupta：也就是说，没关系你怎么调都行？</p> <p>Nick Joseph：对，只要别太离谱。但有趣的是，实际上这并没有那么重要。我记得这在早期的扩展定律论文里也提到过——你可以调整这些参数获得一些小的提升，但只要你持续增加计算资源，模型的性能一般都会稳定地提升。当然，如果你调得太离谱，效果会停止提升，但你又不会知道到底出了什么问题。这其实是最难的部分之一。</p> <p>Ankit Gupta：因为你不知道反事实是什么，对吧？你没有足够长时间去跑出结果。</p> <p>Nick Joseph：对。我们有这些“扩展定律”，你可以预期，当你增加计算量时，损失值会按幂律下降——实际上是“幂律+常数”。所以最终你会看到这条曲线开始偏离幂律，这就意味着出问题了。而这时问题可能是根本性的，也可能只是你应该微调一下学习率。如何判断是哪一种，就是一大挑战。通常的做法是先在小规模下测试，再在大规模上运行。</p> <p>Ankit Gupta：这里的小规模是指数据规模，还是别的？</p> <p>Nick Joseph：是所有方面。你希望比例缩放，比如如果你要把计算量增加十倍，你就要有一套理论去指导：这十倍的算力该怎么分配——多少给层数、多少给数据、多少给注意力机制。然后在小规模下按比例缩放去验证。</p> <p>Ankit Gupta：那在Anthropic早期，你们团队可能十来个人的时候，作为一个小型但有资金的创业公司，你们当时能用到什么样的大规模基础设施？</p> <p>Nick Joseph：这其实是件挺疯狂的事。你永远不知道别人到底在做什么，但我们感觉自己就在最前沿。那时关心这件事的人其实很少。我当时的心态是：我们在做AGI，这是人类历史上最重要的技术。但我环顾四周，发现好像全世界只有30个人在认真干这件事。虽然我只是个初级工程师，但我惊讶于事情的“简单”——比如当时公开估算GPT-3的训练成本是500万美元。听起来多，但从公司角度看其实不算高。所以我们完全能买到足够的算力来训练类似规模的模型。</p> <p>Ankit Gupta：你们是用云计算，还是自建机房？</p> <p>Nick Joseph：我们用的是云服务，但其实差别不大。让我意外的是，我们真的得理解硬件的物理布局。有次同事甚至跑了聚类算法来推测芯片分布在哪些机房，因为我们怀疑不同机房间的网络延迟导致训练瓶颈。结果真能“反推”出来两个聚类，连接状况不同。这种极限优化在当时很重要——我们资金比别人少，只能靠算力效率取胜。</p> <p>Ankit Gupta：那你们具体做了什么来提升算力利用率？这听起来像早期Google那种“便宜硬件+极致优化”的故事。</p> <p>Nick Joseph：我们主要是把分布式框架调到极致。因为训练要跨大量GPU并行进行。分布式有多种模式：数据并行、流水线并行、模型并行等，要把这些都整合好。</p> <p>Ankit Gupta：那时候还没有现成的开源框架能直接用，对吧？</p> <p>Nick Joseph：对，有一些，但很有限。比如我们当时用的数据并行方法需要自己写all-reduce通信，不能完全依赖现有包。因为像PyTorch Distributed虽然有工具，但我们要扩展到比Facebook还大的规模，就得自己写，以便后续能灵活修改。</p> <p>Ankit Gupta：你刚说“我们要比Facebook更大规模”，这挺颠覆的。毕竟那时Facebook AI Research是最顶尖实验室之一。你们不觉得冒险吗？</p> <p>Nick Joseph：确实有点意外。但也许我有点自信过头吧。当时我觉得他们都忽略了重点。扩展定律已经很清楚了，而反对的论点听起来没道理。那篇原始论文横跨11个数量级，学界却争论它能不能再延伸一个数量级——我觉得，这种怀疑没什么根据。</p> <p>Ankit Gupta：毕竟都已经延展11个数量级了。</p> <p>Nick Joseph：对，所以我觉得继续下去的概率相当高。而且这类规律很多时候真的就是“直接有效”。但我能理解，从外部看就没那么显然——论文太多，每篇都说自己很重要。尤其像FAIR那种地方，研究者更独立，重视发表，而不是去协调一整个庞大的工程项目。训练一个大型语言模型需要几十人协作搭建复杂的基础设施，而那种成果不会是一篇论文，所以那类文化对这事其实并不重视。</p> <p>Ankit Gupta：你提到你们有时要修改PyTorch底层实现，那你们是停留在高层API还是直接写CUDA？</p> <p>Nick Joseph：看情况。大多数操作我停留在torch.matmul这种层面，不去管矩阵乘法内部实现。但像注意力机制这种复杂操作，我们会深入优化底层，因为它在GPU上效率很难调。</p> <p>Ankit Gupta：那你们会先纸上推导理论效率，再实现？</p> <p>Nick Joseph：对。你其实可以用纸笔算出理论上能达到的最大效率（MFU）。效率差的原因通常就是显存带宽瓶颈、CPU传输瓶颈等等。这些约束项不多，也就六七个。建模清楚后再实现。实现后用profiler分析每步耗时，对比预期，再不断优化。</p> <p>Ankit Gupta：那时有现成的分析工具吗？</p> <p>Nick Joseph：单GPU的Python profiler已经不错，但成百上千GPU的分布式Profiling几乎没人做过，我们得自己改写分析器来聚合所有追踪数据。</p> <p>Ankit Gupta：那你这些都是怎么学的？</p> <p>Nick Joseph：我入职Anthropic时，内部资料还少，我第一天就把整个内部Wiki都读完了。然后主要靠结对编程学，像Tom Brown、Sam McCandlish这些人之前都做过。我跟他们大量结对，看他们怎么调Profiler、怎么排错。比如我以前从没用过调试器，只靠print调试，后来才发现PDB调试器的效率高太多。</p> <p>Ankit Gupta：后来你们的预训练规模越来越大，算力暴涨——那策略上发生了什么变化？</p> <p>Nick Joseph：其实变化不大，真的令人惊讶。到今天我还在盯着和当年一样的指标——损失函数。可以把五年前第一个模型的loss曲线和现在的放在一起看，核心目标一直没变。</p> <p>Ankit Gupta：所以你们的OKR就是“loss越低越好”？</p> <p>Nick Joseph：对，当然现在的变化是团队更专业化。早期我会看每个PR，知道所有模块；后来各模块分工更细，有人专攻attention，有人专攻并行策略。但这样带来一个挑战：要平衡“专家”和“通才”。如果全是通才，大家懂点皮毛没人精通；全是专家，又容易出现系统割裂，需要管理者去把体系连起来。我个人更喜欢保持平衡。</p> <p>Ankit Gupta：那在算力规模暴涨后，有没有遇到一些意想不到的挑战？</p> <p>Nick Joseph：有，比如最典型的——“连接问题”。当你要并联越来越多的GPU时，最小的硬件故障都可能导致整个训练崩溃。比如一台GPU坏了，整个模型就会挂掉。你可以想象，如果模型每层在不同GPU上，那掉一个“第七层”的GPU就意味着整个网络失效。</p> <p>还有一点，整个技术栈都太新，从芯片架构到数据中心布局都在快速演进。以前我写代码出错会想“肯定是我写错了”，但现在有时真的是“电脑错了”。比如有次GPU真的坏了，换了之后一切正常。如今你得考虑的变量太多：GPU可能有瑕疵、供电可能波动、数据中心线路可能老化等等，对一个Python程序员来说，这些都是意料之外的复杂性。</p> <p>Ankit Gupta：那早期你们单次训练大概用多少GPU？</p> <p>Nick Joseph：上千个，能塞进一个机房。现在则是一整栋楼、甚至园区。那时我们还在研究：这些GPU是不是得放在同一个房间？带宽要多少？供电能不能扛得住？有时候只是一个电容不足，都可能导致整个训练任务瞬间断电崩溃。</p> <p>Ankit Gupta：那么，你们是否需要考虑不同类型芯片之间的差异？我的意思是，你们和各种不同的云服务提供商都有合作。从你的角度来看，这些芯片只是计算资源吗？还是说如果你们使用TPU和GPU，它们就像谷歌的CPU和英伟达的GPU那样？作为工程师，你在使用这两者进行训练时，是否需要采取不同的思路？</p> <p>Nick Joseph：是的，从根本上来说，它们做的都是同样的事情，对吧？都是在进行计算。进行同一类操作、矩阵乘法等等。它们实现的方式相当不同，编程方式也很不同。而且实际规格也差别很大。有些芯片可能拥有很多浮点运算能力，但内存不多；有些可能内存带宽大，但内存容量有限。所以拥有多种芯片在某些方面很有优势，这意味着你可以根据任务特点选择最合适的芯片来执行。</p> <p>Ankit Gupta：比如，有些任务更适合在TPU集群上执行，而另一些更适合在NBHP集群上？你们会怎么选择？你可以谈谈这个。</p> <p>Nick Joseph：有趣，我举个例子，推理任务通常需要更多的HBM带宽，因为你在一次时间步中需要加载每个token的所有权重，这意味着需要很高的HBM带宽。而预训练通常更依赖于浮点运算，因为批量更大。所以你可以针对不同用途选择不同芯片。但拥有多种芯片的缺点是，你可能需要为每种芯片重复编写代码。理论上可以通过抽象层来统一，但实际差异太大，很难做到。因此如果你处理所有工作负载和所有芯片，工作量会按芯片数量成倍增加。</p> <p>Ankit Gupta：关于你提到计算机有时会出故障，我记得你之前做过类似事情。我当时公司在使用Google TPU时遇到一些奇怪的segfault错误，你告诉我一个有效的方法，如果六个月前用上它们，就能解决一半的问题。我可以想象，你们在使用这些非常新的芯片时，会遇到很多问题，需要和提供商紧密合作来解决。</p> <p>Nick Joseph：是的，项目组在解决问题上很积极。有趣的是如何找到合适的合作方式。他们有强烈动机去修复问题，因为希望芯片能正常工作，从而未来能卖更多芯片。我们当然也有很强动机让芯片工作，因为我们提前大量采购这些芯片。所有工作都建立在让集群正常运行的基础上，但我们不一定能共享所有信息。有些信息不能完全共享。因此一种策略是制作小规模复现环境，当遇到问题时，你可以在单芯片或单文件上复现，然后发给对方进行调试。</p> <p>Ankit Gupta：你们是在共享的Slack上交流，互相发送问题和数据吗？还是说你们和大厂的人员实际上在同一个办公室？</p> <p>Nick Joseph：主要是共享Slack，有时候见面更有效，但Slack是常用方式。</p> <p>合成数据风险、模型评估指标设计与价值嵌入博弈 Ankit Gupta：那我们谈谈近期预训练的现状。这几年，各家公司对预训练的关注似乎有所分散，除了预训练，还有后训练的强化学习、微调和安全性调整等。从外部看，预训练似乎相对不那么受关注，而推理类模型的进展主要依赖后训练。你怎么看？这种理解是否合理？在推理和新型后训练方法的时代，有没有预训练层面仍然重要的因素，对实现优秀模型有影响？</p> <p>Nick Joseph：以前预训练意味着做一个大规模训练，但其实已经有变化，比如直接进行大量自由训练，把大部分计算资源用于训练。现在人们发现强化学习也能带来很大收益，你可以把更多算力投入强化学习，从而得到更好的模型。因此如何平衡预训练和后训练、各自作用叠加还是互补，这些问题仍在早期阶段，还没有定论。</p> <p>Ankit Gupta：你认为这些更多是经验性问题吗？像我们之前讨论的，你会尝试多种方法看效果，还是有一些基于原理的方式来判断？</p> <p>Nick Joseph：最终还是经验为主。可以提出理论，但实践中大多数理论都需要验证，多半是错的。所以最可靠的办法是收集数据再做判断。解决问题的经验方法对于做出正确决策至关重要，但在组织内很难实现。关键是不要因为你管理预训练，就坚持预训练必须胜出。</p> <p>Ankit Gupta：团队间是否存在某种竞争？还是视为同一个整体？</p> <p>Nick Joseph：我们这边合作性很强，基本上是共同产出一个模型。但据我了解，有些公司团队间存在摩擦，这是一个有趣的组织设计问题：如何设置团队，避免科学问题被个人团队观念绑架。</p> <p>Ankit Gupta：关于预训练，你如何看待高质量数据的可用性？你们已经训练了几乎所有互联网文本。外界常说数据已经枯竭，是否如此？尤其当大量数据由AI生成时，是否存在模式崩塌风险，模型会过拟合AI生成数据？</p> <p>Nick Joseph：我对数据问题看到很多自信的说法。有人说互联网数据已经用尽，我不确定实际情况。数据的质量与数量总是有权衡。基本事实是数据量巨大，增长速度慢于算力增长。</p> <p>Ankit Gupta：也就是说，虽然新数据在增加，但算力也在增加，不容易判断哪个增长更快。</p> <p>Nick Joseph：我需要强调，我并不完全确定。一方面互联网似乎无限，你可以不断生成文本。但“有用的互联网”规模无人知晓。</p> <p>Ankit Gupta：我脑子里简单想法是，用PageRank过滤出有价值的网页，不是可行吗？</p> <p>Nick Joseph：我认为不完全可行。人眼认为有价值的内容与模型学习所需的有用信息不完全相同。有些内容虽然链接少，但对模型可能很重要，尤其是处理难题的尾部知识。</p> <p>Ankit Gupta：这是原始Google的链接算法。</p> <p>Nick Joseph：对，它是质量指标，但不一定是最优指标。</p> <p>Ankit Gupta：AI的任务可能不同。</p> <p>Nick Joseph：尾部数据可能更有价值，可以通过蒸馏或智能模型生成数据训练新模型，接近原始模型的智能水平。</p> <p>Ankit Gupta：开放源代码模型中小模型蒸馏大模型的例子很多。</p> <p>Nick Joseph：完全可行。问题是，如果用现有模型生成数据训练更好模型，可能无法超越原模型，因为你只是学到了原模型的分布。如果分布有误，新模型也不会学到真实知识。</p> <p>Ankit Gupta：这是因为下一个token预测的损失对原模型生成内容很低，对吧？</p> <p>Nick Joseph：没错。模型只会学到原分布，如果原分布错误，就学不到真理。例如模型认为5+5=11，新模型也会学到11。这是一个研究难点，因为小规模研究难以直接扩展到大规模训练。还有一层问题是互联网上大量内容由LLM生成，其影响不易量化。1%的LLM内容，会浪费1%的算力，还是破坏5%-10%的模型表现？难以判断。</p> <p>Ankit Gupta：这不一定坏吧？如果训练目标是从当前分布向真实分布靠近，互联网上流通的数据本身可能有助于校正。</p> <p>Nick Joseph：你说的是通过互联网自然过滤，可能有效，但垃圾内容和恶意内容仍存在，这会影响模型。</p> <p>Ankit Gupta：你之前提到评估指标，除了模型本身，还有数据质量等指标。有没有可以量化的数据和模型质量指标，除了损失函数？</p> <p>Nick Joseph：损失函数其实很有效。理想的评估应关注实际关心的目标，而不是代理指标；评估需低噪声、快速易用。三点标准缺一不可，但第一点最难：明确你关心的到底是什么。</p> <p>Ankit Gupta：即使微小差异在评估中也要能体现出来，以便优化方向。</p> <p>Nick Joseph：对，比如GPT-4的MLA分数86.4%，下一代Gemini90%，差异明显，可用于判断优劣。评估需快速执行且易于操作。</p> <p>Ankit Gupta：例如AI医生任务，考试题可能轻松通过，但实际长时间与患者交流、提取关键信息更难，这类评估难度高。</p> <p>Nick Joseph：确实。创业公司可以做这些评估，而大实验室往往只优化标准化指标。医生场景中，我认为可用真实医生与患者的对话记录训练，预测对话文本，低噪声，模型可用于辅助诊疗。</p> <p>Ankit Gupta：这可以作为创业项目。外部讨论排列时，你能定义排列吗？它在预训练中如何体现？</p> <p>Nick Joseph：我们目标是AGI，即能完成大部分人类能做的事情。Sci-fi常被误导，人类一般想象一个机器人，而实际上应是大量智能体复制。关键问题是AI的目标是什么，目前下一token预测是目标，但人类目标不同。排列就是让模型目标与人类一致。</p> <p>Ankit Gupta：这与人类目标不同。</p> <p>Nick Joseph：是的。排列可从理论或经验角度解决，现有模型往往不符合期望。另一层是实际控制模型行为，比如通过宪法式规则或系统提示来约束模型交互方式。</p> <p>Ankit Gupta：系统提示类似提词，而非训练时调整。</p> <p>Nick Joseph：可以训练时加入，也可通过系统提示，取决于所需鲁棒性。</p> <p>Ankit Gupta：如何选择模型体现哪些价值？</p> <p>Nick Joseph：这是个难题。比喻为装方向盘，先获取控制权，再考虑驾驶者是谁。最好有民主化的价值设定，而非单人价值，以避免走向极权。</p> <p>Ankit Gupta：现阶段，你们如何实现排列？是通过后训练调整模型人格吗？</p> <p>Nick Joseph：大体正确。后训练迭代快，效果反馈快，适合调整模型。预训练用于基础科学探索，小模型无法有效模拟复杂行为。</p> <p>Ankit Gupta：必须在足够智能的模型上进行。</p> <p>Nick Joseph：对，但某些排列可以融入预训练，增强鲁棒性和智能性，例如让模型在学习智能的过程中融入人类反馈。</p> <p>Ankit Gupta：这如何在预训练中体现？</p> <p>Nick Joseph：可参考论文《Pretraining on Human Feedback》，将人类反馈特性融入预训练，观察效果。但缺点是灵活性下降，无法在后训练中快速调整。</p> <p>Ankit Gupta：你提到迭代速度关键，三个月与一天的差距巨大，可在短时间内尝试多种后训练策略并行执行。</p> <p>Nick Joseph：完全正确，自由训练本质困难，因为一次训练周期长且不可中断。</p> <hr/> <h2 id="五agi之路范式转移焦虑架构变革与未来展望">五、AGI之路：范式转移焦虑、架构变革与未来展望</h2> <h3 id="51-未来最大的挑战范式转变与隐藏的bug">5.1 未来最大的挑战：范式转变与隐藏的Bug</h3> <p><strong>Ankit Gupta</strong>：所以我现在在想，未来几年你们打算做的事情，作为团队，你们是如何考虑的呢？比如，你们会遇到哪些已知的问题，必须去应对？</p> <p><strong>Nick Joseph</strong>：我觉得我最关注的可能是<strong>范式的转变</strong>。我认为向强化学习的转变就是一个领域内的范式变化。我认为未来可能还会出现更多变化。很多人会争论，比如说，现有的范式是否足够让我们达到通用智能。我不知道，也许够，但我几乎可以肯定还会有新的范式。</p> <p>如果结果只是简单地扩大规模，没有任何意外，那会非常令人惊讶。但我真正感到最紧张的，其实是一些<strong>非常难以解决的bug</strong>。</p> <p>Ankit Gupta：这很有意思。</p> <p>Nick Joseph：是的，可能有点出乎我意料，但一个bug就可能让你搁置几个月。因为模型训练需要几个月的时间，一个小小的代码错误可能导致整代模型失效，而且难以被发现。机器学习本身就很难找到bug，但在大规模系统中，这类问题更难解决。</p> <p>Ankit Gupta：是啊，有时候甚至不知道从哪里下手。比如你写一个网络架构，漏掉了单元测试，或者没办法写测试去覆盖这种架构，你该怎么验证呢？</p> <p>Nick Joseph：你可以发送一个数据包，确认它正常传输，或者用小模型训练试一下，但即使是小模型，也不明显。</p> <p>Ankit Gupta：如果早期做ML的人遇到过经典bug，比如网络有10层，层7连到了层9，而不是8到9，模型依然可以训练，权重也在更新，但架构实际上是错的。这类bug非常隐蔽，很难发现。你说的这些随机bug，是这个意思吗？</p> <p>Nick Joseph：对，就是这种情况。随着系统复杂度上升，你可能在某个底层内核里用了错误的精度，这会让大规模模型崩掉。</p> <p>Ankit Gupta：训练一个月后才发现，甚至可能永远都发现不了。</p> <p>Nick Joseph：对，有时候甚至永远不会发现。代码量成万行，你怎么排查？所以最让我担心的就是这种微妙而棘手的bug。有时候你会看到模型崩溃或者训练速度突然下降，这类问题也非常难调试。我记得Nelson Nelha曾遇到过一个“诅咒”的bug，我遇到它时就觉得很棘手，于是把它交给别人，一个月后才松了一口气，自己可能永远解决不了。</p> <p>我觉得一个非常有价值的能力是可以深入到任何层面去分析问题，但这是一项罕见技能。我在Torch底层工作过，如果不了解CUDA，Torch底层出问题，我就无法自己解决。通讯方面也是一样，我可以发送数据，但如果底层网络协议有问题，我必须学整个领域才能理解数据包和TCP等细节。能从ML学习动态到字节级别全栈掌握的人非常少。</p> <p>Ankit Gupta：完全理解。那你们团队成员的背景是怎样分布的？外界可能认为你们全是写论文的博士研究员，但我觉得事实可能并非如此。</p> <p>Nick Joseph：是的，团队混合多种背景。我们最需要的几乎一直都是工程师。在这个领域的历史中，往往只要算力足够，模型就能工作，真正的挑战是如何正确实现。</p> <p>Ankit Gupta：行动力是关键，对吧？</p> <p>Nick Joseph：没错。实现正确并非ML问题，架构其实很简单，你甚至不必完全理解数学，只要实现正确即可。然后你面临的工程问题是如何在大规模上并行化、验证正确性。这种工程技能是关键，但与快速迭代网站等技能不同，更偏向于解决极难工程问题。</p> <p>Ankit Gupta：你们找的工程师是有类似Anthropic经验的人，还是学术出身的人？</p> <p>Nick Joseph：目前我们倾向于直接聘请有相关经验的人。早期我们会从各种背景挑选，但现在领域足够大，有经验的人可以直接上手。也有一些非常聪明、勤奋的人可以快速学习，比如一些理论物理学家，他们通过实习掌握编程，也能做出出色工作。</p> <p>Ankit Gupta：我想换个话题，谈谈未来你对AI领域其他方向的看法。你怎么看非下一步预测的方向？比如不是用Transformer架构，或者非自回归训练，有没有值得关注的？</p> <p>Nick Joseph：我觉得这些很有趣，但我不认为自回归是唯一路径。不过，自回归可能足够达到EGI。主要驱动还是规模和对基础科学的精细研究，而不是完全新颖的架构。确实存在更优新颖方法，但规模更容易、可靠性更高，而且仍有很大提升空间。</p> <p>Ankit Gupta：你花很多时间研究开源论文，看到一些中国实验室对架构做优化，比如缓存或高效注意力机制。你觉得这些优化是“加算力就行”的小改进，还是可能像Transformer一样带来革命性变化，必要时才能实现AGI？</p> <p>Nick Joseph：我认为是两者结合。我的猜测是，你会不断调整，随着算力增加，实验价值也会提升。同时还有推理优化，比如服务大量用户时做出更低成本、高效率的推理，这涉及推理堆栈和芯片细节。</p> <p>Ankit Gupta：预训练团队是否需要考虑推理问题，还是只管降低loss，然后交给别人处理？</p> <p>Nick Joseph：不，我们非常关注推理问题。我们提供的模型必须快速运行，否则用户体验会受影响。</p> <p>Ankit Gupta：能举例说明吗？</p> <p>Nick Joseph：最简单的例子是把模型做得过大，推理会变慢，或者增加不必要的通信步骤，使推理复杂化。这并非技术难题，而是架构设计问题。</p> <p>Ankit Gupta：明白。</p> <p>Nick Joseph：推理团队是我最常合作的团队，因为我们共同设计模型，使其既聪明又经济高效。有限算力下，优化推理是服务更多用户的关键。</p> <p>Ankit Gupta：如果算力无限呢？</p> <p>Nick Joseph：即便算力无限，挑战也在于如何利用它。大规模系统会遇到芯片故障等工程问题。无限算力能大幅提升速度和实验频率，但仍需解决复杂工程问题。现在的AI研究非常受制于算力，算力限制了模型训练和迭代速度。</p> <p>Ankit Gupta：你怎么看Diffusion方法，比如Gemini diffusion模型？在蛋白质设计等领域应用广泛，你觉得有发展潜力吗？</p> <p>Nick Joseph：坦白说，我们没做过图像生成，主要是扩散模型用途。我自己理解不够深入，但团队中有人可以更好回答。我觉得这类方法属于“不是范式性转变，但可能带来运行效率提升”的类别。</p> <p>Ankit Gupta：那么在近期，如果Anthropic每年持续改进模型，你认为创业公司有哪些机会？</p> <p>Nick Joseph：任何能受益于模型智能提升的方向都有机会。虽然大公司算力和资源更强，但他们做的是通用系统，我们的目标是让创业公司用这些模型完成具体工作。关键是利用当前模型基础上稍加努力就能落地的应用，但要注意不要过度依赖搭建复杂脚手架，因为下一代模型可能不需要。</p> <p>Ankit Gupta：在训练堆栈中，有没有问题是如果有公司解决，我们会直接购买产品的？</p> <p>Nick Joseph：有很多。比如芯片计算错误，如果有初创公司能检测每块芯片的精确性并报告故障，非常有用。还有组织管理方面，如果有服务能帮快速扩展团队，管理人员招聘和组织问题，也很有价值。未来更值得关注的是AGI普及后的社会影响，如何让其积极造福世界，而不仅仅是经济增长。</p> <p>Ankit Gupta：非常认同。最后一个问题，如果回到10年前，你还是学生，正从经济学转向AI，你会给学生什么建议，尤其是想进入你现在这种岗位的学生？</p> <p>Nick Joseph：时间点不同，策略也不同。如果回到10年前，我会专注AI，特别是工程技能，而不仅仅是数学或理论ML文献。同时会关注AGI相关应用，这是我认为最重要的两件事。</p> <hr/> <h2 id="结语">结语</h2> <p>Nick Joseph的访谈为我们提供了一个难得的窗口，深入了解Anthropic预训练团队的核心思考与实践。从AI安全初心到技术路线选择，从万卡集群的工程挑战到对齐问题的深层思考，这次对话展现了一个预训练团队负责人的完整视角。</p> <p><strong>核心要点总结：</strong></p> <ol> <li><strong>算力是王道</strong>：预训练的核心就是推动损失函数下降，扩展定律已被证明在11个数量级上有效</li> <li><strong>工程能力关键</strong>：正确实现比理论创新更重要，全栈能力是稀缺资源</li> <li><strong>预训练与后训练的平衡</strong>：如何平衡两者仍在早期探索阶段，强化学习带来了新的范式转变</li> <li><strong>推理与训练并重</strong>：预训练团队必须考虑推理问题，模型要既聪明又经济高效</li> <li><strong>未来挑战</strong>：范式转变和隐藏的Bug是最大威胁，AGI的社会影响值得关注</li> </ol> <p>正如Nick所说，现在AI研究最大的瓶颈是计算资源受限，而非算法突破。在这个领域，工程实践的价值远远超过纸面上的理论创新。对于想要进入这个领域的年轻人来说，培养扎实的工程技能，保持对AGI应用的关注，将是最重要的两件事。</p> <hr/> <p><strong>原视频：</strong> <a href="https://www.youtube.com/watch?v=YFeb3yAxtjE">Anthropic Head of Pretraining on Scaling Laws, Compute, and the Future of AI</a></p> <table> <tbody> <tr> <td><strong>编译：</strong> Zhenning Du</td> <td><strong>来源：</strong> Z Potentials</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="ai-insights"/><category term="AI"/><category term="LLM"/><category term="pretraining"/><category term="anthropic"/><category term="scaling-laws"/><summary type="html"><![CDATA[Anthropic预训练团队负责人Nick Joseph详谈预训练策略、缩放定律、数据挑战与AI对齐的核心议题]]></summary></entry><entry><title type="html">人工智能时代的教育</title><link href="https://emigmo.github.io/blog/2025/xuyangsheng-CUHK-SZ/" rel="alternate" type="text/html" title="人工智能时代的教育"/><published>2025-10-05T00:00:00+00:00</published><updated>2025-10-05T00:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/xuyangsheng-CUHK-SZ</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/xuyangsheng-CUHK-SZ/"><![CDATA[<h1 id="人工智能时代的教育">人工智能时代的教育</h1> <p><strong>演讲人：徐扬生</strong><br/> <em>中国工程院院士、美国国家工程院外籍院士、香港中文大学（深圳）校长</em></p> <hr/> <h2 id="目录">目录</h2> <ul> <li><a href="#引言教育的三个核心问题">引言：教育的三个核心问题</a></li> <li><a href="#一理解未来ai时代的本质特征">一、理解未来：AI时代的本质特征</a> <ul> <li><a href="#11-教育将受到最大冲击">1.1 教育将受到最大冲击</a></li> <li><a href="#12-人工智能的本质认知感知行动">1.2 人工智能的本质：认知、感知、行动</a></li> <li><a href="#13-人工智能与人类智能的七大差异">1.3 人工智能与人类智能的七大差异</a></li> <li><a href="#14-教育的真正重点比感情比创造比个性">1.4 教育的真正重点：比感情、比创造、比个性</a></li> </ul> </li> <li><a href="#二重新定义人才从会考试到真优秀">二、重新定义人才：从”会考试”到”真优秀”</a> <ul> <li><a href="#21-ai时代对人才的四项要求">2.1 AI时代对人才的四项要求</a></li> <li><a href="#22-当前教育的误区培养会考试的人">2.2 当前教育的误区：培养”会考试”的人</a></li> <li><a href="#23-真正优秀人才的四个维度">2.3 真正优秀人才的四个维度</a></li> </ul> </li> <li><a href="#三培养人才学思践悟">三、培养人才：学、思、践、悟</a> <ul> <li><a href="#31-学掌握学习的方法">3.1 学：掌握学习的方法</a></li> <li><a href="#32-思闲暇是思考的土壤">3.2 思：闲暇是思考的土壤</a></li> <li><a href="#33-践实践比学校更重要">3.3 践：实践比学校更重要</a></li> <li><a href="#34-悟事教人是教训">3.4 悟：事教人是教训</a></li> </ul> </li> <li><a href="#四教育改革的七个核心方向">四、教育改革的七个核心方向</a> <ul> <li><a href="#41-少教一点多育一点">4.1 少”教”一点，多”育”一点</a></li> <li><a href="#42-重理性轻记忆">4.2 重”理性”轻”记忆”</a></li> <li><a href="#43-增加实践环节践能生悟">4.3 增加实践环节：”践”能生”悟”</a></li> <li><a href="#44-培养创新人才学会提问">4.4 培养创新人才：学会提问</a></li> <li><a href="#45-加强艺术教育爱与美是生命支点">4.5 加强艺术教育：爱与美是生命支点</a></li> <li><a href="#46-文理融合跨学科发展">4.6 文理融合：跨学科发展</a></li> <li><a href="#47-观世界才能有世界观">4.7 观世界，才能有世界观</a></li> </ul> </li> <li><a href="#结语带着灵魂往前走">结语：带着灵魂往前走</a></li> </ul> <hr/> <h2 id="引言教育的三个核心问题">引言：教育的三个核心问题</h2> <p>今天现场有来自全国各地的中学校长，还有网上那么多关心教育事业的家长、同学、老师跟校长，我想把我这几十年来的一些思考跟大家分享。</p> <p>我做过中学老师，做过大学老师，做过大学校长，也教过大概三十几个国家的学生，所以有点感悟。同时我也在人工智能领域做了整整40年的研究工作，所以有一点点思考，跟大家分享。</p> <p><strong>教育是什么？</strong> 我把它定义为：<strong>教育是”为未来社会培养人才”</strong>。</p> <p>这里有两个关键词：</p> <ul> <li><strong>未来</strong>：未来到底是怎么样的？</li> <li><strong>人才</strong>：什么样的人算人才？</li> </ul> <p>由此引出第三个问题：<strong>怎么培养未来的人才？</strong></p> <p>今天就围绕这三个核心问题展开讨论。</p> <hr/> <h2 id="一理解未来ai时代的本质特征">一、理解未来：AI时代的本质特征</h2> <h3 id="11-教育将受到最大冲击">1.1 教育将受到最大冲击</h3> <p>教育，就是教书、育人。但我们需要重新审视这个定义。</p> <p><strong>学校还是教书最好的地方吗？</strong> 上个星期，我的一位美国朋友在硅谷跟我讨论这件事情。他说，教书，学校不是最好的地方。我非常吃惊，我说，哪里是最好的地方呢？他说你去看看网上，全是最好的课程，而且还都是免费的，为什么要把孩子送到学校来呢？</p> <p><strong>学校还是育人最好的地方吗？</strong> 在美国西部，有些家庭，大概十来个小孩，他们组成一个团体，做一个像小作坊这样的东西，来培养自己的孩子。</p> <blockquote> <p><strong>如果一个学校不知道教什么书，育什么样的人，那要我们学校干什么？</strong></p> </blockquote> <p>人工智能时代来了，<strong>最直接的冲击在哪里？教育。</strong></p> <p>为什么？因为人工智能可能是社会乃至人类历史上最重要的进展，整个社会要重构，社会阶层要重新划分，职业也会重新划分，会深深影响人类思想文明的走向，而教育就是为人类社会培养各阶层各领域的人才。</p> <h3 id="12-人工智能的本质认知感知行动">1.2 人工智能的本质：认知、感知、行动</h3> <p>人工智能是什么？实际上，<strong>人工智能就是仿照人的智能去做一些事情</strong>。</p> <p><strong>1. 认知</strong> — 怎么认识这个世界，怎么做判断。比如这是白衣服，白衣服大概是衬衣，衬衣大概是夏天穿的。这些都是判断、推理、决策。</p> <p><strong>2. 感知</strong> — 生活环境反馈给我们的信息。比如视觉、听觉，我们看到这个人穿白衣服，不是黑衣服。</p> <p><strong>3. 行动</strong> — 对感受到的世界有认知，有意愿去参与，就要付诸行动，比如走路、挥手。</p> <p>一个东西能够做到有认知的功能、感知的功能、行动的功能，合起来，我们叫作人工智能。</p> <p><strong>AI发展的现状：</strong></p> <ul> <li><strong>认知部分</strong>：尤其是大语言模型出来以后，做得很好，大家感觉真的是智能</li> <li><strong>感知部分</strong>：稍微差一点。比如打开一瓶茅台酒让它闻闻，这是不是酱香型的？它不一定懂</li> <li><strong>感情部分</strong>：更加不行</li> <li><strong>行动部分</strong>：进步很慢。我40年前开始做这件事情，到今天，进度非常之慢，目前状态跟二十几年前差不多</li> </ul> <p>现在都在讲具身智能，这个名字很好，也是我们提出来的。但很多人有点夸张。其实，<strong>现在的人工智能就是一个能说会道的残疾人</strong>。与人类共建社会是在将来，距离现在有多远，很难说。</p> <h3 id="13-人工智能与人类智能的七大差异">1.3 人工智能与人类智能的七大差异</h3> <p>很多人觉得，人工智能都比人聪明了，那人怎么办？朋友们，<strong>人工智能跟人的智能不是一回事</strong>。</p> <table> <thead> <tr> <th>维度</th> <th>人工智能</th> <th>人类智能</th> </tr> </thead> <tbody> <tr> <td><strong>知识与智慧</strong></td> <td>知道得越多越聪明</td> <td>知道得多不一定更聪明</td> </tr> <tr> <td><strong>时间取向</strong></td> <td>向后看（总结、整理）</td> <td>向前看（创造、预见）</td> </tr> <tr> <td><strong>智能分布</strong></td> <td>集中型智能</td> <td>分布性智能（全身都是智能体）</td> </tr> <tr> <td><strong>思维模式</strong></td> <td>重理性</td> <td>感性与理性结合</td> </tr> <tr> <td><strong>认知方式</strong></td> <td>重视分析与整合</td> <td>重视直觉与体验</td> </tr> <tr> <td><strong>心脑关系</strong></td> <td>重视脑</td> <td>心脑并用</td> </tr> <tr> <td><strong>个性通性</strong></td> <td>追求通性</td> <td>追求个性</td> </tr> </tbody> </table> <p><strong>一个重要的文化观察：</strong> 在中国文化里，”脑”这个字是月字旁，是身体的一部分，凡是身体一部分的字，都是月字旁，比如肝、脾、胃、胆。唯一一个例外是”心”。因为古人始终认为心不仅是人身体的一部分，”心”字有两点，一点在中心，是身体内的，另一点在旁，不在身体内。对人来说，心脑是并用的。</p> <p><strong>AI时代的危险：机器越来越像人，人越来越像机器</strong></p> <p>前一阵子，我让我的司机开车带我到广州一个地方见朋友。晚上11点，他一直开到一个大工地里面，下面全是水泥地，漆黑一片。我说这是什么地方？他说这是您叫我来的地方，GPS就导航到这里来了。</p> <blockquote> <p><strong>现在的人开车不去思考了，因为有GPS。人放弃思考是非常危险的，而思考是人区别于动物的唯一的东西。</strong></p> </blockquote> <p>人工智能时代的特征：</p> <ul> <li>获取知识的途径改变了</li> <li>整个社会将会趋于平庸</li> <li>10年以后，讲的话大家都差不多，个性没有了，观点没有了</li> </ul> <h3 id="14-教育的真正重点比感情比创造比个性">1.4 教育的真正重点：比感情、比创造、比个性</h3> <p>你不能跟AI比聪明，就像我们发明了汽车，我们不可以跟汽车比谁跑得快；我们也不能跟它比记忆，它记得比你好；我们跟它比知识，也比不过它。</p> <p><strong>那我们可以跟它比什么？</strong></p> <blockquote> <p><strong>比感情、比创造、比个性，这才是我们教育的真正重点。</strong></p> </blockquote> <h2 id="二重新定义人才从会考试到真优秀">二、重新定义人才：从”会考试”到”真优秀”</h2> <h3 id="21-ai时代对人才的四项要求">2.1 AI时代对人才的四项要求</h3> <p>人工智能时代对人才的要求：</p> <ol> <li><strong>领导力</strong> — 语言、沟通、判断、同理心</li> <li><strong>理性</strong> — 提问、分析、逻辑、批判性思维</li> <li><strong>创造力</strong> — 想象、艺术、探索能力</li> <li><strong>品性</strong> — 勇气、顽强、世界观、人文素养</li> </ol> <h3 id="22-当前教育的误区培养会考试的人">2.2 当前教育的误区：培养”会考试”的人</h3> <p>我分析了这几年的高考试卷，我们对人才的要求大概体现在高考卷子上面：</p> <table> <thead> <tr> <th>能力维度</th> <th>当前高考占比</th> <th>AI时代需求</th> </tr> </thead> <tbody> <tr> <td>记忆</td> <td>70%</td> <td>↓</td> </tr> <tr> <td>理性</td> <td>20%</td> <td>↑</td> </tr> <tr> <td>创造力</td> <td>5%</td> <td>↑↑</td> </tr> <tr> <td>品性</td> <td>5%</td> <td>↑↑</td> </tr> </tbody> </table> <p>不仅是文科的卷子，理科的卷子也是这样。化学、生物甚至物理卷子都基本上是这样，记忆占百分之六七十。</p> <blockquote> <p><strong>记忆好的人，高考成绩就好，而高考成绩好的，我们就叫他优秀人才——所以我们目前在培养的是会考试的人才。</strong></p> </blockquote> <h3 id="23-真正优秀人才的四个维度">2.3 真正优秀人才的四个维度</h3> <p>什么样的人才是真正优秀的？我总结了4条：</p> <ol> <li><strong>勤奋</strong> — 勤奋你才有动力</li> <li><strong>理性</strong> — 有主见，能思辨，能逻辑分析</li> <li><strong>创造性</strong> — 能创新，能突破</li> <li><strong>顽强</strong> — 能坚持，不放弃</li> </ol> <p><strong>中国学生的现状（我40年教育经验的观察）：</strong></p> <table> <thead> <tr> <th>能力</th> <th>中国学生表现</th> <th>对成功的重要性</th> </tr> </thead> <tbody> <tr> <td>勤奋</td> <td>★★★★☆</td> <td>15%</td> </tr> <tr> <td>理性</td> <td>★★★☆☆</td> <td>15%</td> </tr> <tr> <td>创造性</td> <td>★★☆☆☆</td> <td>30%</td> </tr> <tr> <td>顽强</td> <td>★☆☆☆☆</td> <td><strong>40%</strong></td> </tr> </tbody> </table> <p>很多人问我，你认识那么多聪明的人，优秀的人才有什么共同特点？<strong>一个非常明显的就是，他们无论在什么样的情况下都能够坚持下来。</strong></p> <p>我们中国内地的学生，勤奋是很好的，但顽强这点是最差的。我们看美国以及其他国家，比如印度、东南亚、欧洲跟南美一些国家，根据我的观察，他们的理性、勤奋可能不如我们，但他们能坚持，顽强性比较好。他们不怕批评，明天照样继续坚持做。</p> <h2 id="三培养人才学思践悟">三、培养人才：学、思、践、悟</h2> <p>我们到底该怎么来培养人才呢？我认为大概是这4个过程：<strong>学、思、践、悟</strong>。</p> <h3 id="31-学掌握学习的方法">3.1 学：掌握学习的方法</h3> <p>学是不容易的，要有好奇心，有兴趣。没有兴趣，学到最后还给老师了。我们学的东西是前人的东西。学习要专注，要学会学习方法。</p> <blockquote> <p><strong>Learn how to learn，这是最重要的。</strong></p> </blockquote> <p>因为大多数知识到最后，不是老师教的，是你自己学的。我的经验是，<strong>90%以上的知识都不是老师教授的，是你自己学的</strong>。所以要培养自己学习的能力，在学校要系统地教学生这样的能力。</p> <h3 id="32-思闲暇是思考的土壤">3.2 思：闲暇是思考的土壤</h3> <p>学完了以后你要能够去思考。有一位领导问我，怎么让我们的孩子们真正能够思考？我说，给他们点空余时间。</p> <p>他们晚上睡觉之前是否有一个小时是自由的时间？你去想一想，如果没有闲暇的时间，人怎么会思考呢？</p> <blockquote> <p><strong>闲暇的时间是思考的土壤，没有土壤你怎么能成长呢？</strong></p> </blockquote> <h3 id="33-践实践比学校更重要">3.3 践：实践比学校更重要</h3> <p>今天我会着重讲这个事情。<strong>人工智能教育本质上就是人的教育，实践的教育，创新的教育，如果不重视实践的话，根本谈不上人工智能教育。</strong></p> <p>人工智能时代，体验是重要的。优秀教育的效果主要看体验。实践本身就是一所学校，甚至实践比学校更重要。</p> <p>中国的同学欠缺就欠缺在实践这里，没有实践，那就把知识全部还给了老师：</p> <ul> <li>学，不是你的东西</li> <li>思考，无非你想过而已</li> <li>没有实践，光靠课堂教学，学到的就会全部还给老师，你悟不出来</li> </ul> <blockquote> <p><strong>因为只有践，才能生悟。</strong></p> </blockquote> <h3 id="34-悟事教人是教训">3.4 悟：事教人是教训</h3> <blockquote> <p><strong>人教人是知识，事教人是教训。</strong></p> </blockquote> <p>只有通过实践，你才能真正领悟在你一生中应该记住的东西。</p> <h2 id="四教育改革的七个核心方向">四、教育改革的七个核心方向</h2> <p>人工智能时代的教育改革，应该往以下几个核心方向走。</p> <h3 id="41-少教一点多育一点">4.1 少”教”一点，多”育”一点</h3> <p>我们要把重心放在育人上，而不在教书上。现在，我们全社会的人，包括你们，包括我，都应该问一下我们自己：<strong>我们是知道的太多了，还是知道的太少了？</strong></p> <p>每天晚上睡觉的时候我在想：我今天看了那么多东西，有多少是我应该看到的，有多少其实我不应该看到？我大概算了算，有60%是我不应该看到的，40%是我应该看到的。大多数人估计还达不到这个程度，估计80%～20%之间。</p> <blockquote> <p><strong>信息与知识把我们每一个人的脑袋占得太满了。</strong></p> </blockquote> <p>我的一个学生问我，你认识那么多有智慧的人，你告诉我这些有智慧的人跟我们一般的人有什么差别？我想来想去，有一个差别：<strong>他们知道得很少</strong>。</p> <ul> <li>因为知道很少，他们就能专注</li> <li>因为专注，他们就能出成果</li> </ul> <p>所以我们教的东西不用教得太多，你把核心的东西教给人家就可以了。</p> <p><strong>为什么读书越多越不容易创新？</strong></p> <p>这个问题我思考了13年，我把它想清楚了。因为人的脑袋是有容量的，你装东西都装满了以后，脑袋里面会有一个固定的”程式”，这个程式决定了，每当出现新的问题，他会习惯用已有的程式来处理这些新问题。换言之，<strong>他会把所有新的问题都当作这个程式能够处理的旧问题</strong>。</p> <p>举例：</p> <ul> <li>你问一个学生问题，他首先想到的是上网去找一找——他在假定人家已经问过这个问题了</li> <li>我带一个博士生，我跟他说某个课题应该怎么做。他第一时间就问我，教授，你告诉我，有没有参考文献？</li> </ul> <p>参考文献是什么？那是与这个课题有关的、人家已经做过的工作，当然应该知道点。但假如这个工作是你第一个开始做的，你就没有参考文献。</p> <p>朋友们，这不是小事情。<strong>任何一个人，如果你去问他一个事情，他都是假定人家已经碰到过这件事情，他就没有意愿去创新了。</strong></p> <p><strong>读书越多的三个负面效应：</strong></p> <ol> <li><strong>胆子越小</strong> — 了解太多案例，越小心谨慎，越害怕失败</li> <li><strong>大趋势越不清楚，小事情越清楚</strong> — 被细节困住</li> <li><strong>越来越成为小心翼翼的观察者，而不是勇敢的实践者</strong></li> </ol> <h3 id="42-重理性轻记忆">4.2 重”理性”轻”记忆”</h3> <p>理性其实比记忆重要得多。但是我们目前的教育是倾向于记忆，我们要把重心转移到理性的思辨能力的培养上来，要培养学生的科学思维、科学精神。</p> <p>我走了100所中学，我很清楚地告诉大家，有些中学做得蛮好的，在全世界的中学里都是好的，有些则不是。</p> <p><strong>中学需要做的：</strong></p> <ul> <li>加强理性分析的课程内容和考试要求</li> <li>加强对学生综合分析能力的培养</li> <li>数理的教学要<strong>提高深度，而不是广度</strong></li> <li>加强跨学科的课程教学</li> <li>加强学生理性分析辩论的兴趣小组</li> </ul> <h3 id="43-增加实践环节践能生悟">4.3 增加实践环节：”践”能生”悟”</h3> <p><strong>实践比学校重要。</strong> 人工智能教育，如果不重视实践的话，你最后要损失很多东西。我们要培养具有伟大格局的实践者。</p> <p>朋友们，你去看看，我们每天在看的网上的所有信息，都是在观察人家，看看A在说什么，B在做什么，C在做什么，哪个好哪个不好，在那里评论。</p> <blockquote> <p><strong>评论家很多，观察家很多，实践家很少。</strong></p> </blockquote> <p>体验是AI时代首要的教育重点。人教人是知识，事教人是教训。没有教训就没有实践经验，孩子长不大，所以我们的孩子普遍晚熟。</p> <p><strong>一个有趣的对比：</strong></p> <p>我们这个学校是国际大学，有很多国家的学生，各个国家学生跟我们国内的学生混在一起，你去看，他们总是比我们的孩子要成熟一点。我们的孩子，有什么事情都要跟爸爸妈妈商量一下，他们从来不是这样，可以自己做决定。</p> <blockquote> <p><strong>我们的孩子普遍晚熟，我们的家长普遍早熟。</strong></p> </blockquote> <p>小孩子还没长大，我们的家长就什么事情都给他想好了——”我这个孩子数学不行，以后不知道能不能到银行里面去工作？”他还是三年级的小学生啊，家长就这么想。</p> <p><strong>家长的早熟决定了孩子的晚熟</strong>，因为什么都给他想好了，孩子没有机会能自己做一些事情。</p> <p><strong>一个值得赞赏的案例：</strong> 我们有一个同事，他的孩子考上了全世界最好的大学之一。在上大学之前，他让孩子去非洲支教，很多人想不通。他跟我说，我举双手赞成，你这个孩子以后一定有出息。</p> <h3 id="44-培养创新人才学会提问">4.4 培养创新人才：学会提问</h3> <p>创新并不是你想创新就可以创新的。<strong>创新是一种文化</strong>，我们的社会，这种文化有没有呢？教育是这种文化的体现。</p> <p><strong>中国文化当中创新为什么比较困难？三个根源：</strong></p> <p><strong>1. 过于重实用</strong></p> <p>什么东西一出来，首先想有没有用。做个机器人出来了，人家说机器人干什么用？我做一个爬树机器人，他们就问爬树为什么要用机器人？</p> <p>实用当然是重要的，但你光想着实用价值，你就会停留在那个实用上面，就不会去深入了解后面的东西了。我们看到了烟花和炸药，不会想到它背后的化学，不会深入去研究，创新就被阻碍了。</p> <p><strong>2. 对传承的纠结</strong></p> <p>中国人一讲到创新就想到传承。我的感悟，世界上那么多创新，不是所有东西都是要通过传承的，有的是直接可以创新的。</p> <p><strong>3. 认识论上的不严格性</strong></p> <p>在我们中国文化当中，认识论是不严格的，而且非常严重。在西方的哲学里，包括苏格拉底，提出一个事情后会问你：为什么可以这样说呢？中国人是不讲的，不过庄子是个例外。这使人不会深究，创新就有困难了。</p> <blockquote> <p><strong>人工智能时代最大的挑战就是培养创新人才。在座的校长们，你们是中国的希望，中国的希望在10岁到30岁的人当中，而你们肩负着培养这一代人的任务。</strong></p> </blockquote> <p><strong>创新的三个条件：</strong></p> <ol> <li> <p><strong>不满</strong> — 鲁迅先生讲”不满是向上的车轮”，一个人对所有东西都满意的话，他还创什么新？创新就是打破格局。不满就是要提出问题来。</p> </li> <li> <p><strong>想象</strong> — 如果没有想象能力是无法创新的。想象是怎么来的？是跟艺术有关，跟跨学科有关。</p> </li> <li> <p><strong>自由</strong> — 要有面对现实，完全自由的、充分想象的能力。</p> </li> </ol> <blockquote> <p><strong>人工智能时代的教育要教我们的孩子学会提问，提问本身就是创新的一个元素。</strong></p> </blockquote> <h3 id="45-加强艺术教育爱与美是生命支点">4.5 加强艺术教育：爱与美是生命支点</h3> <p>我在不同的场合都讲过，<strong>艺术是了不起的</strong>。</p> <p>我走了100所中学，大概有25所中学是符合艺术教育方面的要求的，欧洲的学校、美国的学校做得比我们好。艺术教育的严重缺乏，将影响我们下一代的整体素质。</p> <blockquote> <p><strong>因为缺少爱是一个生命的缺陷，一个孩子，永远有一个生命的缺陷在那里，这是多么遗憾的一件事情。</strong></p> </blockquote> <p><strong>世界上有两样东西使我们的生活值得苟且，那就是爱与美。</strong></p> <p>这是生命的支点，生命退到最后退不过去的那点，而这两点，都跟艺术有关。如果不懂艺术的话，生命的支点就没有了，情感世界塌陷了。</p> <p>而在理性世界当中，<strong>艺术是创造力的源泉</strong>，所以艺术是很重要的东西。未来社会可能会有一半的生活跟艺术有关。</p> <h3 id="46-文理融合跨学科发展">4.6 文理融合：跨学科发展</h3> <p>这句话好像是跟大学讲的，但其实我是对中学讲的。<strong>文科跟理科是一个世界的两面，不是两个世界。</strong></p> <p>现在教孩子们是两个世界，所以造成了：</p> <ul> <li>理科生严重缺乏人文素养</li> <li>文科生的就业产生了很大的困难</li> </ul> <p>所以：</p> <ul> <li>要让我们的理科生能够欣赏文科</li> <li>要让文科生了解理科</li> <li>AI是不分文理、跨学科的</li> </ul> <blockquote> <p><strong>学校的目的是提供完整的教育，启发完整的人格。从这个意义上讲，文理是不应该分科的。</strong></p> </blockquote> <h3 id="47-观世界才能有世界观">4.7 观世界，才能有世界观</h3> <p>我跟你们讲一个小小的故事。以前我在香港教书的时候，有一群香港学生来跟我讲世界观。</p> <ul> <li>我问一个同学：你是哪里人？——我是番禺人。</li> <li>番禺是哪个省的？——不知道。</li> <li>旁边一个女同学说：我妈妈说我是中山人。</li> <li>你知道为什么那个地方叫中山吗？——不知道。</li> <li>去过中国内地吗？——回乡证里注着15年前去过。</li> </ul> <p>15年当中，中国内地发生了巨大的变化。我说那世界上你还去过什么地方呢？”我其实没去过世界什么地方，我就在沙田这一带。”</p> <blockquote> <p><strong>我说你下次跟我讲的时候，你就跟我讲”沙田观”，你不要讲”世界观”了。</strong></p> </blockquote> <p><strong>一个人的世界观很重要，但世界观怎么来？世界观是从观世界中来。</strong></p> <p>要让学生们：</p> <ul> <li>长见识</li> <li>有全球眼光</li> <li>有同理心</li> <li>有审视世界的能力</li> <li>用世界的眼光看中国</li> <li>更要用中国的精神来引导世界</li> </ul> <hr/> <h2 id="结语带着灵魂往前走">结语：带着灵魂往前走</h2> <p>总结一下我刚才讲的几个观点：</p> <blockquote> <p><strong>人工智能时代的教育，是人的教育，是实践的教育，是创新的教育。</strong></p> </blockquote> <p>最后我用两句话，来结束我的演讲。</p> <h3 id="第一句话">第一句话</h3> <blockquote> <p><strong>人类因为创造了人工智能而伟大，因为知道人工智能的局限而成熟。</strong></p> </blockquote> <p>创造了人工智能，可能是人类历史上最伟大的贡献。所有人，连我也不知道，后面可能会发生什么。以前我们所知道的是人是在进化的，生物是进化的，现在说地球也是在进化的，整个宇宙都是在进化的。</p> <p>人类是不是已经创造了人工智能？现在只是在中途，刚刚开始；人类是不是知道人工智能的局限，如何来面对这些局限性，都在考验人类的智慧和成熟。</p> <h3 id="第二句话">第二句话</h3> <blockquote> <p><strong>对世界文明的真正贡献不在于人口多少，不在于高楼大厦，不在于科技发展，是在这个国家和这个地区造就了什么样的人。</strong></p> </blockquote> <p>所以在座的各位任重道远，世界走得很快，<strong>要教育我们的孩子，带着灵魂往前走。</strong></p> <p>谢谢各位。</p> <hr/> <p><strong>注释：</strong> 本文根据中国工程院院士、美国国家工程院外籍院士、香港中文大学（深圳）校长徐扬生在普通高中校长年会（2025）上的主旨报告《人工智能时代的教育》整理而成。</p> <p>关注我，点击最上端的蓝字”徐扬生”或长按识别二维码关注</p> <p>Image</p>]]></content><author><name></name></author><category term="教育思考"/><category term="人工智能"/><category term="教育"/><category term="创新"/><summary type="html"><![CDATA[徐扬生院士在普通高中校长年会上的主旨报告，探讨AI时代教育改革的核心方向]]></summary></entry><entry><title type="html">姚顺雨AI与Agent研究观点集</title><link href="https://emigmo.github.io/blog/2025/yao-shunyu-ai-agent-insights/" rel="alternate" type="text/html" title="姚顺雨AI与Agent研究观点集"/><published>2025-09-17T10:00:00+00:00</published><updated>2025-09-17T10:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/yao-shunyu-ai-agent-insights</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/yao-shunyu-ai-agent-insights/"><![CDATA[<h2 id="前言">前言</h2> <p>本文整理了OpenAI研究员姚顺雨关于AI与Agent系统的深度思考。作为从清华姚班到普林斯顿PhD，再到加入OpenAI的研究者，姚顺雨在AI发展的关键节点上提出了许多具有前瞻性的观点。他的”下半场”理论、记忆层级理论等思想为我们理解AI发展趋势提供了重要视角。</p> <hr/> <h2 id="个人背景与研究历程">个人背景与研究历程</h2> <h3 id="学术经历">学术经历</h3> <p><strong>标准化的成长路径：</strong></p> <blockquote> <p>“我感觉我是个非常乖的学生。从小到大就是按部就班的学习。本科从合肥考到清华，读姚班。在姚班大家会告诉你去美国读PhD，我就去美国读PhD，我在普林斯顿读PhD。读PhD之后很自然，OpenAI是做research最好的地方，就加入OpenAI——感觉我前28年的人生，非常的乖。”</p> </blockquote> <p><strong>时间线：</strong></p> <ul> <li>2015-2019年：清华姚班</li> <li>2019-2024年：普林斯顿大学PhD</li> <li>2024年：加入OpenAI</li> </ul> <h3 id="研究转向的关键时刻">研究转向的关键时刻</h3> <p><strong>从理论到应用的觉醒：</strong></p> <blockquote> <p>“当时，我觉得很多重要理论问题已经解决得差不多，比如将某个图算法的复杂度从n的2.83次方优化到n的2.82次方，这种改进在现实中意义不大。”</p> </blockquote> <p><strong>深度学习的启蒙：</strong></p> <blockquote> <p>“我在2016年上李建老师的一门课，看到一个multi-modal embedding的demo，展示了embedding一个非常神奇的例子：比如用’king’的embedding减去’man’，再加上’woman’，结果接近’queen’的embedding——这让我第一次意识到，深度学习在语义表示上居然能做到这么惊艳的计算。”</p> </blockquote> <p><strong>从视觉到语言：</strong></p> <blockquote> <p>“最初我做的是Computer Vision，但渐渐意识到Vision很难实现通用人工智能。我的直觉告诉我：Language是一个更核心、更有潜力的方向，于是读博后转向语言模型研究。”</p> </blockquote> <h3 id="非共识的选择">非共识的选择</h3> <p><strong>早期的非共识：</strong></p> <blockquote> <p>“我一直有这个非共识：我想要去做Agent。”</p> </blockquote> <blockquote> <p>“当时做的人很少，因为它太难了，或者不是一个共识类的事情。当时共识类任务是做问答，做翻译，或者做一些已经被社区接受的任务。”</p> </blockquote> <p><strong>简单通用的追求：</strong></p> <blockquote> <p>“我一直想做简单且通用的东西。我不想做一个很复杂、但只能在一个领域奏效的东西。这个方向在传统意义上很难被接受，大家习惯了做AI的方式：把问题不停细分，做很多细分方法。”</p> </blockquote> <hr/> <h2 id="agent系统的本质认知">Agent系统的本质认知</h2> <h3 id="agent的定义与演进">Agent的定义与演进</h3> <p><strong>广义定义：</strong></p> <blockquote> <p>“任何能进行自我决策、与环境交互，并试图optimize reward的系统，都可以被称为Agent。”</p> </blockquote> <p><strong>现代Agent的特点：</strong></p> <blockquote> <p>“今天我们讲的Agent更多是指：怎么基于语言模型这样的foundation model去做具备自我决策能力的系统，而不是传统意义上基于规则或仅在某个领域用强化学习训练出来的Agent。”</p> </blockquote> <p><strong>三波Agent发展：</strong></p> <ol> <li><strong>第一波：符号主义AI</strong> <ul> <li>基于规则的推理系统</li> <li>专家系统的尝试</li> <li>局限：无法覆盖所有情况，导致AI寒冬</li> </ul> </li> <li><strong>第二波：深度强化学习</strong> <ul> <li>DeepMind的游戏AI、AlphaGo</li> <li>OpenAI的机器手、Dota</li> <li>局限：环境特定，无法泛化</li> </ul> </li> <li><strong>第三波：基于语言模型</strong> <ul> <li>具备推理能力</li> <li>可以进入数字环境（编程、互联网）</li> <li>核心：方法线+任务线的共同进化</li> </ul> </li> </ol> <h3 id="language-agent的核心优势">Language Agent的核心优势</h3> <p><strong>开放空间决策能力：</strong></p> <blockquote> <p>“如果你要做一个language Agent，你需要的不只是选择能力，而是去自由产生新动作的能力。世界的本质就是，你的行为空间是open-ended的，这种在开放空间决策的能力BERT永远做不到。”</p> </blockquote> <p><strong>与传统Agent的区别：</strong></p> <blockquote> <p>“最大区别在于，语言模型提供了一个足够强的先验，这个先验让你可以推理，而推理又可以在不同的环境间泛化。”</p> </blockquote> <h3 id="推理能力是泛化的关键">推理能力是泛化的关键</h3> <p><strong>人类vs AI的差异：</strong></p> <blockquote> <p>“为什么我可以一下子去玩一个新的游戏，但现在这些系统或AI需要几十万步甚至几百万步训练，才能完成类似的事？我发现，是因为我可以思考。”</p> </blockquote> <p><strong>推理的价值：</strong></p> <blockquote> <p>“如果没有这样的思考能力，而是直接从复杂语言去预测’我要往后走’，就很难——没有推理做不到。核心是推理能力，推理才能带来泛化。”</p> </blockquote> <hr/> <h2 id="技术发展的下半场理论">技术发展的”下半场”理论</h2> <h3 id="上半场vs下半场">上半场vs下半场</h3> <p><strong>转折点的判断：</strong></p> <blockquote> <p>“主线正从’上半场’转向’下半场’。我说的主线是基于语言的智能体。从语言出发，去定义Reasoning、定义Agent，我们终于有了一个非常general的方法，而且这个方法是可泛化的——我们实现了一个基点时刻。”</p> </blockquote> <p><strong>本质变化：</strong></p> <blockquote> <p>“这带来一个本质变化：以前我面对很多怪兽，需要造出各种不同武器去打它们；现在我有了一把通用武器，比如机关枪，我不需要再为每个怪兽单独造武器。接下来要思考的问题就变成：我该朝哪个方向开枪？”</p> </blockquote> <h3 id="从训练模型到使用模型">从训练模型到使用模型</h3> <p><strong>瓶颈的转移：</strong></p> <blockquote> <p>“大家过去往往更关注模型训练、方法设计，但我觉得现在的bottleneck已经转移了：变成怎么去定义好的任务，怎么去定义好的环境。”</p> </blockquote> <p><strong>研究价值的重新定位：</strong></p> <blockquote> <p>“当时最有价值的，就是去研究怎么使用模型。如果你想训练模型，会落后OpenAI或这些公司好几年。你做的很有可能几年前别人已经发现了。如果你想做不一样的，可能怎么去使用模型更有价值。”</p> </blockquote> <h3 id="任务定义的重要性">任务定义的重要性</h3> <p><strong>环境的重要性：</strong></p> <blockquote> <p>“第二个learning是：任务或环境非常重要。当你有一个非常差的任务，你永远不可能学到非常好的东西。”</p> </blockquote> <p><strong>好任务的标准：</strong></p> <blockquote> <p>“首先你要找一个足够有挑战的任务，这个任务能做出本质的新方法。”</p> </blockquote> <hr/> <h2 id="openai能力分级体系解读">OpenAI能力分级体系解读</h2> <h3 id="l1-l3的递进逻辑">L1-L3的递进逻辑</h3> <p><strong>逻辑关系：</strong></p> <blockquote> <p>“逻辑是，首先你要有语言的先验知识。基于语言的先验知识，最早能做出来的应用是Chatbot（L1）。接下来，基于语言先验，你需要具备推理能力，这是Reasoner（L2）。当你既有语言知识，又具备推理能力，才可能进一步做各种Agent（L3），尤其是能泛化的Agent。”</p> </blockquote> <h3 id="l4与l5的正交关系">L4与L5的正交关系</h3> <p><strong>并行发展的观点：</strong></p> <blockquote> <p>“我一开始是认为Innovator（L4）和Organization（L5）是更正交或并列的关系。我当时在群里问了一个问题：当一个大公司CEO和一个科学家，到底哪一个难？这个不好说，实现路径有区别。所以，不用太纠结谁是第四级，谁是第五级，都很重要。不一定要先实现哪一个才能实现另一个，可以同时去探索。”</p> </blockquote> <p><strong>L4（创新者）的要求：</strong></p> <ol> <li><strong>Long-Term Memory</strong>： <blockquote> <p>“你作为一个Innovator，首先你需要一个Long-Term Memory。比如，我是Wiles，我研究费马大定理，可能花了20年。我就需要一个长期记忆。”</p> </blockquote> </li> <li><strong>内在奖励机制</strong>： <blockquote> <p>“我有这个长期记忆还不够，还需要有内在的reward。因为在你真正证明那件事之前，没有任何外部奖励——你没有获奖，没有做成任何’可交付’的事情，也没人给你feedback。你需要自己给自己反馈。”</p> </blockquote> </li> </ol> <p><strong>L5（组织者）的挑战：</strong></p> <blockquote> <p>“作为一个Organization，你需要解决的问题是：Agent和Agent之间怎么协作？怎么让Multi-Agent协作scale？”</p> </blockquote> <h3 id="实现路径与技术突破">实现路径与技术突破</h3> <p><strong>三个关键方向：</strong></p> <blockquote> <p>“在fundamental research上，比较重要的有三方面：一个是Memory，一个是Intrinsic Reward，还有一个是Multi-Agent。”</p> </blockquote> <p><strong>当前水平的定位：</strong></p> <blockquote> <p>“现在的Agent就像一个普通大学生，做一个数字化的实习生。或者说，AGI就是一个普通一本大学生在电脑上能做所有事情的一个能力。”</p> </blockquote> <p><strong>人类社会的价值边界：</strong></p> <blockquote> <p>“但是，人类社会的边界是什么？这当然覆盖80%或90%的人。但我们最崇拜的人，是哪两种？一种是创造新东西，在认知或审美上开创新领域的人：爱因斯坦、高更、梵高、贝多芬；另一种是能创造新组织、伟大组织的人：伊隆·马斯克、乔布斯。”</p> </blockquote> <hr/> <h2 id="code作为ai的手">Code作为AI的”手”</h2> <h3 id="数字世界的affordance">数字世界的Affordance</h3> <p><strong>类比人类的手：</strong></p> <blockquote> <p>“Code有点像人的手。它某种程度上，是AI最重要的affordance。对于物理世界，人最重要的affordance是手——我们围绕它制造各种工具，比如锤子、笔、筷子。但对AI、对Digital Agent来说，最重要的affordance可能就是code。”</p> </blockquote> <p><strong>天然的机器语言：</strong></p> <blockquote> <p>“因为其他东西，都是给人定义的。比如网页、小说、视频，是为人类设计的；但code是一个天然就给机器使用的表达形式。”</p> </blockquote> <h3 id="api-vs-gui的争论">API vs GUI的争论</h3> <p><strong>经典辩论：</strong></p> <blockquote> <p>“有个非常经典的debate：最终的AGI，是基于API或code的？还是基于GUI？或者是为人定义的前端环境？还是它是一个混合体？”</p> </blockquote> <p><strong>现实的解决方案：</strong></p> <blockquote> <p>“这个问题有点像：你是想改造你的车让它适应所有路，还是改造所有路让它适应现在的车？当然，最终结果很可能是meet in the middle，两边都会做，而且这个事情可能没那么难。”</p> </blockquote> <h3 id="编程环境的特殊价值">编程环境的特殊价值</h3> <p><strong>多轮反馈的重要性：</strong></p> <blockquote> <p>“我们当时做了一个工作叫InterCode。大家都在做的是：给一个coding task模型生成一段代码，然后你去evaluate它。但我们就在想：为什么不把执行结果反馈给模型？我们可以让它变成一个多轮Agent task，构造成一个环境，而不是单次完成的任务。”</p> </blockquote> <p><strong>非共识的坚持：</strong></p> <blockquote> <p>“有时候，很有意思的一点：一个东西明明非常重要，但就是没人做。如果你是一个研究员，觉得你做的事很重要，但别人不觉得、也没人做，并不是坏事——可能它真的很重要，只是大家还没开始。”</p> </blockquote> <hr/> <h2 id="任务设计与评估哲学">任务设计与评估哲学</h2> <h3 id="基于结果的奖励机制">基于结果的奖励机制</h3> <p><strong>设计原则：</strong></p> <blockquote> <p>“我从很早就有一个偏好：我想定义一个基于结果的reward，而不是基于过程的；而且这个reward应该是基于规则、可计算的，而不是来自人的偏好、模型的偏好，或者一些黑盒指标。”</p> </blockquote> <p><strong>成功案例的特征：</strong></p> <blockquote> <p>“像math和coding这种任务，之所以能做出来，核心就是：Reward是基于结果，不是基于过程；Reward是白盒的、基于规则的，不是基于人的偏好或模型的偏好。”</p> </blockquote> <p><strong>避免Hacking的重要性：</strong></p> <blockquote> <p>“但如果你reward是基于过程，就会出现hacking。你去优化人的偏好、模型的偏好，也会出现hacking。比如你生成一段非常优美的代码，但它并不解决实际问题。”</p> </blockquote> <h3 id="passk-vs-passk">Pass@k vs Pass^k</h3> <p><strong>两种不同的评估需求：</strong></p> <blockquote> <p>“有些任务我们需要优化的是Pass@k（多次尝试中至少成功一次），而另一些任务，比如客服，我们需要优化的是Pass^k（每次都成功），或者我们最关心的是Pass@1（一次就要成功）。”</p> </blockquote> <p><strong>任务特性的差异：</strong></p> <ul> <li><strong>创造性任务</strong>：允许多次失败，只要有一次成功</li> <li><strong>可靠性任务</strong>：需要极高的稳定性，每次都要成功</li> </ul> <h3 id="robustness的重要性">Robustness的重要性</h3> <p><strong>被忽视的问题：</strong></p> <blockquote> <p>“现在我们对于简单任务的robustness并没有特别重视——这是因为大家做AI还是在做一些benchmark，而不是实际应用。”</p> </blockquote> <p><strong>思维转变的价值：</strong></p> <blockquote> <p>“但如果你接受了这个mindset转变，很自然你就会意识到：有些应用是需要特别强调robustness的，那你就需要去优化它的robustness。现在大家还没完全意识到这件事；但我相信，如果大家意识到这个转变，会带来很大进步。”</p> </blockquote> <hr/> <h2 id="语言与泛化的本质">语言与泛化的本质</h2> <h3 id="语言作为通用工具">语言作为通用工具</h3> <p><strong>独特性的来源：</strong></p> <blockquote> <p>“为什么语言非常独特？因为它是人在这个世界完成各种各样事情的工具。语言也是人类发明的工具，像火或笔一样。但它之所以特殊，是因为它是一个帮助你解决任何事情的通用性或泛化性的工具。”</p> </blockquote> <p><strong>与其他工具的区别：</strong></p> <blockquote> <p>“当你学会了这门工具，你可以去做很多新任务。比如你学会了攀岩，它帮不了你完成新任务。但你学会了语言，你可以通过语言和人交流，学习、思考、推理。”</p> </blockquote> <p><strong>本质认知：</strong></p> <blockquote> <p>“2020年以前，大家没把这个事想清楚，觉得语音、文字、图像、视频都是一些数据，没什么区别。但我觉得最大区别是：语言是人为了实现泛化而发明出来的工具，这一点比其他东西更本质。”</p> </blockquote> <h3 id="强化学习的泛化能力">强化学习的泛化能力</h3> <p><strong>历史性突破：</strong></p> <blockquote> <p>“我之所以这么说，是因为在此前，如果你在一个特定环境上训练，模型只能在这个环境表现良好，不能轻易迁移到其他环境。但现在，你在一个环境上训练，模型可以适应更多不同环境，这才是最本质的区别。”</p> </blockquote> <p><strong>具体表现：</strong></p> <blockquote> <p>“DeepSeek大家觉得一个有趣结果是：你在数学和编程领域用强化学习训练模型，但它在创意写作上也变得更强。这体现了本质区别：AlphaGo只能下围棋，不能下象棋；而现在你学会数学，也能提高创意写作。”</p> </blockquote> <p><strong>泛化的机制：</strong></p> <blockquote> <p>“但我觉得，它还是泛化的。原因是它能够推理。当你能在一个环境学到如何思考的技能，并且这种思考能力能迁移到新环境，这才是泛化的本质原因。”</p> </blockquote> <h3 id="内在激励机制">内在激励机制</h3> <p><strong>创新者的驱动力：</strong></p> <blockquote> <p>“就像我刚刚说的，很多创新者之所以能在没有外在激励的情况下坚持，是因为他有内在的价值观或激励机制。”</p> </blockquote> <p><strong>婴儿的好奇心模型：</strong></p> <blockquote> <p>“这个问题，AI和神经科学已经研究多年。婴儿是最典型的例子。他们拥有基于好奇心或自我激励的机制。很多婴儿会反复玩一个玩具，用嘴去咬一个东西，或者做一些看似’无意义’的动作。”</p> </blockquote> <p><strong>成长的转变：</strong></p> <blockquote> <p>“当人长大之后，会发生重要变化。当你是婴儿，你对世界的理解，是基于视觉、触觉，基于物理世界的。当你长大之后，你对世界的理解方式变了，变成一个基于语言、推理、文字系统的理解。你玩的，不再是一个物理游戏，而是一个文字游戏。”</p> </blockquote> <p><strong>AI面临的挑战：</strong></p> <blockquote> <p>“这是AI面临的挑战：传统AI，比如玩迷宫、做机器人仿真，它可以定义一些基于世界模型或者模仿婴儿阶段好奇心的内在激励。但当AI在玩的是一个语言游戏，要怎么定义内在激励？——这个问题就变得不太一样了。”</p> </blockquote> <hr/> <h2 id="记忆层级理论">记忆层级理论</h2> <h3 id="环境作为最外层记忆">环境作为最外层记忆</h3> <p><strong>冯诺依曼的洞察：</strong></p> <blockquote> <p>“前年冬天，我读到冯诺依曼临终前写的一本书，The Computer and the Brain。最让我印象深刻的一句话是：Essentially, the Environment is always the most outer part of the Memory Hierarchy.（基本上，环境永远是记忆层级中最外层的部分。）这很哲学。”</p> </blockquote> <p><strong>人类的记忆层级：</strong></p> <blockquote> <p>“对于人，你有你的Memory Hierarchy，有Working Memory、Long-Term Memory在脑子里，但最外层是你的笔记本、Google Doc、Notion，这些是你最外层Memory Hierarchy的一部分。”</p> </blockquote> <p><strong>Agent的记忆层级：</strong></p> <blockquote> <p>“某种程度上，是的。从Agent角度看，这个世界有一个Memory Hierarchy。Memory Hierarchy最外层永远是环境。”</p> </blockquote> <h3 id="long-context-vs-long-term-memory">Long Context vs Long-Term Memory</h3> <p><strong>实现关系：</strong></p> <blockquote> <p>“Long Context是实现Long-Term Memory的一种方式。如果你能实现1亿或1千亿或无限长的Context，它是实现Long-Term Memory的一种方式。它是一种和人区别很大的方式，但这是有可能的。”</p> </blockquote> <p><strong>评估的问题：</strong></p> <blockquote> <p>“起码到去年为止，大家主要还在做所谓Long Range Arena，比如needle in the haystack——我有一个很长的输入，我在中间插入一句话，比如’姚顺雨现在在OpenAI’，然后我问你相关问题。这是一个必要但不充分的任务。”</p> </blockquote> <h3 id="context的经济价值">Context的经济价值</h3> <p><strong>人类不可替代的原因：</strong></p> <blockquote> <p>“为什么我们现在的模型，推理很强，考试很强，玩游戏很强；但它还没创造出足够经济价值？——根本原因是：它没有这些Context。”</p> </blockquote> <p><strong>分布式系统的特点：</strong></p> <blockquote> <p>“人类社会比较tricky的一点是：当然，我们确实写下了很多东西——我们用文字、Google Doc、Notion，记录了很多东西；但很多Context永远只存在人的大脑，是通过一个分布式的系统来维护。”</p> </blockquote> <p><strong>解决方案的价值：</strong></p> <blockquote> <p>“如果这个问题解决了，Utility问题就可以在很大程度被解决。这个世界，大多数人并不是乔布斯，也不是爱因斯坦，只是一个普通人。他的数学推理没有o3强，但他能manage Context。”</p> </blockquote> <hr/> <h2 id="创业与产品思考">创业与产品思考</h2> <h3 id="创业公司的机会">创业公司的机会</h3> <p><strong>正确的担心：</strong></p> <blockquote> <p>“创业公司应该担心的是模型没有溢出能力，这样你就真的什么都做不了了。有溢出能力是个非常好的事情，这几乎意味着你有机会。”</p> </blockquote> <p><strong>最大机会所在：</strong></p> <blockquote> <p>“创业公司最大机会是：能设计不同的interface，或者说人和数字世界交互的方式。”</p> </blockquote> <p><strong>成功的条件：</strong></p> <blockquote> <p>“对于创业公司，最好的机会是：你做新的交互方式，并且模型不停有新的溢出能力，让你能够赋能这些新的交互方式——两者缺一不可。”</p> </blockquote> <h3 id="交互方式的创新">交互方式的创新</h3> <p><strong>大厂的路径依赖：</strong></p> <blockquote> <p>“但拥有一个Super App对于公司是双刃剑。当你已经有了一个交互方式，你必然形成路径依赖。当你有像ChatGPT这样的Super App，很自然你的研究就会center around这个Super App，会center around这个交互方式。”</p> </blockquote> <p><strong>Cursor的价值：</strong></p> <blockquote> <p>“Cursor是很好的例子，创造了一种新的交互。不是像人一样的交互，而是像Copilot。写代码的时候，它能给你提示或编辑。没有人和人是这样交互的。这是它的价值所在。”</p> </blockquote> <p><strong>通用性与应用的平衡：</strong></p> <blockquote> <p>“一个比较理想的情况，你有一个非常通用的交互方式，这个交互方式想象力足够大。但并不矛盾的是，你可以有每个阶段的Killer App。”</p> </blockquote> <h3 id="数据飞轮的条件">数据飞轮的条件</h3> <p><strong>成功案例分析：</strong></p> <blockquote> <p>“比较成功的是Midjourney，有非常清晰的reward——人更喜欢哪张图，这个reward和应用是对齐的，reward做得更好，公司就更成功，模型也更好——一切都对齐。有了这种情况，才能自己训练模型，做数据飞轮。”</p> </blockquote> <p><strong>必要条件：</strong></p> <blockquote> <p>“如果你要有数据飞轮，首先你要能自己去训模型，并且能通过交互有很好的reward，使你能把好的数据和不好的数据分开。”</p> </blockquote> <p><strong>相互借鉴的关系：</strong></p> <blockquote> <p>“这世界是个相互抄的关系，而不是一个单向抄的关系。”</p> </blockquote> <hr/> <h2 id="未来生态构想">未来生态构想</h2> <h3 id="多元化vs单极化">多元化vs单极化</h3> <p><strong>多面向系统的观点：</strong></p> <blockquote> <p>“对于不同的任务和交互，需要不同的Agent系统去解决。模型是可以share的，但如果你讨论的是整个系统，那就不一样了。就像你问，这个世界上最强的互联网网站是什么？最强的互联网公司是什么？很难回答。它是一个multiface的系统，有很多不同侧面。”</p> </blockquote> <p><strong>避免灰暗的未来：</strong></p> <blockquote> <p>“AI可能也会变成这样的结构。OpenAI可能会成为一个类似Google的公司，成为新世界里非常重要的一环——但这并不代表，这个世界就会被这样一个单极系统垄断。如果真是那样，这个世界就会变得很灰暗。大多数人也就没什么价值了。”</p> </blockquote> <h3 id="agent社会的可能性">Agent社会的可能性</h3> <p><strong>信息差的价值：</strong></p> <blockquote> <p>“为什么这个世界上很多人有价值？不是因为他们的数学或编码能力强，而是因为他们拥有别人没有的信息。中间商本质是拥有信息差。拥有信息差的人会想维护自己的权利和资源。”</p> </blockquote> <p><strong>分布式网络的形态：</strong></p> <blockquote> <p>“在交易世界里，信息很重要，每个人只拥有信息的一小部分，这种情况会出现新的不同形态。可能是Multi-Agent，每个人有自己的Agent，Agent之间可以与百万甚至更多人交换信息，达成交易或某些目的。”</p> </blockquote> <h3 id="中心化与去中心化的平衡">中心化与去中心化的平衡</h3> <p><strong>双重趋势：</strong></p> <blockquote> <p>“我最近的一个思考是这样：我感觉人类社会是一个网络，它有两个重要性质：一个性质是中心化程度，也可以说是资源分配的集中性。还有另一个维度，是你从网络边缘到中心的速度或可能性。”</p> </blockquote> <p><strong>历史的观察：</strong></p> <blockquote> <p>“过去几百年发生的事情是这样：网络越来越中心化，贫富差距越来越大，二八定律、马太效应更明显；但与此同时，平民或普通人翻身的机会也变多了。”</p> </blockquote> <p><strong>技术发展的影响：</strong></p> <blockquote> <p>“看起来，技术发展的趋势是两件事同时加剧——一方面，中心化加剧，因为效率这个因素是根本性的；另一方面，创造新东西的机会，起码到目前为止，是越来越多的。变得更中心化和变得更diverse，可能并不矛盾。”</p> </blockquote> <p><strong>力量的平衡：</strong></p> <blockquote> <p>“根本上，现在非常强的巨头和重要节点，有动力继续推动中心化。但在中心化之外的力量，也有动力做一些非中心化的事情。这个世界可能不会是单方压倒另一方，双方都会有自己的力量。”</p> </blockquote> <hr/> <h2 id="人与ai的关系思考">人与AI的关系思考</h2> <h3 id="效用导向的设计原则">效用导向的设计原则</h3> <p><strong>核心判断标准：</strong></p> <blockquote> <p>“Again，这是一个Utility Problem。很多问题上，人的方式并不一定更有价值。比如下围棋、开车。但有些事情，人就是做得更好。那你就应该思考，怎么去bridge the gap？”</p> </blockquote> <p><strong>具体应用场景：</strong></p> <blockquote> <p>“下围棋、打游戏，基于强化学习可以学到和人不一样、甚至更好的方式，就不需要像人。但如果在一个公司打工，和老板搞好关系，完成各种各样的任务，人就是比AI做得更好，就需要更像人。”</p> </blockquote> <p><strong>拟人化的条件：</strong></p> <blockquote> <p>“一个事情如果有价值，就会产生。比如，很多人很孤独，他需要一个朋友，技术如果能创造这样的体验，拟人化就是合理存在的未来。”</p> </blockquote> <h3 id="从认知科学到第一性原理">从认知科学到第一性原理</h3> <p><strong>认知的转变：</strong></p> <blockquote> <p>“我现在觉得，一个更好的方法是：你先去思考人能做什么，而机器现在不能做。这是客观事实。但你找到差异之后，你可以基于第一性原理去思考，如何解决这个问题。你不一定要依赖’人是怎么解决这个问题的’来解决它。”</p> </blockquote> <p><strong>借鉴的边界：</strong></p> <blockquote> <p>“所以，从人身上可以借鉴的一点：哪些事情是人可以做，而机器目前不能做？这点比较robust和客观。但至于’人是怎么做到的’，以及’我们在多大程度上要借鉴这种方式’，这个问题本身更主观、也更noisy。”</p> </blockquote> <h3 id="安全与价值的权衡">安全与价值的权衡</h3> <p><strong>现实的优先级：</strong></p> <blockquote> <p>“我会担心。但现在最大问题是——AGI还没实现，我们还没创造足够价值。如果我们还没想清楚，怎么把它变得有价值，就急着把它变得很安全，好像没有意义。”</p> </blockquote> <p><strong>商业驱动的安全：</strong></p> <blockquote> <p>“安全是很复杂的问题。比如ChatGPT，如果它不安全，产品就失败了，没有商业价值。即使是为了商业价值，它也会重视安全。”</p> </blockquote> <hr/> <h2 id="openai内部视角">OpenAI内部视角</h2> <h3 id="非共识决策的勇气">非共识决策的勇气</h3> <p><strong>历史的选择：</strong></p> <blockquote> <p>“但问题在于，如果你没有一个different bet，很难超越前面的霸主。如果OpenAI一直做强化学习，可能很难超过DeepMind。即使你在某些任务上做得比它好，人们提到强化学习，想到的还是DeepMind。”</p> </blockquote> <p><strong>GPT的赌注：</strong></p> <blockquote> <p>“你要想超越之前的霸主，就必须有一个different bet。而GPT是那个不同的赌注——但这个选择在当时是一个非共识的事情。”</p> </blockquote> <p><strong>内部的分歧：</strong></p> <blockquote> <p>“我说实话，当时OpenAI内部绝大多数人也不认为scale-up是最promising的方向，我觉得这是有可能的。”</p> </blockquote> <p><strong>领导者的价值：</strong></p> <blockquote> <p>“Ilya最大贡献并不是他做了GPT‑1，或者他具体参与了什么技术工作；而是，他是那个号召大家all in这个方向的人。”</p> </blockquote> <h3 id="强化学习的持续重要性">强化学习的持续重要性</h3> <p><strong>历史的连续性：</strong></p> <blockquote> <p>“历史并不是说我把强化学习彻底抛弃，转而走另一条路，再返回来走强化学习，而是更soft的过程。”</p> </blockquote> <p><strong>产品化的关键：</strong></p> <blockquote> <p>“后来证明，ChatGPT成功，强化学习也很关键。没有RLHF，没有Alignment技术，它也没办法形成一个产品。”</p> </blockquote> <h3 id="扩展维度的未来">扩展维度的未来</h3> <p><strong>新的可能性：</strong></p> <blockquote> <p>“会有新的scaling dimension出现。如果你有大量的Memory，你的test-time compute就会有所增加，可以用新的方式scale。如果你有了Multi-Agent，那你的test-time compute又会出现另一个新维度去扩展。”</p> </blockquote> <p><strong>复杂的选择：</strong></p> <blockquote> <p>“我觉得会有新的scale dimension出现，但当你有很多scale dimension，怎么去选择？怎么基于某一个应用去分配不同scale维度的比重？——这会是一个很有意思的问题。”</p> </blockquote> <hr/> <h2 id="个人哲学与价值观">个人哲学与价值观</h2> <h3 id="通用性的追求">通用性的追求</h3> <p><strong>从小的特质：</strong></p> <blockquote> <p>“我从小是一个比较general的人——我想试图变得很通用，试图了解很多不同的学科，做很多不同的事情。”</p> </blockquote> <p><strong>认知的升华：</strong></p> <blockquote> <p>“但后来我发现，一个人即使再聪明、再有精力，他能理解的知识或能做的事情，也只是人类社会积累的知识的很小一部分。更好的是，你去创造一个比你更通用、更general的事情。”</p> </blockquote> <p><strong>执念的力量：</strong></p> <blockquote> <p>“我好像一直对于通用性，有一种执念或追求。”</p> </blockquote> <h3 id="创造不同的驱动力">创造不同的驱动力</h3> <p><strong>价值的追求：</strong></p> <blockquote> <p>“用一个非常俗的话说，希望你对这个世界创造一些不同——探索新的、根本性的研究，是一种创造不同的方式；创造一种完全不同的新的产品形态，也是一种创造不同的方式。”</p> </blockquote> <p><strong>导师的智慧：</strong></p> <blockquote> <p>“我导师令我印象最深的是这样一句话。学术圈经常发生这样的事——你有一个想法，然后别人做了，你会很烦。他说：If someone else can do it, then it’s okay to let them do it（如果别人能做，那就让他们去做吧）。”</p> </blockquote> <p><strong>全局的思考：</strong></p> <blockquote> <p>“从人类全局的角度，如果这个事情很多人能做，别人做可能是不是也没有什么区别？对这个社会，或者对整体来说，似乎没有什么变化。”</p> </blockquote> <h3 id="时代机遇的把握">时代机遇的把握</h3> <p><strong>时代的特殊性：</strong></p> <blockquote> <p>“但我觉得恰好是这个时代，你去做上限更高的事情是更好的。因为现在有一个巨大的机会。如果没有这样一个巨大的机会，最佳路径可能是去做incremental、确定性强的事情，一步一步地积累。但恰好有一个上限非常高的事情。”</p> </blockquote> <p><strong>勇气的回报：</strong></p> <blockquote> <p>“如果你敢想，或者你胆子特别大，或者你想象力很丰富，就会有好事发生。”</p> </blockquote> <p><strong>技术的通用性：</strong></p> <blockquote> <p>“但这个时代很幸运的一点：这个技术非常通用，这个技术非常伟大，有足够多探索的空间。”</p> </blockquote> <p><strong>个人的选择：</strong></p> <blockquote> <p>“另一点是，我想让生活更有趣，更有意思，更快乐，就去做一些自己喜欢的事情。这很难用语言解释，就是一个taste或preference的问题。”</p> </blockquote> <hr/> <h2 id="结语">结语</h2> <p>姚顺雨的观点体系体现了一个研究者从技术深度到哲学高度的完整思考。他不仅在技术层面提出了”下半场”理论、记忆层级理论等重要观点，更在价值层面思考了AI发展的多元化可能性。他的非共识选择、对通用性的追求，以及对创造差异化价值的坚持，为AI研究者和创业者提供了重要的思维框架。</p> <p>他的核心信念可以概括为：<strong>在技术快速发展的时代，要有勇气做非共识的选择，追求简单而通用的解决方案，并始终以创造真实价值为导向。</strong> 这种哲学不仅适用于AI研究，也适用于更广泛的创新活动。</p>]]></content><author><name></name></author><category term="research-insights"/><category term="AI"/><category term="Agent"/><category term="LLM"/><category term="research"/><summary type="html"><![CDATA[OpenAI研究员姚顺雨关于AI Agent发展的深度思考与独到见解]]></summary></entry><entry><title type="html">杨植麟观点精华：AI时代的技术哲学与实践</title><link href="https://emigmo.github.io/blog/2025/yang-zhilin-ai-philosophy/" rel="alternate" type="text/html" title="杨植麟观点精华：AI时代的技术哲学与实践"/><published>2025-07-15T14:00:00+00:00</published><updated>2025-07-15T14:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/yang-zhilin-ai-philosophy</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/yang-zhilin-ai-philosophy/"><![CDATA[<h2 id="前言">前言</h2> <p>本文基于2025年7月对月之暗面创始人杨植麟的深度访谈整理而成。作为AI领域的重要创业者和技术专家，杨植麟在这次访谈中分享了他对AI发展的深度思考，从哲学层面的认知框架到具体的技术实践，从产品理念到组织管理，构建了一个完整的AI时代思考体系。</p> <hr/> <h2 id="一技术哲学与认知框架">一、技术哲学与认知框架</h2> <h3 id="11-无限的山问题与解决的永恒循环">1.1 无限的山：问题与解决的永恒循环</h3> <p>杨植麟深受David Deutsch的《无穷的开始》影响，认为AI研发正处于类似启蒙运动后的动态知识创造状态：</p> <p><strong>核心观点</strong>：</p> <ul> <li><strong>问题的不可避免性</strong>：”问题是不可避免的”和”问题是可以解决的”是可以刻在石头上的两句话</li> <li><strong>知识边界的拓展</strong>：每当解决一个问题，就会带来新的问题，因为知识边界在不断拓展</li> <li><strong>技术进步的本质</strong>：每解决一个问题，技术就能往上攀登几百米</li> </ul> <p><strong>实践体现</strong>：</p> <ul> <li>解决了强化学习问题，又遇到评估、衡量、验证的新问题</li> <li>AI研发是一个不断解决问题、产生新问题的过程</li> <li>“也许有一天会发现，这座雪山没有尽头——我希望它一直没有尽头”</li> </ul> <h3 id="12-agi不是终点而是方向">1.2 AGI不是终点，而是方向</h3> <p><strong>对AGI的重新定义</strong>：</p> <ul> <li>AGI不是某一级台阶，不会突然一夜之间达到</li> <li>它是一个持续的方向，而非固定的终点</li> <li>今天在很多领域已经可以认为达到了AGI水平（如数学、编程竞赛）</li> </ul> <p><strong>两个层面的理解</strong>：</p> <ol> <li><strong>技术层面</strong>：技术能力持续提升</li> <li><strong>社会层面</strong>：技术对人类社会的影响，需要几十、几百年消化</li> </ol> <p><strong>与登月的区别</strong>：</p> <ul> <li>登月有明确的成功标志</li> <li>AI很难在某个时间点宣布”实现了AGI”</li> <li>这是一个动态进化的过程</li> </ul> <h3 id="13-ai是人类文明的放大器">1.3 AI是人类文明的放大器</h3> <p><strong>AI的本质定位</strong>：</p> <ul> <li>AI是”人类文明的放大器”和”巨大的杠杆”</li> <li>它将成为Meta science（元科学）</li> <li>从启蒙运动到现在，下一个突破知识边界的是AI</li> </ul> <p><strong>对人类价值的思考</strong>：</p> <ul> <li>人的一生有三个意义：<strong>创造、体验和爱</strong></li> <li>“创造”的很大部分AI可以做，但”体验”和”爱”会是以人为中心的</li> <li>人类的独特价值在AI时代会持续存在</li> </ul> <p><strong>风险与应对</strong>：</p> <ul> <li>承认AI摧毁人类文明的风险存在</li> <li>但不能因噎废食，放弃就等于放弃了人类文明的上限</li> <li>需要更安全的对齐和更好的社会机制</li> </ul> <h2 id="二技术范式与发展路径">二、技术范式与发展路径</h2> <h3 id="21-两种推理范式缸中之脑vs交互式智能体">2.1 两种推理范式：”缸中之脑”vs交互式智能体</h3> <p><strong>“缸中之脑”模式（如o1）</strong>：</p> <ul> <li>特点：在自己大脑里思考，不需要与外界交互</li> <li>机制：通过让模型做很多尝试和反思，反思是重点</li> <li>能力：提出新猜想 + 验证猜想</li> <li>本质：把Pass@k变成Pass@1</li> </ul> <p><strong>交互式智能体模式</strong>：</p> <ul> <li>特点：与外界进行多轮交互</li> <li>行为：边思考边操作（搜索、浏览器、写代码等）</li> <li>优势：下一步行为基于交互反馈和状态更新</li> </ul> <p><strong>共同指向</strong>：</p> <ul> <li>两种范式都指向<strong>Test-time Scaling</strong>（测试时扩展）</li> <li>通过规模化token来完成更复杂任务</li> <li>能够端到端完成复杂工作，如代码仓库翻译、调试、测试</li> </ul> <h3 id="22-l1-l5能力等级的非线性发展">2.2 L1-L5能力等级的非线性发展</h3> <p><strong>OpenAI分级体系</strong>：</p> <ul> <li>L1：Chatbot（聊天机器人）</li> <li>L2：Reasoner（推理者）</li> <li>L3：Agent（智能体）</li> <li>L4：Innovator（创新者）</li> <li>L5：Organizer（组织者）</li> </ul> <p><strong>杨植麟的核心观点</strong>：</p> <ul> <li><strong>非串行关系</strong>：这些能力不一定是互相依赖的线性发展</li> <li><strong>技术路径选择</strong>：可以先做Agent再做Reasoning，如Claude的路线</li> <li><strong>最终融合</strong>：要做到最好的Agent，必须把Reasoning也做到最好</li> </ul> <p><strong>L4创新者的关键</strong>：</p> <ul> <li>标志：模型参与模型本身的开发</li> <li>实现：K2参与K3的开发过程</li> <li>依赖：需要强大的Agentic能力</li> </ul> <p><strong>L4与L3的互相促进</strong>：</p> <ul> <li>用L4的技术去解决L3的问题</li> <li>Agent泛化不够，需要用创新去解决</li> <li>体现了能力发展的非线性特征</li> </ul> <h3 id="23-test-time-scaling的核心价值">2.3 Test-time Scaling的核心价值</h3> <p><strong>定义与意义</strong>：</p> <ul> <li>在测试时或推理时实现更好的规模化</li> <li>突破传统单轮对话的token限制</li> <li>通过增加轮数或每轮思考token数来完成复杂任务</li> </ul> <p><strong>实现方式</strong>：</p> <ul> <li>长思考的强化学习</li> <li>Agent的强化学习</li> <li>多轮交互和工具使用</li> </ul> <p><strong>价值体现</strong>：</p> <ul> <li>能够花几小时完成复杂任务，无需人工参与</li> <li>实现端到端的复杂工作流程</li> <li>大幅提升模型的实际应用能力</li> </ul> <h2 id="三模型训练与技术创新">三、模型训练与技术创新</h2> <h3 id="31-token-efficiency突破数据墙的关键">3.1 Token Efficiency：突破数据墙的关键</h3> <p><strong>问题背景</strong>：</p> <ul> <li>高质量数据增长缓慢，接近常数</li> <li>多模态数据无法很好提升文本”智商”</li> <li>Scaling Law遇到数据墙</li> </ul> <p><strong>解决方案</strong>：</p> <ul> <li>提升<strong>Token Efficiency</strong>，把一份数据当成几份用</li> <li>使用Muon优化器，获得两倍提升效果</li> <li>通过数据改写（Rephrase）提升泛化能力</li> </ul> <p><strong>Muon优化器的优势</strong>：</p> <ul> <li>不是独立考虑每个元素，而是整体考虑矩阵参数的依赖关系</li> <li>在compute optimal情况下，学一份数据相当于用Adam学两份数据</li> <li>解决了大规模训练中的max logit爆炸问题</li> </ul> <p><strong>数据改写策略</strong>：</p> <ul> <li>对高质量数据进行多种改写操作</li> <li>避免同一份数据多次学习的过拟合问题</li> <li>通过改写增加一定程度的泛化能力</li> </ul> <h3 id="32-强化学习范式的转变">3.2 强化学习范式的转变</h3> <p><strong>技术路线转变</strong>：</p> <ul> <li>从预训练+SFT转向预训练+强化学习</li> <li>发现不需要太多process reward或value function</li> <li>直接用端到端的reward就能训练得很好</li> </ul> <p><strong>强化学习的优势</strong>：</p> <ul> <li>泛化性比SFT更好</li> <li>有更多on-policy采样，模型从自身采样中学习</li> <li>具有负梯度，scaling效率比Pre-Training高很多</li> </ul> <p><strong>挑战与限制</strong>：</p> <ul> <li>泛化仍然有限，”种瓜得瓜，种豆得豆”</li> <li>需要搭配好的评估和验证机制</li> <li>希望通过更多AI参与训练来摆脱局限</li> </ul> <h3 id="33-agent泛化性的挑战与解决">3.3 Agent泛化性的挑战与解决</h3> <p><strong>核心挑战</strong>：</p> <ul> <li>Agent最缺的是泛化能力</li> <li>现有RL技术局限于单点任务和评价指标</li> <li>容易过拟合到特定工具、环境或任务</li> </ul> <p><strong>泛化的重要性</strong>：</p> <ul> <li>如果泛化能力强，垂直Agent就没那么必要</li> <li>通用Agent能泛化到长尾工具，解决专有问题</li> <li>只需接入定制数据库、API、文档接口即可</li> </ul> <p><strong>解决思路</strong>：</p> <ol> <li><strong>用AI训练AI</strong>：让模型参与更多训练过程</li> <li><strong>AI native方式</strong>：摆脱人工设计的局限</li> <li><strong>更好的评估机制</strong>：解决Benchmark不够用或失效的问题</li> <li><strong>课程学习</strong>：从中等难度任务开始，逐步提升</li> </ol> <p><strong>Agent的两个关键特征</strong>：</p> <ul> <li><strong>多轮</strong>：实现test-time scaling</li> <li><strong>工具</strong>：连接”脑”与外部世界</li> </ul> <h2 id="四产品与商业思考">四、产品与商业思考</h2> <h3 id="41-模型即产品的理念">4.1 模型即产品的理念</h3> <p><strong>核心理念</strong>：</p> <ul> <li>训练模型时就要把整套系统搭好</li> <li>模型训练完成，产品也基本完成</li> <li>产品在训练过程中完成，而非训练后开发</li> </ul> <p><strong>Agent产品的特殊性</strong>：</p> <ul> <li>需要把模型与工具和Context结合</li> <li>模型性能在训练中已经与工具、环境适配好</li> <li>交互改进只是锦上添花</li> </ul> <p><strong>系统复杂性</strong>：</p> <ul> <li>简单：所有东西放在同一个模型，不需要维护多个模型</li> <li>复杂：要让模型在各种场景下都能工作，对通用性要求很高</li> <li>挑战：避免只拟合单点能力，要保证真正的通用性</li> </ul> <h3 id="42-开源vs闭源的战略选择">4.2 开源vs闭源的战略选择</h3> <p><strong>对开源的重新认识</strong>：</p> <ul> <li>承认之前”领先者不会开源”的判断，因为月之暗面在全球范围内还没完全领先</li> <li>开源更多是赋能下游应用，而非反哺基础模型提升</li> <li>社区贡献主要在推理侧，模型本身的提升仍只有原厂能做</li> </ul> <p><strong>开源的价值</strong>：</p> <ul> <li>可基于开源模型做Agentic Post-Training，催生专用智能体</li> <li>与社区分享技术know-how，加速技术提升</li> <li>形成开源生态，推动技术发展</li> </ul> <p><strong>战略平衡</strong>：</p> <ul> <li>希望长期分享更多技术，但不一定只做开源</li> <li>既有技术信仰，也有市场博弈策略</li> <li>最终希望让技术更安全、更快达到更好水平</li> </ul> <h3 id="43-一方产品的竞争优势">4.3 “一方产品”的竞争优势</h3> <p><strong>定义</strong>：</p> <ul> <li>模型公司自己做产品，控制上下文环境、工具接口、prompt结构</li> <li>自己当”使用方”，而不只是提供API</li> </ul> <p><strong>相对于”三方产品”的优势</strong>：</p> <ul> <li>正向设计vs逆向工程</li> <li>先设计好工具和Context Engineering，再在此环境中训练模型</li> <li>模型天然在自己环境中表现更好</li> <li>可以更好整合工具和模型，端到端训练</li> </ul> <p><strong>发展趋势</strong>：</p> <ul> <li>Claude Code、ChatGPT Agent都是”一方产品”</li> <li>上限可能更高，但不一定能覆盖所有Agent领域</li> <li>与”三方产品”会形成合作与竞争并存的生态</li> </ul> <h2 id="五组织管理与个人成长">五、组织管理与个人成长</h2> <h3 id="51-用rl方式管理团队">5.1 用RL方式管理团队</h3> <p><strong>管理哲学的转变</strong>：</p> <ul> <li>科研、模型训练、组织管理都遵循RL原理</li> <li>从SFT式管理转向RL式管理</li> <li>核心是掌握SFT和RL的平衡</li> </ul> <p><strong>RL管理的特点</strong>：</p> <ul> <li>给团队成员目标和奖励，而非具体指令</li> <li>保持团队成员的主观能动性和创新能力</li> <li>建立多个观测指标，避免过拟合单一目标</li> </ul> <p><strong>挑战与风险</strong>：</p> <ul> <li><strong>Reward Hacking</strong>：容易被利用漏洞，看起来结果很好但实际没达到目标</li> <li><strong>奖励定义</strong>：需要深入理解具体细节，合理定义reward</li> <li><strong>平衡艺术</strong>：SFT太多会失去创造力，RL过度会被hack</li> </ul> <h3 id="52-技术决策的方法论">5.2 技术决策的方法论</h3> <p><strong>决策原则</strong>：</p> <ul> <li>基于充分的实验数据，不能拍脑袋</li> <li>需要非常了解实验的具体结果</li> <li>技术战略是公司战略的关键部分</li> </ul> <p><strong>关键技术bet</strong>：</p> <ul> <li>很早投入long CoT的RL</li> <li>采用新的优化器（Muon）</li> <li>做更大规模的Pre-Training</li> <li>做第一个开源的Agentic模型</li> </ul> <p><strong>决策流程</strong>：</p> <ul> <li>持续思考下一代模型应该什么样</li> <li>看工具箱里有什么新技术可以用</li> <li>通过实验验证技术的有效性</li> <li>数据足够充分时，判断比较显然</li> </ul> <h3 id="53-创业心态与价值观">5.3 创业心态与价值观</h3> <p><strong>核心驱动力</strong>：</p> <ul> <li>“寻找真相的过程，去不断发现新问题、解决它的过程”</li> <li>认为AI很重要，是人类文明的放大器</li> <li>享受攀登无限之山的过程本身</li> </ul> <p><strong>心态管理</strong>：</p> <ul> <li>“做时间的朋友”</li> <li>“不以物喜，不以己悲”，避免情绪化决策</li> <li>关注当前能做什么，而非过度担忧未来</li> </ul> <p><strong>成长感悟</strong>：</p> <ul> <li>最大成长：认识到问题不可避免但可以解决，持续解决新问题是最有意思的</li> <li>在自己的故事里不断感受和思考</li> <li>很多复杂性是人为强加的，实际并没有那么复杂</li> </ul> <p><strong>对成功的理解</strong>：</p> <ul> <li>只要每往上爬，成功概率就会变大</li> <li>会有恐惧，但更重要的是专注当下这一步</li> <li>任何中间状态都可能被批评，但要在投入”不变”的东西和适应调整之间找平衡</li> </ul>]]></content><author><name></name></author><category term="ai-insights"/><category term="AI"/><category term="philosophy"/><category term="LLM"/><category term="moonshot"/><category term="technology"/><summary type="html"><![CDATA[月之暗面创始人杨植麟关于AI发展的深度思考，从技术哲学到实践创新的全面洞察]]></summary></entry><entry><title type="html">Jason Wei：理解2025年AI进展的三种关键思路</title><link href="https://emigmo.github.io/blog/2025/jason-wei-ai-insights/" rel="alternate" type="text/html" title="Jason Wei：理解2025年AI进展的三种关键思路"/><published>2025-01-22T10:00:00+00:00</published><updated>2025-01-22T10:00:00+00:00</updated><id>https://emigmo.github.io/blog/2025/jason-wei-ai-insights</id><content type="html" xml:base="https://emigmo.github.io/blog/2025/jason-wei-ai-insights/"><![CDATA[<h2 id="前言">前言</h2> <blockquote> <p><strong>「所有能被验证的任务，最终都会被 AI 解决。」</strong></p> <p><strong>「智能未来将成为一种商品，未来获取知识或进行某种推理的成本和可及性将趋近于零。」</strong></p> </blockquote> <p>最近，前 OpenAI 核心研究员、CoT（思维链）作者 Jason Wei 在斯坦福大学 AI Club 做了一场精彩的演讲。这也是他加入 Meta 超级智能实验室后少有的公开分享。</p> <p>Jason Wei 提出了三个理解和驾驭 2025 年 AI 发展至关重要的核心思想：<strong>验证者定律</strong>、<strong>智能的锯齿状边缘</strong>和<strong>智能商品化</strong>。</p> <p>某种意义上来说：</p> <ul> <li><strong>验证者定律</strong>决定「哪些点会被率先突破」</li> <li><strong>智能商品化</strong>解释「突破后如何被规模化与降本」</li> <li><strong>锯齿状边缘</strong>则强调「能力突破的时间序与不均衡版图」</li> </ul> <p>虽然没提创业，但似乎又句句不离创业。</p> <p><strong>Jason Wei 背景</strong>：目前在 Meta 超级智能实验室工作。在加入 Meta 之前，是 OpenAI 的核心科学家，参与了 o1 模型和 Deep Research 产品的创建，也是 CoT（Chain of Thought，思维链）的作者之一。</p> <hr/> <h2 id="一智能商品化智能和知识会变得又快又便宜">一、智能商品化：智能和知识会变得又快又便宜</h2> <h3 id="11-ai发展的两个阶段">1.1 AI发展的两个阶段</h3> <p>首先，我们来谈谈「智能商品化」。我认为，AI 的发展可以分为两个阶段：</p> <p><strong>第一阶段：推动前沿</strong></p> <ul> <li>AI 还不能很好地完成某项任务</li> <li>研究人员正在努力解锁这项新能力</li> <li>以 MMLU（大规模多任务语言理解）为例，过去五年的表现曲线显示性能逐渐提升</li> </ul> <p><strong>第二阶段：能力商品化</strong></p> <ul> <li>一旦 AI 掌握了某种能力，它就会被商品化</li> <li>达到特定性能水平所需的成本（以美元计）每年都在下降</li> <li>使用达到特定智能水平的模型成本趋近于零</li> </ul> <h3 id="12-自适应计算的突破">1.2 自适应计算的突破</h3> <blockquote> <p><strong>为什么这种趋势会持续下去？</strong></p> </blockquote> <p>我的观点是，这是深度学习历史上，<strong>自适应计算（adaptive compute）第一次真正奏效</strong>。</p> <p><strong>过去的模式</strong>：</p> <ul> <li>无论任务简单还是困难，用于解决特定问题的计算量都是固定的</li> <li>回答「加利福尼亚州的首府是什么」和解决奥数竞赛题使用相同的计算量</li> </ul> <p><strong>现在的突破</strong>：</p> <ul> <li>进入自适应计算时代，可以根据任务调整所使用的计算量</li> <li>如果任务非常简单，可以将计算成本降到最低</li> <li>不再需要持续扩大模型规模</li> </ul> <p><strong>o1模型的证明</strong>：</p> <ul> <li>一年多前发布的 o1 模型是最初突破</li> <li>在测试阶段为解决数学问题投入更多计算资源</li> <li>模型在基准测试上的表现显著提升</li> </ul> <h3 id="13-信息检索的四个时代">1.3 信息检索的四个时代</h3> <p>AI 商品化还有另外一个方面是，<strong>获取公共信息的时间会越来越短</strong>。我把信息检索分成了四个时代：</p> <table> <thead> <tr> <th>时代</th> <th>示例问题</th> <th>所需时间</th> <th>方法</th> </tr> </thead> <tbody> <tr> <td><strong>前互联网时代</strong></td> <td>1983年釜山的人口</td> <td>几小时</td> <td>开车去图书馆，翻阅百科全书</td> </tr> <tr> <td><strong>互联网时代</strong></td> <td>同上</td> <td>几分钟</td> <td>搜索引擎，浏览网页</td> </tr> <tr> <td><strong>聊天机器人时代</strong></td> <td>同上</td> <td>即时</td> <td>直接询问ChatGPT</td> </tr> <tr> <td><strong>智能Agent时代</strong></td> <td>1983年亚洲最多人口30城市的结婚人数排序</td> <td>几小时→几分钟</td> <td>AI自主搜索、分析、整合</td> </tr> </tbody> </table> <p><strong>复杂查询示例</strong>：</p> <ul> <li>问题：「1983 年釜山有多少人结婚」</li> <li>GPT-3：无法完成</li> <li><strong>OpenAI Operator</strong>：可以做到 <ul> <li>访问韩国统计信息服务（KOSIS）数据库</li> <li>自主点击查找</li> <li>找到正确的数据库查询</li> <li>返回答案</li> </ul> </li> </ul> <h3 id="14-browsecomp基准测试">1.4 BrowseComp基准测试</h3> <p>为了衡量这种能力，OpenAI 创建了 <strong>BrowseComp 基准测试</strong>：</p> <p><strong>特点</strong>：</p> <ul> <li>答案容易验证，但找答案非常耗时</li> <li>示例：「找出符合所有这些限制条件的足球比赛」</li> </ul> <p><strong>人类vs AI表现</strong>：</p> <ul> <li>人类：平均需要两个多小时，很多问题在两小时内无法完成</li> <li><strong>OpenAI Deep Research</strong>：可以解决其中大约一半的问题</li> </ul> <h3 id="15-智能商品化的三大影响">1.5 智能商品化的三大影响</h3> <p><strong>1. 领域民主化</strong></p> <blockquote> <p>那些过去因为知识门槛（比如编程）而受限的领域，会变得更加开放。</p> </blockquote> <ul> <li><strong>编程</strong>：技术门槛大幅降低</li> <li><strong>个人健康</strong>：过去去医生那里说「我想改善我的鼻呼吸」，医生可能只会说「试试我告诉你的方法」。但现在，ChatGPT 几乎能提供一个好医生能给你的所有信息</li> </ul> <p><strong>2. 私有信息价值提升</strong></p> <blockquote> <p>既然公共信息的成本降得这么低，那那些私密的、内部的、不公开的信息，相对价值就会高得多。</p> </blockquote> <ul> <li>非市场挂牌出售的房屋信息更值钱</li> <li>内部数据、专有知识的价值凸显</li> </ul> <p><strong>3. 个性化信息流</strong></p> <blockquote> <p>你访问的不再是人人共享的公共互联网，而是一个为你量身定制的个性化互联网。</p> </blockquote> <ul> <li>获取信息变得无摩擦、毫不费力</li> <li>AI会专门为你展示你想知道的内容</li> </ul> <hr/> <h2 id="二验证者定律训练ai解决任务的能力与任务的可验证性成正比">二、验证者定律：训练AI解决任务的能力与任务的可验证性成正比</h2> <h3 id="21-验证与求解的不对称性">2.1 验证与求解的不对称性</h3> <p><strong>核心概念</strong>：</p> <blockquote> <p>对于某些任务，验证解决方案比找到解决方案要容易得多。</p> </blockquote> <p><strong>典型示例</strong>：</p> <table> <thead> <tr> <th>任务类型</th> <th>生成难度</th> <th>验证难度</th> <th>不对称性</th> </tr> </thead> <tbody> <tr> <td><strong>数独</strong></td> <td>中等</td> <td>容易</td> <td>正向（易验证）</td> </tr> <tr> <td><strong>Twitter代码</strong></td> <td>困难（需几千工程师）</td> <td>容易（刷网页点几下）</td> <td>正向（易验证）</td> </tr> <tr> <td><strong>竞赛数学题</strong></td> <td>困难</td> <td>中等</td> <td>平衡</td> </tr> <tr> <td><strong>数据处理脚本</strong></td> <td>简单（自己写）</td> <td>困难（理解别人代码）</td> <td>反向（难验证）</td> </tr> <tr> <td><strong>事实性文章</strong></td> <td>容易（编造听起来有理的事实）</td> <td>困难（逐条核实）</td> <td>反向（难验证）</td> </tr> <tr> <td><strong>饮食方案</strong></td> <td>容易（随口说出）</td> <td>困难（需长期实验验证）</td> <td>反向（难验证）</td> </tr> </tbody> </table> <h3 id="22-验证者定律的定义">2.2 验证者定律的定义</h3> <blockquote> <p><strong>验证者定律（Verifiers Law）：训练 AI 解决任务的能力，与该任务的可验证性成正比。</strong></p> </blockquote> <p><strong>推论</strong>：</p> <ul> <li>任何<strong>可解决且易于验证</strong>的任务，最终都会被 AI 攻克</li> <li>首先被自动化的任务，将是那些非常容易验证的任务</li> </ul> <h3 id="23-可验证性的五个维度">2.3 可验证性的五个维度</h3> <p>具体来说，我认为「可验证性」体现在以下五个方面：</p> <ol> <li><strong>客观性</strong>：有没有明确的对错标准？</li> <li><strong>验证速度</strong>：检查起来快不快？</li> <li><strong>可批量验证</strong>：能不能一次性检查几百万个方案？</li> <li><strong>低噪音</strong>：验证结果是否稳定可靠？</li> <li><strong>连续反馈</strong>：是只有「对」和「错」两种结果，还是能给出具体的分数，衡量质量的好坏？</li> </ol> <h3 id="24-特权信息改变任务位置">2.4 特权信息改变任务位置</h3> <p>有意思的是，你可以通过提供一些<strong>特权信息（privileged information）</strong>来改变任务在「生成-验证」图上的位置：</p> <p><strong>示例</strong>：</p> <ul> <li><strong>竞赛数学</strong>：如果给你提供答案，检查就变得非常容易</li> <li><strong>编程任务</strong>：如果给你测试用例（如 SWE-bench），检查也变得非常容易</li> </ul> <blockquote> <p><strong>核心思想</strong>：有些任务你可以预先做一些工作，从而增加验证的不对称性。</p> </blockquote> <h3 id="25-ai基准测试的快速攻克">2.5 AI基准测试的快速攻克</h3> <p>大多数 AI 基准测试，从定义上来说，都是易于验证的。</p> <p><strong>历史证明</strong>：</p> <ul> <li>过去五年中关注的所有基准测试都相对快速地被 AI 解决</li> <li>这是验证者定律的最好实例</li> </ul> <h3 id="26-deepmind-alphadev的实践">2.6 DeepMind AlphaDev的实践</h3> <p><strong>AlphaDev项目</strong>是利用验证不对称性的绝佳例子：</p> <p><strong>任务示例</strong>：</p> <blockquote> <p>「找到这 11 个六边形的放置方式，使得围绕它们绘制的最小外围六边形面积最小。」</p> </blockquote> <p><strong>符合五个标准</strong>：</p> <ul> <li>✅ 结果客观（画出来就能验证）</li> <li>✅ 验证速度快且可扩展（计算性的，能批量检查）</li> <li>✅ 噪音低（每次检查结果都一样）</li> <li>✅ 连续反馈（外接六边形的大小直接反映方案优劣）</li> </ul> <p><strong>进化式搜索算法</strong>：</p> <pre><code class="language-mermaid">graph LR
    A[生成Sample] --&gt; B[评估Grade]
    B --&gt; C[迭代Iterate]
    C --&gt; A
    style A fill:#e1f5ff
    style B fill:#ffe1e1
    style C fill:#e1ffe1
</code></pre> <ol> <li><strong>生成（Sample）</strong>：让大语言模型生成大量候选解决方案（代码）</li> <li><strong>评估（Grade）</strong>：任务高度可验证，自动、快速地给每个方案打分</li> <li><strong>迭代（Iterate）</strong>：将得分最高的方案作为「灵感」，反馈给大语言模型</li> </ol> <p><strong>成果</strong>：</p> <ul> <li>通过投入海量计算资源进行循环</li> <li>能够发现比人类专家设计的算法更优的解</li> </ul> <p><strong>聪明之处</strong>：</p> <ul> <li>巧妙地避开了「泛化」问题</li> <li>训练和测试是同一个任务</li> <li>只关心解决这一个具体问题</li> <li>需要挑选那些有可能找到比已知答案更好的答案的问题</li> </ul> <h3 id="27-创业启示">2.7 创业启示</h3> <blockquote> <p><strong>未来一个非常重要的领域（无论是你想创业还是看好它会发展），就是发明衡量事物的方法。</strong></p> </blockquote> <p><strong>机会点</strong>：</p> <ul> <li>如果你能为某个原本难以衡量的领域（比如创造力、用户体验）设计出一套快速、客观、可扩展的评估体系</li> <li>那么接下来就可以利用 AI 来大规模地优化它</li> </ul> <hr/> <h2 id="三智能的锯齿状边缘发展不均衡">三、智能的锯齿状边缘：发展不均衡</h2> <h3 id="31-对ai影响的分歧">3.1 对AI影响的分歧</h3> <blockquote> <p><strong>「AI 的发展会怎么改变我们的世界？」</strong></p> </blockquote> <p>你会发现，不同的人会给出完全不同的答案：</p> <p><strong>量化交易朋友的看法</strong>：</p> <blockquote> <p>「ChatGPT 确实很酷，但它做不了我工作中那些具体的事情。」</p> </blockquote> <p><strong>顶尖实验室AI研究员的看法</strong>：</p> <blockquote> <p>「我们基本上只剩下两到三年的工作时间了，之后 AI 就会取代我们的工作。」</p> </blockquote> <p><strong>Boaz的观点</strong>：</p> <blockquote> <p>「东海岸的人低估了即将到来的变革，他们可能会说’哦，当前的模型做不到这个’，而不太考虑其发展轨迹。而在湾区，可能又会低估把我们训练出来的模型真正落地应用，需要克服多少障碍，耗费多少时间。」</p> </blockquote> <p><strong>Roon的观点</strong>：</p> <blockquote> <p>「现在不应该给出或接受任何职业建议。所有人普遍低估了变革的广度和规模，还有未来职业生涯的巨大不确定性。」</p> </blockquote> <h3 id="32-为什么快速起飞不会发生">3.2 为什么「快速起飞」不会发生</h3> <p>长期以来，有一个假说叫做<strong>「快速起飞」（fast takeoff）</strong>：</p> <p><strong>假说内容</strong>：</p> <ul> <li>一旦 AI 在某个方面超越了人类</li> <li>就会突然变得比人类强大得多</li> <li>在很短的时间内，实现智能的爆炸式增长</li> </ul> <p><strong>我的观点</strong>：这种情景可能不会发生。</p> <p><strong>更现实的场景</strong>：</p> <table> <thead> <tr> <th>时间线</th> <th>AI能力状态</th> </tr> </thead> <tbody> <tr> <td><strong>第一年</strong></td> <td>AI连研究代码库都跑不起来</td> </tr> <tr> <td><strong>第二年</strong></td> <td>AI可以勉强训练一个模型，但效果很差</td> </tr> <tr> <td><strong>第三年</strong></td> <td>AI可以自主训练了，但效果不如顶尖的人类研究团队</td> </tr> <tr> <td><strong>第四年</strong></td> <td>AI训练得很好，但偶尔还需要人类介入来解决疑难杂症</td> </tr> </tbody> </table> <blockquote> <p><strong>这更像是一个自我改进能力的「光谱」，而不是一个二元的选择。</strong></p> </blockquote> <h3 id="33-锯齿状边缘的形成">3.3 锯齿状边缘的形成</h3> <p><strong>核心观点</strong>：</p> <ul> <li>自我改进的速度，应该按「每个具体任务」来考量</li> <li>各种任务就像锯齿状的边缘</li> </ul> <pre><code class="language-mermaid">graph TD
    A[AI能力版图] --&gt; B[高峰：表现出色的领域]
    A --&gt; C[低谷：表现较弱的领域]
    B --&gt; D[复杂数学题]
    B --&gt; E[编程竞赛题]
    C --&gt; F[9.11 vs 9.9比较]
    C --&gt; G[特林吉特语翻译]
    style B fill:#90EE90
    style C fill:#FFB6C1
</code></pre> <p><strong>高峰示例</strong>（AI表现出色）：</p> <ul> <li>复杂的数学题</li> <li>某些编程竞赛题</li> </ul> <p><strong>低谷示例</strong>（AI表现较弱）：</p> <ul> <li>ChatGPT 曾经很长时间都说 9.11 比 9.9 大</li> <li>特林吉特语（Tlingit）翻译：只有几百个美洲原住民才会说的语言</li> </ul> <blockquote> <p><strong>我并不认为我们会看到这样的情况：一个自我改进的模型，突然之间就什么都搞定了。</strong></p> </blockquote> <h3 id="34-预测ai进步速度的三个窍门">3.4 预测AI进步速度的三个窍门</h3> <h4 id="窍门1ai擅长数字任务">窍门1：AI擅长数字任务</h4> <p><strong>原因</strong>：</p> <ul> <li>核心是<strong>迭代速度</strong></li> <li>搞数字任务，扩展计算资源比用真实机器人做实验容易多了</li> </ul> <p><strong>示例</strong>：</p> <ul> <li>「家庭作业机器」漫画（1981年）：对AI工作方式的描绘在今天看来还挺准</li> <li>《我，机器人》那种场景：也许很快会有，但目前还没实现</li> </ul> <h4 id="窍门2对人类来说越容易的任务ai往往也觉得越容易">窍门2：对人类来说越容易的任务，AI往往也觉得越容易</h4> <p><strong>推论</strong>：</p> <ul> <li>可以想象一下人类任务难度的分布</li> <li>AI可能能完成人类因为生理限制而无法完成的任务 <ul> <li>示例：预测乳腺癌（如果能看过1000万张图像）</li> </ul> </li> </ul> <h4 id="窍门3数据越充足ai就越如鱼得水">窍门3：数据越充足，AI就越如鱼得水</h4> <p><strong>实证</strong>：</p> <ul> <li>语言模型在不同语言中的数学表现</li> <li>某个语言的「使用频率」（数据量）和它的表现呈正相关</li> <li><strong>趋势非常明显：数据越多，AI在这个任务上就表现得越好</strong></li> </ul> <p><strong>强化学习的补充</strong>：</p> <blockquote> <p>如果存在一个明确的、单一的客观评估指标，那么你就可以采用 AlphaEvolve 或 AlphaZero 的策略，通过强化学习来生成「假数据」，实现自我训练。</p> </blockquote> <p><strong>Danny Do的推特</strong>：</p> <blockquote> <p>「只要任务提供清晰的评估指标，可以作为训练时的奖励信号，任何基准测试都可以被迅速解决。」</p> </blockquote> <h3 id="35-ai任务时间表预测">3.5 AI任务时间表预测</h3> <table> <thead> <tr> <th>任务</th> <th>人类难度</th> <th>是否数字任务</th> <th>数据充足度</th> <th>预计时间</th> </tr> </thead> <tbody> <tr> <td><strong>翻译（前50种语言）</strong></td> <td>不难</td> <td>✅</td> <td>充足</td> <td>✅ 已完成</td> </tr> <tr> <td><strong>调试基础代码</strong></td> <td>中等</td> <td>✅</td> <td>充足</td> <td>✅ 2023年</td> </tr> <tr> <td><strong>竞赛数学</strong></td> <td>难</td> <td>✅</td> <td>充足</td> <td>✅ 2024年</td> </tr> <tr> <td><strong>AI研究</strong></td> <td>很难</td> <td>✅</td> <td>中等</td> <td>🔮 2027年？</td> </tr> <tr> <td><strong>化学研究</strong></td> <td>难</td> <td>❌</td> <td>中等</td> <td>🔮 比AI研究晚</td> </tr> <tr> <td><strong>拍电影</strong></td> <td>非常难</td> <td>✅</td> <td>充足</td> <td>🔮 2029年？</td> </tr> <tr> <td><strong>预测股市</strong></td> <td>极难</td> <td>✅</td> <td>充足</td> <td>❓ 不确定</td> </tr> <tr> <td><strong>翻译特林吉特语</strong></td> <td>不难（对懂的人）</td> <td>✅</td> <td>极少</td> <td>❌ 可能性极低</td> </tr> <tr> <td><strong>修水管</strong></td> <td>中等</td> <td>❌</td> <td>不确定</td> <td>❓ 不确定</td> </tr> <tr> <td><strong>理发</strong></td> <td>中等</td> <td>❌</td> <td>中等</td> <td>❌ 很难</td> </tr> <tr> <td><strong>手工地毯制作</strong></td> <td>非常难</td> <td>❌</td> <td>极少</td> <td>❌ 短期不可能</td> </tr> <tr> <td><strong>带女朋友约会让她开心</strong></td> <td>😊</td> <td>❌</td> <td>极少</td> <td>❌ 永远搞不定！</td> </tr> </tbody> </table> <blockquote> <p><strong>注</strong>：这些年份都是我随口说的，大家别当真。</p> </blockquote> <h3 id="36-总结与启示">3.6 总结与启示</h3> <p><strong>核心观点</strong>：</p> <ol> <li>不会出现某种快速的超级智能起飞</li> <li>每项任务的能力和改进速度都不同</li> <li>AI影响最大的，是那些符合特定属性的任务： <ul> <li>✅ 数字任务</li> <li>✅ 对人类来说不难</li> <li>✅ 数据丰富</li> </ul> </li> </ol> <p><strong>影响预测</strong>：</p> <pre><code class="language-mermaid">graph LR
    A[AI影响] --&gt; B[极大加速的领域]
    A --&gt; C[保持不变的领域]
    B --&gt; D[软件开发]
    B --&gt; E[数据分析]
    B --&gt; F[内容创作]
    C --&gt; G[理发]
    C --&gt; H[水管维修]
    C --&gt; I[手工艺品]
    style B fill:#90EE90
    style C fill:#FFE4B5
</code></pre> <ul> <li><strong>某些领域将因 AI 而极大地加速</strong>：例如软件开发</li> <li><strong>另一些领域可能会保持不变</strong>：例如理发</li> </ul> <hr/> <h2 id="四结语与思考">四、结语与思考</h2> <p>Jason Wei 的三个核心思想为我们理解2025年AI发展提供了一个完整的框架：</p> <h3 id="核心框架总结">核心框架总结</h3> <pre><code class="language-mermaid">graph TD
    A[AI发展框架] --&gt; B[验证者定律]
    A --&gt; C[智能商品化]
    A --&gt; D[锯齿状边缘]
    B --&gt; E[决定哪些点被率先突破]
    C --&gt; F[解释突破后如何规模化与降本]
    D --&gt; G[强调能力突破的时间序与不均衡]
    style A fill:#FFD700
    style B fill:#87CEEB
    style C fill:#90EE90
    style D fill:#FFB6C1
</code></pre> <h3 id="对创业者的启示">对创业者的启示</h3> <p>虽然演讲没有直接提到创业，但其中蕴含的洞察对创业者极具价值：</p> <ol> <li><strong>选择易验证的问题域</strong> <ul> <li>优先进入那些可以快速验证结果的领域</li> <li>投资于建立评估体系</li> </ul> </li> <li><strong>关注成本降低趋势</strong> <ul> <li>智能商品化意味着私有信息的价值提升</li> <li>专注于获取和利用独特的数据资源</li> </ul> </li> <li><strong>理解不均衡发展</strong> <ul> <li>不要指望AI在所有领域同步突破</li> <li>在数字任务、数据丰富的领域优先布局</li> <li>物理世界的服务仍将是人类的优势领域</li> </ul> </li> </ol> <h3 id="对研究者的启示">对研究者的启示</h3> <ol> <li><strong>关注验证机制设计</strong> <ul> <li>好的验证机制能加速AI在该领域的进步</li> <li>评估体系的设计本身就是重要的研究课题</li> </ul> </li> <li><strong>重视自适应计算</strong> <ul> <li>Test-time compute是重要的研究方向</li> <li>如何根据任务难度动态调整计算资源</li> </ul> </li> <li><strong>数据效率仍然重要</strong> <ul> <li>长尾任务、小语种、特殊领域的数据仍然稀缺</li> <li>如何用有限数据达到好效果仍是关键问题</li> </ul> </li> </ol> <h3 id="最后的思考">最后的思考</h3> <p>Jason Wei 的框架提醒我们：</p> <blockquote> <p><strong>AI的发展不是一场革命，而是一系列不均衡的进化。</strong></p> </blockquote> <p>我们不应该期待某个「奇点」时刻的到来，而应该理解：</p> <ul> <li>哪些任务会被快速解决（易验证、数字化、数据丰富）</li> <li>哪些能力会被商品化（成本趋近于零）</li> <li>哪些领域仍将保持人类优势（物理世界、低数据领域）</li> </ul> <p>这种务实而精准的视角，或许正是我们在AI时代最需要的。</p> <hr/> <p><strong>演讲视频</strong>：<a href="https://www.youtube.com/watch?v=b6Doq2fz81U">Jason Wei at Stanford AI Club</a></p> <p><strong>编译整理</strong>：Founder Park</p>]]></content><author><name></name></author><category term="ai-insights"/><category term="AI"/><category term="LLM"/><category term="reasoning"/><category term="OpenAI"/><category term="Meta"/><category term="CoT"/><summary type="html"><![CDATA[前OpenAI核心研究员、CoT作者Jason Wei在斯坦福的演讲精华：验证者定律、智能商品化与锯齿状边缘]]></summary></entry></feed>