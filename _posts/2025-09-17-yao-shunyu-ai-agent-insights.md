---
layout: post
title: 姚顺雨AI与Agent研究观点集
date: 2025-09-17 10:00:00
description: OpenAI研究员姚顺雨关于AI Agent发展的深度思考与独到见解
tags: AI Agent LLM research
categories: research-insights
toc:
  sidebar: left
---

## 前言

本文整理了OpenAI研究员姚顺雨关于AI与Agent系统的深度思考。作为从清华姚班到普林斯顿PhD，再到加入OpenAI的研究者，姚顺雨在AI发展的关键节点上提出了许多具有前瞻性的观点。他的"下半场"理论、记忆层级理论等思想为我们理解AI发展趋势提供了重要视角。

---

## 个人背景与研究历程

### 学术经历

**标准化的成长路径：**
> "我感觉我是个非常乖的学生。从小到大就是按部就班的学习。本科从合肥考到清华，读姚班。在姚班大家会告诉你去美国读PhD，我就去美国读PhD，我在普林斯顿读PhD。读PhD之后很自然，OpenAI是做research最好的地方，就加入OpenAI——感觉我前28年的人生，非常的乖。"

**时间线：**
- 2015-2019年：清华姚班
- 2019-2024年：普林斯顿大学PhD
- 2024年：加入OpenAI

### 研究转向的关键时刻

**从理论到应用的觉醒：**
> "当时，我觉得很多重要理论问题已经解决得差不多，比如将某个图算法的复杂度从n的2.83次方优化到n的2.82次方，这种改进在现实中意义不大。"

**深度学习的启蒙：**
> "我在2016年上李建老师的一门课，看到一个multi-modal embedding的demo，展示了embedding一个非常神奇的例子：比如用'king'的embedding减去'man'，再加上'woman'，结果接近'queen'的embedding——这让我第一次意识到，深度学习在语义表示上居然能做到这么惊艳的计算。"

**从视觉到语言：**
> "最初我做的是Computer Vision，但渐渐意识到Vision很难实现通用人工智能。我的直觉告诉我：Language是一个更核心、更有潜力的方向，于是读博后转向语言模型研究。"

### 非共识的选择

**早期的非共识：**
> "我一直有这个非共识：我想要去做Agent。"

> "当时做的人很少，因为它太难了，或者不是一个共识类的事情。当时共识类任务是做问答，做翻译，或者做一些已经被社区接受的任务。"

**简单通用的追求：**
> "我一直想做简单且通用的东西。我不想做一个很复杂、但只能在一个领域奏效的东西。这个方向在传统意义上很难被接受，大家习惯了做AI的方式：把问题不停细分，做很多细分方法。"

---

## Agent系统的本质认知

### Agent的定义与演进

**广义定义：**
> "任何能进行自我决策、与环境交互，并试图optimize reward的系统，都可以被称为Agent。"

**现代Agent的特点：**
> "今天我们讲的Agent更多是指：怎么基于语言模型这样的foundation model去做具备自我决策能力的系统，而不是传统意义上基于规则或仅在某个领域用强化学习训练出来的Agent。"

**三波Agent发展：**

1. **第一波：符号主义AI**
   - 基于规则的推理系统
   - 专家系统的尝试
   - 局限：无法覆盖所有情况，导致AI寒冬

2. **第二波：深度强化学习**
   - DeepMind的游戏AI、AlphaGo
   - OpenAI的机器手、Dota
   - 局限：环境特定，无法泛化

3. **第三波：基于语言模型**
   - 具备推理能力
   - 可以进入数字环境（编程、互联网）
   - 核心：方法线+任务线的共同进化

### Language Agent的核心优势

**开放空间决策能力：**
> "如果你要做一个language Agent，你需要的不只是选择能力，而是去自由产生新动作的能力。世界的本质就是，你的行为空间是open-ended的，这种在开放空间决策的能力BERT永远做不到。"

**与传统Agent的区别：**
> "最大区别在于，语言模型提供了一个足够强的先验，这个先验让你可以推理，而推理又可以在不同的环境间泛化。"

### 推理能力是泛化的关键

**人类vs AI的差异：**
> "为什么我可以一下子去玩一个新的游戏，但现在这些系统或AI需要几十万步甚至几百万步训练，才能完成类似的事？我发现，是因为我可以思考。"

**推理的价值：**
> "如果没有这样的思考能力，而是直接从复杂语言去预测'我要往后走'，就很难——没有推理做不到。核心是推理能力，推理才能带来泛化。"

---

## 技术发展的"下半场"理论

### 上半场vs下半场

**转折点的判断：**
> "主线正从'上半场'转向'下半场'。我说的主线是基于语言的智能体。从语言出发，去定义Reasoning、定义Agent，我们终于有了一个非常general的方法，而且这个方法是可泛化的——我们实现了一个基点时刻。"

**本质变化：**
> "这带来一个本质变化：以前我面对很多怪兽，需要造出各种不同武器去打它们；现在我有了一把通用武器，比如机关枪，我不需要再为每个怪兽单独造武器。接下来要思考的问题就变成：我该朝哪个方向开枪？"

### 从训练模型到使用模型

**瓶颈的转移：**
> "大家过去往往更关注模型训练、方法设计，但我觉得现在的bottleneck已经转移了：变成怎么去定义好的任务，怎么去定义好的环境。"

**研究价值的重新定位：**
> "当时最有价值的，就是去研究怎么使用模型。如果你想训练模型，会落后OpenAI或这些公司好几年。你做的很有可能几年前别人已经发现了。如果你想做不一样的，可能怎么去使用模型更有价值。"

### 任务定义的重要性

**环境的重要性：**
> "第二个learning是：任务或环境非常重要。当你有一个非常差的任务，你永远不可能学到非常好的东西。"

**好任务的标准：**
> "首先你要找一个足够有挑战的任务，这个任务能做出本质的新方法。"

---

## OpenAI能力分级体系解读

### L1-L3的递进逻辑

**逻辑关系：**
> "逻辑是，首先你要有语言的先验知识。基于语言的先验知识，最早能做出来的应用是Chatbot（L1）。接下来，基于语言先验，你需要具备推理能力，这是Reasoner（L2）。当你既有语言知识，又具备推理能力，才可能进一步做各种Agent（L3），尤其是能泛化的Agent。"

### L4与L5的正交关系

**并行发展的观点：**
> "我一开始是认为Innovator（L4）和Organization（L5）是更正交或并列的关系。我当时在群里问了一个问题：当一个大公司CEO和一个科学家，到底哪一个难？这个不好说，实现路径有区别。所以，不用太纠结谁是第四级，谁是第五级，都很重要。不一定要先实现哪一个才能实现另一个，可以同时去探索。"

**L4（创新者）的要求：**
1. **Long-Term Memory**：
   > "你作为一个Innovator，首先你需要一个Long-Term Memory。比如，我是Wiles，我研究费马大定理，可能花了20年。我就需要一个长期记忆。"

2. **内在奖励机制**：
   > "我有这个长期记忆还不够，还需要有内在的reward。因为在你真正证明那件事之前，没有任何外部奖励——你没有获奖，没有做成任何'可交付'的事情，也没人给你feedback。你需要自己给自己反馈。"

**L5（组织者）的挑战：**
> "作为一个Organization，你需要解决的问题是：Agent和Agent之间怎么协作？怎么让Multi-Agent协作scale？"

### 实现路径与技术突破

**三个关键方向：**
> "在fundamental research上，比较重要的有三方面：一个是Memory，一个是Intrinsic Reward，还有一个是Multi-Agent。"

**当前水平的定位：**
> "现在的Agent就像一个普通大学生，做一个数字化的实习生。或者说，AGI就是一个普通一本大学生在电脑上能做所有事情的一个能力。"

**人类社会的价值边界：**
> "但是，人类社会的边界是什么？这当然覆盖80%或90%的人。但我们最崇拜的人，是哪两种？一种是创造新东西，在认知或审美上开创新领域的人：爱因斯坦、高更、梵高、贝多芬；另一种是能创造新组织、伟大组织的人：伊隆·马斯克、乔布斯。"

---

## Code作为AI的"手"

### 数字世界的Affordance

**类比人类的手：**
> "Code有点像人的手。它某种程度上，是AI最重要的affordance。对于物理世界，人最重要的affordance是手——我们围绕它制造各种工具，比如锤子、笔、筷子。但对AI、对Digital Agent来说，最重要的affordance可能就是code。"

**天然的机器语言：**
> "因为其他东西，都是给人定义的。比如网页、小说、视频，是为人类设计的；但code是一个天然就给机器使用的表达形式。"

### API vs GUI的争论

**经典辩论：**
> "有个非常经典的debate：最终的AGI，是基于API或code的？还是基于GUI？或者是为人定义的前端环境？还是它是一个混合体？"

**现实的解决方案：**
> "这个问题有点像：你是想改造你的车让它适应所有路，还是改造所有路让它适应现在的车？当然，最终结果很可能是meet in the middle，两边都会做，而且这个事情可能没那么难。"

### 编程环境的特殊价值

**多轮反馈的重要性：**
> "我们当时做了一个工作叫InterCode。大家都在做的是：给一个coding task模型生成一段代码，然后你去evaluate它。但我们就在想：为什么不把执行结果反馈给模型？我们可以让它变成一个多轮Agent task，构造成一个环境，而不是单次完成的任务。"

**非共识的坚持：**
> "有时候，很有意思的一点：一个东西明明非常重要，但就是没人做。如果你是一个研究员，觉得你做的事很重要，但别人不觉得、也没人做，并不是坏事——可能它真的很重要，只是大家还没开始。"

---

## 任务设计与评估哲学

### 基于结果的奖励机制

**设计原则：**
> "我从很早就有一个偏好：我想定义一个基于结果的reward，而不是基于过程的；而且这个reward应该是基于规则、可计算的，而不是来自人的偏好、模型的偏好，或者一些黑盒指标。"

**成功案例的特征：**
> "像math和coding这种任务，之所以能做出来，核心就是：Reward是基于结果，不是基于过程；Reward是白盒的、基于规则的，不是基于人的偏好或模型的偏好。"

**避免Hacking的重要性：**
> "但如果你reward是基于过程，就会出现hacking。你去优化人的偏好、模型的偏好，也会出现hacking。比如你生成一段非常优美的代码，但它并不解决实际问题。"

### Pass@k vs Pass^k

**两种不同的评估需求：**
> "有些任务我们需要优化的是Pass@k（多次尝试中至少成功一次），而另一些任务，比如客服，我们需要优化的是Pass^k（每次都成功），或者我们最关心的是Pass@1（一次就要成功）。"

**任务特性的差异：**
- **创造性任务**：允许多次失败，只要有一次成功
- **可靠性任务**：需要极高的稳定性，每次都要成功

### Robustness的重要性

**被忽视的问题：**
> "现在我们对于简单任务的robustness并没有特别重视——这是因为大家做AI还是在做一些benchmark，而不是实际应用。"

**思维转变的价值：**
> "但如果你接受了这个mindset转变，很自然你就会意识到：有些应用是需要特别强调robustness的，那你就需要去优化它的robustness。现在大家还没完全意识到这件事；但我相信，如果大家意识到这个转变，会带来很大进步。"

---

## 语言与泛化的本质

### 语言作为通用工具

**独特性的来源：**
> "为什么语言非常独特？因为它是人在这个世界完成各种各样事情的工具。语言也是人类发明的工具，像火或笔一样。但它之所以特殊，是因为它是一个帮助你解决任何事情的通用性或泛化性的工具。"

**与其他工具的区别：**
> "当你学会了这门工具，你可以去做很多新任务。比如你学会了攀岩，它帮不了你完成新任务。但你学会了语言，你可以通过语言和人交流，学习、思考、推理。"

**本质认知：**
> "2020年以前，大家没把这个事想清楚，觉得语音、文字、图像、视频都是一些数据，没什么区别。但我觉得最大区别是：语言是人为了实现泛化而发明出来的工具，这一点比其他东西更本质。"

### 强化学习的泛化能力

**历史性突破：**
> "我之所以这么说，是因为在此前，如果你在一个特定环境上训练，模型只能在这个环境表现良好，不能轻易迁移到其他环境。但现在，你在一个环境上训练，模型可以适应更多不同环境，这才是最本质的区别。"

**具体表现：**
> "DeepSeek大家觉得一个有趣结果是：你在数学和编程领域用强化学习训练模型，但它在创意写作上也变得更强。这体现了本质区别：AlphaGo只能下围棋，不能下象棋；而现在你学会数学，也能提高创意写作。"

**泛化的机制：**
> "但我觉得，它还是泛化的。原因是它能够推理。当你能在一个环境学到如何思考的技能，并且这种思考能力能迁移到新环境，这才是泛化的本质原因。"

### 内在激励机制

**创新者的驱动力：**
> "就像我刚刚说的，很多创新者之所以能在没有外在激励的情况下坚持，是因为他有内在的价值观或激励机制。"

**婴儿的好奇心模型：**
> "这个问题，AI和神经科学已经研究多年。婴儿是最典型的例子。他们拥有基于好奇心或自我激励的机制。很多婴儿会反复玩一个玩具，用嘴去咬一个东西，或者做一些看似'无意义'的动作。"

**成长的转变：**
> "当人长大之后，会发生重要变化。当你是婴儿，你对世界的理解，是基于视觉、触觉，基于物理世界的。当你长大之后，你对世界的理解方式变了，变成一个基于语言、推理、文字系统的理解。你玩的，不再是一个物理游戏，而是一个文字游戏。"

**AI面临的挑战：**
> "这是AI面临的挑战：传统AI，比如玩迷宫、做机器人仿真，它可以定义一些基于世界模型或者模仿婴儿阶段好奇心的内在激励。但当AI在玩的是一个语言游戏，要怎么定义内在激励？——这个问题就变得不太一样了。"

---

## 记忆层级理论

### 环境作为最外层记忆

**冯诺依曼的洞察：**
> "前年冬天，我读到冯诺依曼临终前写的一本书，The Computer and the Brain。最让我印象深刻的一句话是：Essentially, the Environment is always the most outer part of the Memory Hierarchy.（基本上，环境永远是记忆层级中最外层的部分。）这很哲学。"

**人类的记忆层级：**
> "对于人，你有你的Memory Hierarchy，有Working Memory、Long-Term Memory在脑子里，但最外层是你的笔记本、Google Doc、Notion，这些是你最外层Memory Hierarchy的一部分。"

**Agent的记忆层级：**
> "某种程度上，是的。从Agent角度看，这个世界有一个Memory Hierarchy。Memory Hierarchy最外层永远是环境。"

### Long Context vs Long-Term Memory

**实现关系：**
> "Long Context是实现Long-Term Memory的一种方式。如果你能实现1亿或1千亿或无限长的Context，它是实现Long-Term Memory的一种方式。它是一种和人区别很大的方式，但这是有可能的。"

**评估的问题：**
> "起码到去年为止，大家主要还在做所谓Long Range Arena，比如needle in the haystack——我有一个很长的输入，我在中间插入一句话，比如'姚顺雨现在在OpenAI'，然后我问你相关问题。这是一个必要但不充分的任务。"

### Context的经济价值

**人类不可替代的原因：**
> "为什么我们现在的模型，推理很强，考试很强，玩游戏很强；但它还没创造出足够经济价值？——根本原因是：它没有这些Context。"

**分布式系统的特点：**
> "人类社会比较tricky的一点是：当然，我们确实写下了很多东西——我们用文字、Google Doc、Notion，记录了很多东西；但很多Context永远只存在人的大脑，是通过一个分布式的系统来维护。"

**解决方案的价值：**
> "如果这个问题解决了，Utility问题就可以在很大程度被解决。这个世界，大多数人并不是乔布斯，也不是爱因斯坦，只是一个普通人。他的数学推理没有o3强，但他能manage Context。"

---

## 创业与产品思考

### 创业公司的机会

**正确的担心：**
> "创业公司应该担心的是模型没有溢出能力，这样你就真的什么都做不了了。有溢出能力是个非常好的事情，这几乎意味着你有机会。"

**最大机会所在：**
> "创业公司最大机会是：能设计不同的interface，或者说人和数字世界交互的方式。"

**成功的条件：**
> "对于创业公司，最好的机会是：你做新的交互方式，并且模型不停有新的溢出能力，让你能够赋能这些新的交互方式——两者缺一不可。"

### 交互方式的创新

**大厂的路径依赖：**
> "但拥有一个Super App对于公司是双刃剑。当你已经有了一个交互方式，你必然形成路径依赖。当你有像ChatGPT这样的Super App，很自然你的研究就会center around这个Super App，会center around这个交互方式。"

**Cursor的价值：**
> "Cursor是很好的例子，创造了一种新的交互。不是像人一样的交互，而是像Copilot。写代码的时候，它能给你提示或编辑。没有人和人是这样交互的。这是它的价值所在。"

**通用性与应用的平衡：**
> "一个比较理想的情况，你有一个非常通用的交互方式，这个交互方式想象力足够大。但并不矛盾的是，你可以有每个阶段的Killer App。"

### 数据飞轮的条件

**成功案例分析：**
> "比较成功的是Midjourney，有非常清晰的reward——人更喜欢哪张图，这个reward和应用是对齐的，reward做得更好，公司就更成功，模型也更好——一切都对齐。有了这种情况，才能自己训练模型，做数据飞轮。"

**必要条件：**
> "如果你要有数据飞轮，首先你要能自己去训模型，并且能通过交互有很好的reward，使你能把好的数据和不好的数据分开。"

**相互借鉴的关系：**
> "这世界是个相互抄的关系，而不是一个单向抄的关系。"

---

## 未来生态构想

### 多元化vs单极化

**多面向系统的观点：**
> "对于不同的任务和交互，需要不同的Agent系统去解决。模型是可以share的，但如果你讨论的是整个系统，那就不一样了。就像你问，这个世界上最强的互联网网站是什么？最强的互联网公司是什么？很难回答。它是一个multiface的系统，有很多不同侧面。"

**避免灰暗的未来：**
> "AI可能也会变成这样的结构。OpenAI可能会成为一个类似Google的公司，成为新世界里非常重要的一环——但这并不代表，这个世界就会被这样一个单极系统垄断。如果真是那样，这个世界就会变得很灰暗。大多数人也就没什么价值了。"

### Agent社会的可能性

**信息差的价值：**
> "为什么这个世界上很多人有价值？不是因为他们的数学或编码能力强，而是因为他们拥有别人没有的信息。中间商本质是拥有信息差。拥有信息差的人会想维护自己的权利和资源。"

**分布式网络的形态：**
> "在交易世界里，信息很重要，每个人只拥有信息的一小部分，这种情况会出现新的不同形态。可能是Multi-Agent，每个人有自己的Agent，Agent之间可以与百万甚至更多人交换信息，达成交易或某些目的。"

### 中心化与去中心化的平衡

**双重趋势：**
> "我最近的一个思考是这样：我感觉人类社会是一个网络，它有两个重要性质：一个性质是中心化程度，也可以说是资源分配的集中性。还有另一个维度，是你从网络边缘到中心的速度或可能性。"

**历史的观察：**
> "过去几百年发生的事情是这样：网络越来越中心化，贫富差距越来越大，二八定律、马太效应更明显；但与此同时，平民或普通人翻身的机会也变多了。"

**技术发展的影响：**
> "看起来，技术发展的趋势是两件事同时加剧——一方面，中心化加剧，因为效率这个因素是根本性的；另一方面，创造新东西的机会，起码到目前为止，是越来越多的。变得更中心化和变得更diverse，可能并不矛盾。"

**力量的平衡：**
> "根本上，现在非常强的巨头和重要节点，有动力继续推动中心化。但在中心化之外的力量，也有动力做一些非中心化的事情。这个世界可能不会是单方压倒另一方，双方都会有自己的力量。"

---

## 人与AI的关系思考

### 效用导向的设计原则

**核心判断标准：**
> "Again，这是一个Utility Problem。很多问题上，人的方式并不一定更有价值。比如下围棋、开车。但有些事情，人就是做得更好。那你就应该思考，怎么去bridge the gap？"

**具体应用场景：**
> "下围棋、打游戏，基于强化学习可以学到和人不一样、甚至更好的方式，就不需要像人。但如果在一个公司打工，和老板搞好关系，完成各种各样的任务，人就是比AI做得更好，就需要更像人。"

**拟人化的条件：**
> "一个事情如果有价值，就会产生。比如，很多人很孤独，他需要一个朋友，技术如果能创造这样的体验，拟人化就是合理存在的未来。"

### 从认知科学到第一性原理

**认知的转变：**
> "我现在觉得，一个更好的方法是：你先去思考人能做什么，而机器现在不能做。这是客观事实。但你找到差异之后，你可以基于第一性原理去思考，如何解决这个问题。你不一定要依赖'人是怎么解决这个问题的'来解决它。"

**借鉴的边界：**
> "所以，从人身上可以借鉴的一点：哪些事情是人可以做，而机器目前不能做？这点比较robust和客观。但至于'人是怎么做到的'，以及'我们在多大程度上要借鉴这种方式'，这个问题本身更主观、也更noisy。"

### 安全与价值的权衡

**现实的优先级：**
> "我会担心。但现在最大问题是——AGI还没实现，我们还没创造足够价值。如果我们还没想清楚，怎么把它变得有价值，就急着把它变得很安全，好像没有意义。"

**商业驱动的安全：**
> "安全是很复杂的问题。比如ChatGPT，如果它不安全，产品就失败了，没有商业价值。即使是为了商业价值，它也会重视安全。"

---

## OpenAI内部视角

### 非共识决策的勇气

**历史的选择：**
> "但问题在于，如果你没有一个different bet，很难超越前面的霸主。如果OpenAI一直做强化学习，可能很难超过DeepMind。即使你在某些任务上做得比它好，人们提到强化学习，想到的还是DeepMind。"

**GPT的赌注：**
> "你要想超越之前的霸主，就必须有一个different bet。而GPT是那个不同的赌注——但这个选择在当时是一个非共识的事情。"

**内部的分歧：**
> "我说实话，当时OpenAI内部绝大多数人也不认为scale-up是最promising的方向，我觉得这是有可能的。"

**领导者的价值：**
> "Ilya最大贡献并不是他做了GPT‑1，或者他具体参与了什么技术工作；而是，他是那个号召大家all in这个方向的人。"

### 强化学习的持续重要性

**历史的连续性：**
> "历史并不是说我把强化学习彻底抛弃，转而走另一条路，再返回来走强化学习，而是更soft的过程。"

**产品化的关键：**
> "后来证明，ChatGPT成功，强化学习也很关键。没有RLHF，没有Alignment技术，它也没办法形成一个产品。"

### 扩展维度的未来

**新的可能性：**
> "会有新的scaling dimension出现。如果你有大量的Memory，你的test-time compute就会有所增加，可以用新的方式scale。如果你有了Multi-Agent，那你的test-time compute又会出现另一个新维度去扩展。"

**复杂的选择：**
> "我觉得会有新的scale dimension出现，但当你有很多scale dimension，怎么去选择？怎么基于某一个应用去分配不同scale维度的比重？——这会是一个很有意思的问题。"

---

## 个人哲学与价值观

### 通用性的追求

**从小的特质：**
> "我从小是一个比较general的人——我想试图变得很通用，试图了解很多不同的学科，做很多不同的事情。"

**认知的升华：**
> "但后来我发现，一个人即使再聪明、再有精力，他能理解的知识或能做的事情，也只是人类社会积累的知识的很小一部分。更好的是，你去创造一个比你更通用、更general的事情。"

**执念的力量：**
> "我好像一直对于通用性，有一种执念或追求。"

### 创造不同的驱动力

**价值的追求：**
> "用一个非常俗的话说，希望你对这个世界创造一些不同——探索新的、根本性的研究，是一种创造不同的方式；创造一种完全不同的新的产品形态，也是一种创造不同的方式。"

**导师的智慧：**
> "我导师令我印象最深的是这样一句话。学术圈经常发生这样的事——你有一个想法，然后别人做了，你会很烦。他说：If someone else can do it, then it's okay to let them do it（如果别人能做，那就让他们去做吧）。"

**全局的思考：**
> "从人类全局的角度，如果这个事情很多人能做，别人做可能是不是也没有什么区别？对这个社会，或者对整体来说，似乎没有什么变化。"

### 时代机遇的把握

**时代的特殊性：**
> "但我觉得恰好是这个时代，你去做上限更高的事情是更好的。因为现在有一个巨大的机会。如果没有这样一个巨大的机会，最佳路径可能是去做incremental、确定性强的事情，一步一步地积累。但恰好有一个上限非常高的事情。"

**勇气的回报：**
> "如果你敢想，或者你胆子特别大，或者你想象力很丰富，就会有好事发生。"

**技术的通用性：**
> "但这个时代很幸运的一点：这个技术非常通用，这个技术非常伟大，有足够多探索的空间。"

**个人的选择：**
> "另一点是，我想让生活更有趣，更有意思，更快乐，就去做一些自己喜欢的事情。这很难用语言解释，就是一个taste或preference的问题。"

---

## 结语

姚顺雨的观点体系体现了一个研究者从技术深度到哲学高度的完整思考。他不仅在技术层面提出了"下半场"理论、记忆层级理论等重要观点，更在价值层面思考了AI发展的多元化可能性。他的非共识选择、对通用性的追求，以及对创造差异化价值的坚持，为AI研究者和创业者提供了重要的思维框架。

他的核心信念可以概括为：**在技术快速发展的时代，要有勇气做非共识的选择，追求简单而通用的解决方案，并始终以创造真实价值为导向。** 这种哲学不仅适用于AI研究，也适用于更广泛的创新活动。

---

*本文整理自姚顺雨的多次访谈和演讲内容，旨在为AI研究者和从业者提供思考框架和启发。*
