<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Andrej Karpathy深度对话：Agent的十年征程与AI的幽灵本质 | Chao Yang </title> <meta name="author" content="Chao Yang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emigmo.github.io/blog/2025/karpathy-agent-ten-years/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chao</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/chinese/">中文简介 </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Andrej Karpathy深度对话：Agent的十年征程与AI的幽灵本质</h1> <p class="post-meta"> Created in October 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/blog"> <i class="fa-solid fa-tag fa-sm"></i> blog</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="andrej-karpathy深度对话agent的十年征程与ai的幽灵本质">Andrej Karpathy深度对话：Agent的十年征程与AI的幽灵本质</h1> <p><em>访谈对象：Andrej Karpathy（OpenAI联合创始人、Eureka Labs创始人）</em><br> <em>主持人：Dwarkesh Patel</em><br> <em>时间：2025年10月</em><br> <em>来源：Dwarkesh Patel播客访谈</em></p> <p><em>图片来源：Dwarkesh Patel</em></p> <hr> <h2 id="前言">前言</h2> <p>Andrej Karpathy是OpenAI联合创始人之一，前特斯拉AI与自动驾驶视觉部门负责人，离开特斯拉后创立AI教育公司Eureka Labs。他于2015年在斯坦福大学获得博士学位，师从被誉为AI教母的李飞飞，主要研究领域为自然语言处理与计算机视觉的交叉领域，以及适用于这一任务的深度学习模型。他在2024年被时代杂志评选为在AI领域最有影响力的百名人物之一。</p> <p>在这次深度访谈中，Karpathy分享了对AI发展的深刻洞察，涵盖大语言模型的工作机制、Agent系统的演化潜力、强化学习与人类学习的本质差异，以及对模型坍缩、数据质量和未来教育的长期思考。</p> <h3 id="核心观点">核心观点</h3> <blockquote> <p><strong>“我们并不是在’造动物’，而是在’造幽灵’。这些系统更像是一种数字化的’灵体’或’意识碎片’，因为我们不是通过进化去训练它们，而是通过模仿人类、学习互联网数据的方式让它们成长。”</strong></p> </blockquote> <blockquote> <p><strong>“未来的研究方向之一，是学会减知识，去除部分信息，只保留一个我称之为’认知核心’的部分。那是一种被剥离了知识、却保留了智能算法与问题解决策略的存在。”</strong></p> </blockquote> <blockquote> <p><strong>“我觉得现在整个行业有点跳太快，把还不成熟的能力包装成革命性突破。我们现在处在一个中间阶段：模型惊人地强大，但依然有很长的路要走。”</strong></p> </blockquote> <blockquote> <p><strong>“我之所以认为’认知核心’至少要达到十亿参数规模，是因为训练数据本身太糟糕了。当你真正打开前沿实验室的预训练语料库，随机抽取一个互联网文档看看，几乎全是垃圾。”</strong></p> </blockquote> <blockquote> <p><strong>“让我担忧的不是’AI接管世界’这种情节，而是一个更现实的场景：我们会逐渐失去理解和控制的能力。因为这些系统会被层层叠加在社会的每个角落，理解其运行机制的人会越来越少。”</strong></p> </blockquote> <blockquote> <p><strong>“我认为在多智能体（multi-agent）领域有两个非常重要的方向，但这两件事目前都还没有被真正实现。第一个是’文化’，第二个是’自我博弈（self-play）’。”</strong></p> </blockquote> <hr> <h2 id="目录">目录</h2> <ul> <li><a href="#%E4%B8%80%E4%BB%8Ealexnet%E5%88%B0llmai%E5%88%9A%E8%B5%B0%E5%88%B0agent%E7%9A%84%E9%97%A8%E5%8F%A3">一、从AlexNet到LLM：AI刚走到Agent的门口</a></li> <li><a href="#%E4%BA%8C%E4%BB%8E%E6%96%91%E9%A9%AC%E5%88%B0%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%94%9F%E7%89%A9%E7%9A%84%E5%A4%A9%E7%94%9F%E6%99%BA%E8%83%BD%E6%9C%BA%E5%99%A8%E7%9A%84%E5%90%8E%E5%A4%A9%E6%A8%A1%E4%BB%BF">二、从斑马到大语言模型：生物的天生智能，机器的后天模仿</a></li> <li><a href="#%E4%B8%89%E5%86%99%E4%BB%A3%E7%A0%81%E5%AE%B9%E6%98%93%E7%90%86%E8%A7%A3%E5%8D%B4%E9%9A%BEai%E7%A6%BB%E6%88%90%E4%B8%BA%E8%83%BD%E6%80%9D%E8%80%83%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%91%98%E8%BF%98%E6%9C%89%E8%B7%9D%E7%A6%BB">三、写代码容易，理解却难：AI离成为能思考的程序员还有距离</a></li> <li><a href="#%E5%9B%9B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%98%AF%E7%94%A8%E5%90%B8%E7%AE%A1%E5%90%AE%E5%90%B8%E7%9B%91%E7%9D%A3%E4%BF%A1%E5%8F%B7">四、强化学习是”用吸管吮吸监督信号”</a></li> <li><a href="#%E4%BA%94%E6%9C%AA%E6%9D%A5%E5%8D%81%E5%B9%B4ai%E4%B8%8D%E4%BC%9A%E5%8F%98%E6%88%90%E4%BA%BA%E8%84%91%E4%BD%86%E4%BC%9A%E9%87%8D%E5%BB%BA%E8%AE%A4%E7%9F%A5%E4%B8%8E%E7%94%9F%E4%BA%A7%E6%9E%B6%E6%9E%84">五、未来十年，AI不会变成”人脑”，但会重建认知与生产架构</a></li> <li><a href="#%E5%85%ADmulti-agent%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5%E4%BA%A7%E7%94%9F%E6%96%87%E5%8C%96%E4%B8%8E%E8%87%AA%E6%88%91%E5%8D%9A%E5%BC%88%E7%9A%84%E8%83%BD%E5%8A%9B">六、Multi-Agent的下一步：产生”文化”与”自我博弈”的能力</a></li> <li><a href="#%E4%B8%83%E8%A2%AB%E4%BD%8E%E4%BC%B0%E7%9A%84%E6%8A%80%E6%9C%AF%E6%89%A9%E5%BC%A0%E9%9A%BE%E7%82%B9%E4%BB%8Edemo%E5%88%B0%E8%A7%84%E6%A8%A1%E5%8C%96%E7%9A%84%E9%9A%BE%E5%BA%A6">七、被低估的技术扩张难点：从Demo到规模化的难度</a></li> <li><a href="#%E5%85%AB%E5%BB%BA%E7%AB%8B%E6%98%9F%E9%99%85%E8%88%B0%E9%98%9F%E5%AD%A6%E9%99%A2%E7%94%A8%E6%95%99%E8%82%B2%E5%AE%88%E4%BD%8F%E4%BA%BA%E7%B1%BB%E5%9C%A8ai%E6%97%B6%E4%BB%A3%E7%9A%84%E4%B8%BB%E5%8A%A8%E6%9D%83">八、建立”星际舰队学院”：用教育守住人类在AI时代的主动权</a></li> </ul> <hr> <h2 id="一从alexnet到llmai刚走到agent的门口">一、从AlexNet到LLM：AI刚走到Agent的门口</h2> <h3 id="11-agent需要十年而不是一年">1.1 Agent需要十年，而不是一年</h3> <p><strong>Dwarkesh Patel：</strong> 今天我邀请到了Andrej Karpathy。Andrej，你曾经说过，”这将是Agent的十年，而不是Agent的一年”。可以解释一下你为什么这么说吗？</p> <p>Andrej Karpathy：首先，非常感谢邀请，我很高兴能来聊这个话题。你刚提到那句“Agent的十年”，其实是我对行业内一个流行说法的回应。当时有些实验室在讨论LLM的发展时提出，“今年是Agent之年”。我听到这个说法时有些警觉，因为我觉得现在行业里存在一定程度的过度乐观。从我的角度来看，更准确的说法是“Agent的十年”。目前确实已经出现了一些令人印象深刻的早期Agent，比如Claude和Codex，我自己每天都在用。但距离它们真正成熟，还有很多工作要做。所以我的想法是，我们需要用十年的时间与这些系统并肩工作，它们会逐步变得更好，也会改变很多事情。这将是一个令人兴奋的过程。但就时间尺度而言，我只是想提醒大家，不该低估这条路的漫长。</p> <p>Dwarkesh Patel：你认为哪些问题需要十年才能真正解决？瓶颈在哪里？</p> <p>Andrej Karpathy： 我觉得，关键在于要让Agent真正“能用”。当我们谈论Agent时，无论是实验室的想法还是我个人的理解，都应该把它想象成一个你雇来一起工作的“员工”或“实习生”。你现在身边的团队成员在做一些具体的工作，那么什么时候你会愿意让Claude或Codex来接手？显然，目前还不行。原因很简单，它们现在还没有足够的智能，也不具备真正的多模态能力，无法熟练地使用电脑，也缺乏持续学习的机制。你无法告诉它们一件事并让它们永远记住。它们在认知层面上还很有限，离一个能真正帮你工作的智能体差得很远。要解决这些问题，大概需要一个十年的周期。</p> <p>Dwarkesh Patel： 有趣。作为一个AI的观察者，我可以指出现在的系统缺少持续学习或多模态能力，但我没法判断这些问题需要多长时间才能解决。比如说，持续学习要花五年？十年？还是五十年？你为什么觉得是“十年”，而不是“一年”或者“五十年”？</p> <p>Andrej Karpathy： 这其实更多来自我的直觉，也来自我在这个领域的经验。我从事AI研究大概十五年，虽然不算特别久，但也足以让我见过各种预测，以及它们最终的结果。我既在学术界做过研究，也在工业界待过，所以对行业节奏的体感比较直观。我的感觉是，这些问题是可以被解决的，但它们仍然非常困难。综合经验来看，十年左右是一个比较合理的时间范围。</p> <p>Dwarkesh Patel： 这让我很好奇。你能不能带我们回顾一下这些年AI的重要转折点？当时人们的情绪是怎样的？他们的预期是过度悲观还是过度乐观？</p> <p>Andrej Karpathy： 这是个很大的问题，因为过去十五年里AI经历了几次真正意义上的“地震”。那种地震般的变化，让整个领域的方向都突然发生了转变。我大概经历过两三次这样的时刻，也相信未来还会继续出现这样的转折，而且往往带着一种令人意外的规律性。</p> <p>我职业生涯的开始是在多伦多大学，碰巧就在Geoffrey Hinton身边。Hinton被认为是AI的奠基者之一，那时他正在训练各种神经网络，我觉得那是件极其有趣的事情，于是开始跟他学习。当时的深度学习其实还不是主流，只是AI边缘的小领域。直到AlexNet的出现，才算是第一个巨大的转折点。它让整个领域重新认识到神经网络的力量，几乎所有人都开始投入到这种方法中。</p> <p>不过那个阶段的研究仍然是“针对单一任务”的，比如图像分类或机器翻译。随后，研究者们开始思考：我们是否已经“搞定了视觉皮层”？如果是，那其他“大脑部分”呢？能不能构建出一个真正能与世界交互的完整智能体？2013年前后的Atari强化学习浪潮，在我看来，就是Agent早期探索的一个代表。那时的目标是让Agent不仅能“感知”，还能“行动”，在环境中获得奖励。当时的实验环境是Atari游戏。回头看，这其实是一条有些偏离的道路。包括我所在的OpenAI在内，当时整个行业的关注点都在强化学习、游戏和对抗任务上。那几年几乎所有人都在研究如何让AI通关游戏。但我始终对这种方向持保留态度，因为我觉得真正通向AGI的路径，不应该是游戏，而应该是现实世界的工作，比如一个能完成知识劳动的虚拟会计。</p> <p>因此，我在OpenAI负责的项目叫Universe，目标是让Agent能够用键盘和鼠标操作网页，在数字世界中完成任务。但事实证明，我们太早了。那时的系统几乎学不到任何东西，因为奖励信号太稀疏，只会在屏幕上乱点，浪费算力却得不到有效反馈。根本问题在于缺乏足够强的表示能力。</p> <p>今天我们看到的新一代会用电脑的Agent，其实正是建立在大语言模型之上的。必须先有语言模型，先拥有丰富的表征，才能在此基础上训练Agent。换句话说，当年我们太早追求全能Agent了，正确的路径是先解决表示学习，再实现智能体的行动能力。</p> <p>回顾过去十五年的AI发展，我觉得可以这样理解：最初大家在训练针对具体任务的神经网络；后来尝试构建早期Agent，但时机尚早；直到大型语言模型的出现，研究者才真正掌握了语言与认知的底层表征，才有了更现实的路径去叠加行动与交互能力。</p> <h2 id="二从斑马到大语言模型生物的天生智能机器的后天模仿">二、从斑马到大语言模型：生物的天生智能，机器的后天模仿</h2> <h3 id="21-进化vs预训练我们在造幽灵而不是造动物">2.1 进化vs预训练：我们在造幽灵，而不是造动物</h3> <p><strong>Dwarkesh Patel：</strong> 有意思。如果换个角度想，人类或动物在成长时是同时学习一切的。它们没有语言作为支撑，却能凭感官去理解世界。那为什么AGI不能像人类或动物那样，从零开始，通过感知去探索世界，而非依赖这种漫长的、上亿样本级的训练过程呢？</p> <p>Andrej Karpathy： 这是个非常好的问题。我也看过你邀请Richard Sutton的那期节目，并在那之后写过一篇短文，谈到我对这一话题的看法。我在文中提到，其实我对把AI类比为动物这件事一直非常谨慎，因为动物的形成过程来自一种完全不同的优化机制。动物是经过进化产生的，它们生来就带有大量“硬件”。我在那篇文章里举过一个例子——斑马。斑马出生后几分钟就能站起来奔跑、跟随母亲。这是极其复杂的行为，不是强化学习（reinforcement learning）能解释的，而是进化在基因层面“预设”的结果。显然，进化以某种方式把神经网络的权重编码进了ATCG（DNA碱基序列）中。我们并不知道这种机制具体如何运作，但它确实起作用。</p> <p>因此我认为，大脑的形成路径与我们正在构建的AI完全不同，我们并没有在运行同样的过程。正因如此，我在那篇文章里写道：我们并不是在“造动物”，而是在“造幽灵”。这些系统更像是一种数字化的“灵体”或“意识碎片”，因为我们不是通过进化去训练它们，而是通过模仿人类、学习互联网数据的方式让它们成长。于是，AI呈现出一种介于模仿与思维之间的幽灵式智能——它是完全数字化的，却又带着某种人类智能的投影。从智能空间的角度看，我们其实是从一个完全不同的起点出发的。</p> <p>当然，我也认为未来可以让这些系统更具“动物性”，这本身就是一个值得探索的方向。Sutton的框架其实可以概括为“我们要造动物”。如果那种方法真的能实现，那将非常了不起。想象一下，一个算法能自我运行并学习万物，那会是极其震撼的成果。只不过我个人对此持怀疑态度，因为我认为这样的算法也许并不存在，或者说，这并不是动物学习的方式。动物有一个外层循环，也就是进化。它在生命周期之外完成了大部分优化。而很多看起来像“学习”的过程，其实更多是大脑的成熟过程。动物真正意义上的强化学习可能只出现在运动控制等领域，而非智力层面的推理与决策。因此，我认为人类其实并不依赖强化学习来获得智能。</p> <p>Dwarkesh Patel： 你能重复一下最后一句吗？你是说很多强化学习其实只是运动相关的任务？</p> <p>Andrej Karpathy： 对，我认为强化学习更多用于类似运动技能的任务，比如投掷圈套、跑步、操控等。而在解决问题、推理等智力活动中，人类并不是通过强化学习来完成的。当然，这并不意味着强化学习不值得研究，只是我觉得它在智能的形成中作用有限。</p> <p>Dwarkesh Patel：让我整理一下，因为这里的信息量很大。我想确认自己的理解是否正确。你似乎在说，进化其实扮演的角色有点类似于预训练”，它建立了一种能理解世界的结构。不同之处在于，进化被“压缩”进人类三GB左右的DNA中，而这与模型的权重完全不同。人类大脑的权重并不存储在精子或卵子中，而是在发育中“长出来”的。DNA里的信息量远远不够描述每一个突触，所以进化更像是找到了学习的算法，而不是直接编码了知识。照你刚才的说法，也许这种生命周期内的学习并不等价于强化学习。这种理解与你的观点一致吗？</p> <p>Andrej Karpathy：我同意这种说法。显然，这个过程中发生了某种“奇迹般的压缩”。神经网络的权重并不直接存储在DNA的ATCG序列里，而是通过某种高度压缩的方式，把学习算法嵌入其中，再在生长过程中逐步展开。</p> <p>但我自己的出发点更务实一些。我并不是想造动物，而是想造有用的东西。我更像一个戴着安全帽的工程师，关心如何让技术落地。我们无法重现进化，因为没人知道怎么做。但我们确实能通过模仿人类和互联网数据，构建出这些幽灵式的数字智能。这种方式虽然粗糙，却能让AI获得某种类似进化所赋予的内在智能和知识结构。所以我常说，预训练其实就是一种“低配版的进化”。是我们现有技术条件下，能实现的、最现实的“演化途径”，用以让AI具备一个能够进一步学习和行动的起点。</p> <p>Dwarkesh Patel：我理解。让我从另一角度重新表达一下，也许能更清晰地对比两种观点。进化并不会直接给予知识，它提供的是寻找知识的算法。而预训练似乎直接传递了知识——这二者并不完全等价。或许可以这么理解：如果预训练帮助模型学会如何更好地学习，那它确实类似进化的过程；但如果只是直接灌输知识，那类比就不成立。你怎么看？</p> <p>Andrej Karpathy： 这是个很细微、但关键的区别。你指出的没错。预训练其实同时在做两件事。一方面，它在积累知识，学习互联网上的文本信息；另一方面，它也在变得聪明。当模型在预测下一个token时，它不仅仅是在记忆知识，而是在观察数据中的算法模式，从而在神经网络内部“点亮”一系列思维与学习的电路，比如上下文学习（in-context learning）等机制。</p> <p>事实上，我们甚至不一定需要那么多“知识”。我认为过多的知识反而可能成为神经网络进一步进化的束缚，因为它让模型过于依赖已有信息，难以跳出数据分布的边界。例如，当前的Agent往往难以在互联网上不存在的语境中进行推理或创造。如果它们的“知识”或“记忆”更少一些，或许反而能表现得更好。</p> <p>所以我认为，未来的研究方向之一，是学会减知识，去除部分信息，只保留一个我称之为“认知核心”的部分。那是一种被剥离了知识、却保留了智能算法与问题解决策略的存在，一个真正体现智能本质的核心。</p> <p>Dwarkesh Patel：这里的信息太有意思了。我们从上下文学习（in-context learning）开始谈起吧。这是一个看似显而易见但非常值得细想的点。模型最像有智能的时刻，往往发生在对话的上下文中。当我与它交谈，它能回应、能思考、能意识到错误并调整表达，这种“活的”智能几乎都是在上下文中显现的。而这种上下文学习能力，其实是通过预训练中的梯度下降自发形成的，也就是说，模型在训练过程中“元学习”出了这种能力。但上下文学习本身并不是梯度下降，就像人类的终身学习虽然受进化影响，却并非进化过程本身。我很好奇，这种类比在你看来成立吗？它会在哪里失效？</p> <p>Andrej Karpathy：我会谨慎地说上下文学习并非完全“不是梯度下降”。它确实不是显式的梯度下降，但我认为它在某种意义上依然在执行某种“微型优化”过程。上下文学习的本质是“模式补全”——在token窗口内完成序列模式。而互联网上存在着海量的模式，因此模型通过梯度下降学会了如何补全这些模式。这种能力被固化在神经网络的权重之中。换句话说，权重捕捉并延展了模式。</p> <p>有趣的是，在神经网络内部，似乎确实存在一种“自适应”机制。它在没有显式训练指令的情况下，自发地调整、适应输入模式。这听起来有些神奇，但它确实源自互联网的复杂数据分布。我看到过几篇研究论文探讨这种机制，其中有一篇让我印象深刻：研究者让模型通过上下文学习完成线性回归任务：输入是连续的XY数对，模型要预测下一个Y。结果显示，模型竟然真的学会了执行线性回归，而线性回归本质上就是反复计算误差并用梯度下降优化权重。</p> <p>更令人惊讶的是，研究者在分析模型权重后发现，其内部计算过程与梯度下降机制高度相似。甚至有研究者直接“硬编码”了一种神经网络结构，使其能通过attention层和内部运算执行梯度下降。换句话说，我们并不确切知道上下文学习在内部发生了什么，但它或许真的在执行某种隐含的、局部的梯度下降。</p> <p>Dwarkesh Patel：那么，如果上下文学习与预训练都在某种意义上实现了梯度下降，为何我们在前者中会强烈地感受到智能的生成，而在后者中却没有这种感觉？两者如果本质相似，区别又是什么？也许关键在于信息密度。以Llama 3为例，它在预训练中接触了约15万亿个token，而一个70B参数的模型，其参数信息量相当于每个token仅被“吸收”0.07比特的信息。相比之下，上下文学习的KV cache（键值缓存）在每增加一个token时会扩展约320KB。这意味着两者在“每个token吸收的信息量”上存在约3500万倍的差异。这是否说明上下文学习在信息层面更接近真实智能？</p> <p>Andrej Karpathy： 我基本同意这种看法。我通常会这样描述：模型在预训练阶段学到的知识，其实只是对训练数据的“模糊回忆”。这是因为压缩比太大了。我们把15万亿个token压缩进区区几十亿个参数中。这种剧烈的压缩必然导致信息丢失，所以我称之为“互联网文档的朦胧记忆”。</p> <p>而在模型运行阶段，也就是上下文学习时，所有输入token被装入上下文窗口，模型实时构建KV cache，这些信息在神经网络中是直接可访问的。因此我更愿意把KV cache类比为人类的工作记忆（working memory）。模型权重里的知识，就像我们对一年前读过的书的模糊印象；而上下文窗口中的信息，则是我们此刻脑海中的短期记忆。这种类比其实相当有启发性。比如，当你让模型总结一本书时，它可能只能凭模糊印象回答，但如果你把整章文字输入上下文，它的回答会立刻变得准确，因为那部分内容此时已被加载进模型的工作记忆。这正是上下文学习让我们感到智能鲜活的原因。</p> <p>Dwarkesh Patel： 回到更宏观的层面，在你看来，人类智能中有哪些核心部分是目前的模型最难以复制的？</p> <p>Andrej Karpathy： 几乎是所有方面。我知道这听起来夸张，但确实如此。我们偶然发现了transformer这种极为强大的神经网络结构，它几乎可以处理任何模态的数据：文字、音频、视频，都能识别模式、生成结构。它的通用性让我觉得，它在某种意义上就像大脑皮层的一部分。大脑皮层以高度可塑性著称。即便改变输入信号来源，神经元也能重新分配功能。比如曾有实验把视觉皮层重定向为听觉皮层，动物依然能适应。这让我认为transformer可能对应于某种“通用皮层组织”。而模型中的“推理与规划”模块，尤其是在构建reasoning traces（推理链）时，更像人脑的前额叶皮层。</p> <p>不过，大脑中还有许多结构我们尚未触及。比如基底神经节可能在我们使用强化学习微调模型时发挥了类似作用；但像负责记忆形成的海马体目前并没有明显对应的机制。小脑（cerebellum）或许与认知关系不大，但杏仁核（amygdala）等掌管情绪与本能反应的部分，在AI中几乎完全缺失。</p> <p>当然，我并不主张我们要照搬人脑结构去造机器。我依然是一名工程师，关注实用。但换个角度讲，如果你真的要“雇用”一个AI作为你的实习生，你会明显感觉到它还不够完整。因为它缺乏许多我们习以为常的认知结构与情感机制。从这个意义上说，我们距离把所有脑区都复刻出来还很远。</p> <p>Dwarkesh Patel：这其实和我们预测AI进展速度密切相关。有人认为，像持续学习（continual learning）这样的能力，未来也会像上下文学习那样自发出现——只要我们设计一个外层循环的强化学习框架，让模型在多个会话中被激励去“记忆”更久的内容，它自然就会发展出长期记忆或外部存储写入机制。你觉得这种假设现实吗？</p> <p>Andrej Karpathy： 我对那种说法其实不太认同。因为我觉得这些模型在每次启动时，当它的上下文窗口里没有任何token，它其实就像“重新醒来”，完全从零开始。因此，在这种视角下，我很难想象“持续学习”到底意味着什么。如果用人类作类比（尽管这类比并不完美，但很有启发性），我会说，当我清醒时，我在不断构建一个白天的上下文窗口，它记录了我一天中遇到的人和事。但当我入睡后，似乎发生了一些神奇的过程。我不认为醒来时还能保留完整的白天记忆，而是其中一部分被蒸馏进了大脑的权重。这就是睡眠的作用：它帮助我们把短期经验提炼成长期记忆。而在大语言模型中，这个阶段，也就是从上下文到权重的蒸馏，是完全缺失的。这正是它们无法实现持续学习（continual learning）的关键原因。</p> <p>人类在睡眠时会分析、反思、甚至在梦境中生成新的信息，从而将经验沉淀进神经权重；而模型目前并没有这样的机制。理论上，我们可以让模型在每次对话结束后，执行一种“知识蒸馏”过程，把当天的交互提炼成新数据，再通过微调（比如LoRA）更新部分稀疏权重。这样每个模型都能像人类一样拥有个性化记忆，而不必从头开始。</p> <p>当然，随着上下文窗口越来越长，我们也在探索新的方法来存储和访问这些记忆。例如，稀疏注意力（sparse attention）结构可以让模型在超长上下文中仍能高效聚焦关键信息。像最近发布的DeepSeek v3.2，就引入了这种稀疏注意力机制，使模型能处理极长的上下文。我觉得，我们正在以完全不同的方式，重新发明许多进化在人类大脑中已经完成的认知技巧。最终，我们或许会在认知结构上收敛到相似的架构。</p> <p>Dwarkesh Patel： 很有意思。那在你看来，十年之后我们仍会使用Transformer吗？只是它的注意力机制更复杂、更稀疏，MLP结构更轻量？</p> <p>Andrej Karpathy： 我喜欢用“时间平移等变性”这个思路来想这个问题。也就是，把今天平移到十年前看看差距。2015年我们主要在用卷积神经网络（CNN），ResNet刚刚出现。那时和今天相比，虽然有连续性，但也已经相当不同了。当时Transformer还不存在，更不用说如今各种结构改进了。所以我猜，十年后我们可能依然在用“大规模神经网络 + 前向传播 + 反向传播 + 梯度下降”这一基本范式，但形式上会有一些变化，规模更大，系统更高效。</p> <p>几年前我还做过一个有趣的实验：重现了Yann LeCun在1989年发表的卷积神经网络。那是已知最早用梯度下降训练的现代神经网络之一，用来识别手写数字。我试着用今天的技术“时间旅行”回去，看看如果只调整算法、数据和算力，会发生什么。结果非常有趣：只要把学习率调小一半，就能立刻让模型更稳定。但要进一步提升性能，我必须加入更多数据、训练更久、引入dropout和正则化。这让我意识到，AI的进步从来不是单一因素推动的，而是算法、数据、算力、系统优化的共同演进。</p> <p>未来十年，我们可能会拥有更好的硬件、更大的数据集、更高效的内核与软件，同时算法本身也会继续优化。这四个维度大概率会齐头并进，没有哪一个会独占驱动力。换句话说，AI的进化仍然是一个多维共振的过程。所以回答你的问题：我认为算法上肯定会有变化，但核心框架依然是“用梯度下降训练的巨大神经网络”。这是我最合理的猜测。</p> <p>Dwarkesh Patel： 想不到这一切进步加起来只让误差减少了一半，居然是30年的努力。</p> <h2 id="三写代码容易理解却难ai离成为能思考的程序员还有距离">三、写代码容易，理解却难：AI离成为能思考的程序员还有距离</h2> <h3 id="31-真正的挑战每一个9都要用同样的代价换来">3.1 真正的挑战：每一个”9”都要用同样的代价换来</h3> <p><strong>Andrej Karpathy：</strong> 减半其实已经很惊人了。但真正让我震惊的是，想让模型更好，几乎所有环节都得同时改进：网络结构、损失函数、正则化策略、数据规模、计算系统，没有一项可以掉队。这种整体提升的模式恐怕还会持续很久。</p> <p>Dwarkesh Patel： 我正想问一个类似的问题，关于你最近的NanoChat项目。因为你刚刚亲自从头编写了整个流程，从预训练到对话模型部署，对所有环节都有最新的体会。这个过程中有没有什么让你意外的发现？</p> <p>Andrej Karpathy： 我前几天刚把它开源发布。它的目标是提供一个最简洁但完整的端到端ChatGPT复刻模板。从数据预处理、模型训练到推理部署，整个流程都在一个仓库里呈现。过去几年我已经写过很多演示代码，分别解释每个环节的算法原理，而NanoChat是把这些零碎的部分串联起来，形成一条完整、可运行的管线。老实说，这个项目对我而言更像是一次系统性整理，而不是新的学习。我本来就知道整个流程该怎么做，只是想让它更干净、易懂，让别人能一眼看清一个聊天模型是如何被构建出来的。如果它能帮助更多人理解ChatGPT背后的工程逻辑，那就达到了我的目的。</p> <p>Dwarkesh Patel： 对于想通过这个项目学习的人来说，最好的方式是什么？是直接把所有代码删掉、自己从头实现一遍，还是在此基础上做一些修改？</p> <p>Andrej Karpathy：这是个好问题。整个NanoChat大概有八千行代码，涵盖了从数据到模型部署的完整流程。如果让我建议学习方式，我会这样做：假如你有两台显示器，把原代码放在右边，然后在左边自己从零开始写。可以参考，但不允许复制粘贴。只有当你亲手敲出每一行代码，才能真正掌握背后的逻辑。不过我也要说，这个仓库本身相当庞大。写这样一套系统的过程不是从头到尾顺序完成的，而是分块逐步搭建的。你在看成品仓库时，其实是看不到这种“构建节奏”的——不知道该从哪块开始、先实现什么。因此，仅仅提供最终代码还不够，理想情况下，还需要补充“构建过程”的讲解，比如视频或文档。我可能会在这周找时间做个视频来展示这个过程。</p> <p>总体来说，我建议大家自己从头构建，但不要复制粘贴。因为我认为知识分为两层：表层的理解和深层的掌握。而只有当你动手搭建时，才会暴露出那些“你以为懂了但其实没懂”的地方。动手是唯一能让你逼近真正理解的方式。正如费曼所说：“如果我不能自己造出来，那我就没有真正理解它。”我一直非常相信这一点。不要只写博客、不要只做PPT，去写代码、调逻辑、让它跑通。否则你只是“以为懂”，并没有真正掌握。</p> <p>Dwarkesh Patel：你在推特上提到，代码生成模型（coding models）在你编写NanoChat时其实帮不上什么忙，这让我很好奇，为什么？</p> <p>Andrej Karpathy： 整个仓库我大概花了一个多月完成。现在人们写代码的方式大致可以分为三类。第一类人完全不用LLM，全手写。我认为这种方式在今天已经不是最优解。第二类人——我自己就是这一类——依然主要手写代码，但会使用模型的自动补全功能。当你写出一部分时，模型会自动联想、填充后续内容，大多数情况下是正确的，少数情况需要修改。这种方式下，程序员依然是“架构师”，模型只是辅助。第三类是纯vibe coding，也就是输入一句指令：“Hi，请帮我实现XXX”，然后按下回车，让模型生成完整代码。这基本就是Agent模式。我认为Agent在某些特定场景下非常有用，比如处理模板化的样板代码（boilerplate code），尤其是那种在互联网上出现频率很高的代码片段，因为模型的训练语料中充满了类似样例。在这种场景下，LLM表现得非常好。</p> <p>但NanoChat完全不是这种情况。它是一个高度定制的项目，结构独特，逻辑密集，不存在可直接复用的模式。几乎每一部分都需要精确组织、细致推敲。这种类型的代码要求模型有极强的上下文理解和结构意识。而当前的代码生成模型在这方面仍然存在明显的认知缺陷。一个常见的问题是，模型会不断误会我在写什么。它记忆中过于丰富，总是试图把我正在写的代码套进它在互联网上学到的典型模板里，但我采用的结构恰恰与主流实现方式不同。它会执着地修正我，让代码回到它熟悉的风格，而这正是我不想要的。</p> <p>图片 图片来源：nanochat GitHub Repository</p> <p>Dwarkesh Patel：能举个具体例子吗？</p> <p>Andrej Karpathy： 比如，在NanoChat的训练过程中，我使用了八张GPU同时进行前向计算。通常情况下，梯度同步是通过PyTorch的分布式数据并行（Distributed Data Parallel, DDP）容器实现的。它在反向传播时会自动进行通信和梯度同步。但我并没有用DDP：不是因为不知道，而是因为没必要。我自己在优化器的step函数中写了一个自定义的同步例程。问题是，代码生成模型总是执着地让我使用DDP容器，它似乎“很担心”我没做同步。但我确实不需要它，因为我已经实现了自己的版本。这就说明一个问题：这些模型没法真正“理解”你的上下文和架构假设，它们只是套用它们所熟悉的模式。</p> <p>Dwarkesh Patel： 所以它们无法理解你其实已经有自己的实现。</p> <p>Andrej Karpathy： 对，完全无法。它们会一直卡在那里，疯狂往代码里加各种“防御性”逻辑，塞满try-catch语句，试图把项目变成一个企业级的生产系统。但我的代码是为教学和实验设计的，我明确知道哪些假设成立，所以这些防御性代码完全没必要。它们还经常误用过时API，让整个项目显得臃肿、混乱、不堪入目。虽然我能手动修复，但这显然没什么效率。</p> <p>另外，我也发现让模型写代码有一个令人烦躁的问题：沟通成本太高。我得用自然语言详细描述我要什么，这比直接在代码里打出几个字母还慢。比如我知道代码该写在哪个位置，只要输入前三个字符，自动补全功能立刻能给出我想要的内容。这种方式的信息带宽更高。指向具体位置，手动触发提示，比整段英文prompt更直接。</p> <p>不过我并不是完全不用模型。它们在某些场景下非常有帮助，比如自动生成报告，这种工作模板化程度高、风险低；我会用部分模型生成的字节码（bytecode）来辅助。而另一个例子是在我用Rust重写tokenizer时。Rust我并不算精通，所以当我有一个已经理解透彻的Python版本时，我会让模型帮我生成Rust实现，再配合测试验证效率。这样既安全又节省时间。换句话说，模型能降低你进入新语言或新范式的门槛。比如Rust的语法我还不够熟悉，但模型在这方面表现不错，就能帮我跨过学习初期的壁垒。</p> <p>Dwarkesh Patel： 我之所以觉得这个问题有意思，是因为很多人关于AI加速到“超级智能”的主流叙事，恰恰建立在“AI能自动改进AI”的假设上。想象一下，如果OpenAI或DeepMind内部有成千上万个像你这样的AI同时优化架构参数、尝试模型变体，岂不是指数级的突破？所以听你说AI其实在写原创代码时表现不佳，这一点对预测“AI爆发”的时间线非常关键。</p> <p>Andrej Karpathy：你说得对，这其实也是我为何认为“十年论”比“明年论”更现实的原因之一。我认为模型目前还不擅长写从未被写过的代码。而AI工程本身的核心任务，恰恰是写这种前所未有的东西。</p> <p>Dwarkesh Patel：听起来有点反直觉。毕竟你在NanoChat中用的结构改动，比如Rope Embedding之类的，其实在论文或开源仓库里都已经存在了。那模型为什么还做不好？</p> <p>Andrej Karpathy：它们“知道”，但只知道一半。模型可能了解这些技术的存在，却不知道如何把它们正确地融入到一个具体的代码库中：在我的风格、我的文件结构、我特定的假设下，实现方式就完全不同。它们无法在上下文中整合知识，也无法判断与现有实现的兼容性。</p> <p>当然，这方面正在不断改进。目前我常用的模型是GPT-5 Pro，它的表现已经非常强大。有时候我会花二十分钟，把整个仓库贴进去请它帮我分析某些问题。和一年前相比，它的回答已经非常令人惊喜。但整体上，距离真正“懂代码”还有明显差距。我觉得现在整个行业有点跳太快，把还不成熟的能力包装成革命性突破。其实很多生成代码的结果质量很一般，甚至可以说是半成品。我理解这背后有融资、市场叙事等因素，但实事求是地讲，我们现在处在一个中间阶段：模型惊人地强大，但依然有很长的路要走。对我来说，目前最理想的使用方式仍然是自动补全，而非全自动Agent。</p> <p>Dwarkesh Patel： 这点非常有意思。从编程史来看，每一次生产力的提升——不论是编译器、静态检查、还是更高层语言——都确实提高了开发效率，但并没有引发“智能爆炸”。这听起来很像你说的auto-complete，而不是“全自动程序员”。</p> <p>Andrej Karpathy： 是的，我也这么认为。我们正在经历的，更像是编译器的进化，而不是程序员的替代。我常常觉得很难明确区分“AI从哪里开始、又在哪里结束”，因为在我看来，AI本质上是计算机技术的自然延伸。如果我们回顾历史，其实从最早的编程工具开始，我们就在不断推动这种递归式的自我增强过程。</p> <p>从最初的代码编辑器、语法高亮、类型检查，到更复杂的调试工具、搜索引擎——这些都在不断提升程序员的效率。严格来说，搜索引擎也算AI吧？排名算法本身就是智能决策。事实上，Google在早期就曾称自己是AI公司，因为他们做的正是智能检索系统。我认为这完全合理。</p> <p>因此，我更倾向把AI视作一条连续谱，而不是一个突然出现的分界点。我们现在确实拥有更好的自动补全工具，也开始出现能自主循环的Agent系统，但这些Agent偶尔也会失控。整个过程其实是人类在逐步从底层实现中抽离，交由机器处理更多细节。比如我们早就不再手写汇编代码，因为编译器可以把C语言自动翻译成汇编。同样地，我们正在把更多能被自动化的部分交给机器。就像一个“自主化滑杆”（autonomy slider），每往右滑一点，机器替我们完成的工作就更多，我们则在抽象层级上不断“升高”。</p> <h2 id="四强化学习是用吸管吮吸监督信号">四、强化学习是”用吸管吮吸监督信号”</h2> <h3 id="41-强化学习的本质局限整条轨迹被一个比特牵着走">4.1 强化学习的本质局限：整条轨迹被一个比特牵着走</h3> <p><strong>Dwarkesh Patel：</strong> 我们来谈谈强化学习吧。你在这方面有一些非常有趣的见解。人类能通过与环境交互，建立起丰富的”世界模型”，而这个过程似乎与最终奖励几乎无关。比如一个人创业十年，最后才知道成败结果，但我们仍说他积累了经验与智慧。这种学习显然比单一的奖励信号要丰富得多。那么，在机器学习中，有没有对应的机制？</p> <p>Andrej Karpathy：我可能会直接说人类根本不是靠强化学习在学习。他们做的是完全不同的事。强化学习的效果其实比很多人想象的要糟糕得多。它之所以存在，只是因为以前的方式更差。以前我们只有模仿学习（imitation learning）。举个简单例子：假设你在做一道数学题。强化学习的方式是，你会在同一个问题上尝试上百种不同的解法：这个不行，换一个，再换一个……直到偶然得出正确答案。然后你翻开书后面的答案，看哪个尝试对了。强化学习的逻辑是：凡是得到正确结果的那几条路径，其所有步骤、所有token，全都被上调权重，意思是多做这样的。问题在于，这是一种非常嘈杂、误导性的信号。你可能在前面犯了很多错，最后才碰巧得出对的答案，但强化学习会把那整条路径都当成“好样本”。哪怕中间九成步骤都是错误的，它也会全部“up-weight”。这是纯粹的噪声。</p> <p>我喜欢用一个比喻来形容：强化学习就像是“用吸管吮吸监督信号”。你花了一分钟让模型执行一整条轨迹（rollout），最后得到一个单一的数字——正确或错误。然后你拿这一个比特的信息，去调整整条轨迹的所有参数。这几乎是荒谬的。人类的学习方式完全不同。人不会去做几百次试验，而是在找到答案后会主动回顾：“哪些地方做得好，哪些地方出错了，下次该怎么改。”这种反思性学习在现有的LLM中完全不存在。不过我看到了一些研究在尝试让模型具备这种能力，这说明整个领域已经意识到这一缺陷。</p> <p>回顾过去，模仿学习的出现本身就是一个奇迹。我们最初只有base model，只是纯粹的自动补全机器。后来出现了InstructGPT这篇论文，它让我大开眼界。作者发现：只要拿预训练模型，进行少量基于对话数据的微调，模型就能迅速“风格化”，变得像在与人对话一样自然，同时保留原有的知识结构。这种能力几乎是瞬间发生的，只经过几轮微调循环，模型就从补全文本变成了用户助手。当时我真的被震撼到了。这标志着LLM第一次学会理解语境并与人交流，而不只是预测下一个词。</p> <p>接着强化学习出现了。RL的确比单纯的模仿学习更进一步，因为它能基于奖励函数（reward function）进行爬坡优化（hill climb），不再完全依赖人类示范。对于有正确答案的问题，RL可以直接通过反馈信号不断改进，甚至找到人类没想到的解法。这是非常了不起的进步。然而，它依旧笨拙。缺乏结构化的反思与抽象机制。最近我看到Google有一篇论文，尝试引入反思与回顾（reflect and review）机制，好像叫Memory Bank之类的方向。这类研究越来越多。我认为我们正处在算法层面的一次重大更新前夜。我们可能还需要三到五次类似的突破，才能让LLM的学习方式更接近人类的认知机制。</p> <p>Dwarkesh Patel：你真的很擅长用生动的比喻。“用吸管吮吸监督信号”这句话太妙了。所以你的意思是，结果导向（outcome-based）的奖励机制最大的问题在于：整个学习轨迹可能非常长，但最终你只靠最后一个比特的信息来更新所有行为的学习信号。既然这个问题如此明显，为什么过程导向监督（process-based supervision）——即在学习过程中持续给出反馈——还没有成为更成功的替代方案？是什么阻碍了这种范式的落地？</p> <p>Andrej Karpathy： 所谓“过程监督”，就是你不是在任务结束后才给出奖励或惩罚，而是在执行的每一步都给予反馈。比如不是等十分钟后才告诉模型做得好或不好，而是每个环节都告诉它当前表现如何。听起来很合理，但真正的问题在于：如何自动化地分配部分奖励。当你有了完整答案时，可以直接做等式匹配——对错分明，非常容易实现。但在过程监督中，你得到的是部分解，系统必须判断这部分做得好不好，而这种判断既复杂又模糊。</p> <p>目前不少实验室尝试用LLM裁判（LLM judge）来解决，也就是让一个大模型来评估另一个模型的中间输出。你给它一个提示，比如：“看看这位学生的中间解答，假设最终答案是X，请评价他目前的表现。”然后通过调prompt，让裁判模型输出分数。听起来不错，但这里有个非常微妙却致命的问题：LLM裁判本身是一个拥有数十亿参数的庞大模型，而这种模型极易被投机取巧欺骗。一旦你基于它进行强化学习优化，模型几乎一定会找到它的对抗样本（adversarial examples），也就是那些能骗过裁判的无意义答案。在短期内——比如10步、20步迭代——这种方法还能有效。但如果你训练100步、1000步，模型就会找到各种漏洞，完全钻空子。</p> <p>举个具体例子，这个实验其实后来被公开过：我们用一个LLM裁判来给学生模型打分。模型训练得非常好，奖励值突然暴涨，看起来几乎“完美解决了所有数学问题”。但当我们去看生成结果时，发现完全是胡言乱语——前几步还像样，后面全变成了“the, the, the, the, the…”。你一看就懵了：这是什么？为什么能拿满分？结果发现，“the, the, the, the, the…”对裁判模型而言，是一个对抗样本。因为这串文字在它的训练数据中从未出现过，它无法判断这是无意义的，反而在“纯泛化空间”（pure generalization land）中给出了极高的置信分数，甚至打出了100%的奖励。</p> <p>Dwarkesh Patel： 换句话说，你其实是在训练一个prompt injection模型。</p> <p>Andrej Karpathy： 还不止那样。Prompt injection听起来太高端了。实际上，这只是模型在产生无意义却能欺骗评估者的输出。这些答案显然是错的，但裁判模型却认为它们“极佳”。这就是强化学习对抗的本质——它总能找到漏洞。</p> <p>Dwarkesh Patel： 如果这是强化学习目前的瓶颈，那要解决它，岂不是必须让LLM裁判变得更强、更鲁棒？我们是不是要像GAN那样生成模型对抗判别器，不断训练出更坚固的评估者？</p> <p>Andrej Karpathy： 是的，我相信各大实验室现在都在这么做。最直观的办法就是——好，那我们把“the, the, the, the, the”加进裁判模型的训练集中，并明确标注为0分，告诉它这不是好答案。这样可以缓解一部分问题。但问题在于对抗样本是无限的。每当你修复一批，下一批又会出现。即便你迭代十次、百次，模型依然能在庞大的参数空间里找到新漏洞。毕竟，这些LLM裁判可能拥有上万亿参数，搜索空间巨大。理论上，通过多次迭代确实会让攻击更难，但我并不确定它能彻底解决问题。我猜实验室们确实在努力做这些改进，但我的直觉是：我们仍然需要新的思路。仅靠修补评估器是不够的。</p> <p>Dwarkesh Patel： 很有意思。那你有没有想过，除了强化学习和过程监督之外，还有哪些新的思路可能突破这一瓶颈？</p> <p>Andrej Karpathy： 有一些方向我觉得值得探索，比如反思式训练（review-based training）——也就是让模型在完成任务后回顾自己的解答，生成新的合成样本（synthetic examples），再通过这些样本进行再训练，从而实现某种“元学习”。最近我看到一些论文开始尝试这个方向，不过目前我只读到摘要阶段。大多数还只是概念，还没有谁能在前沿级大模型的规模上真正让这种方法跑通。现在这些论文很多都还处于想法很酷，但实验结果嘈杂的阶段。当然，各大实验室都相对封闭，谁知道他们内部现在在干什么呢。</p> <p>Dwarkesh Patel： 我能理解用合成数据或合成任务去再训练模型，但人类好像还会做另一件事——比如睡眠或者白日梦。这两者不一定是在制造新问题，而更像是一种反思（reflection）。我在想，机器学习里有没有与“白日梦”或“睡眠”相对应的机制？</p> <p>Andrej Karpathy：我也觉得我们确实缺少这一层的机制。举个例子，当人类读书时，他们并不是在逐词学习，就像LLM那样预测下一个token。人类阅读更像是一种思维触发过程。书籍本身是提示（prompt）：它让你去联想、生成自己的思考，或者去和别人讨论。真正的学习发生在这种信息加工和内化的过程中，而不是对文字的逐句吸收。目前的大模型完全没有这种机制。它们在预训练阶段，只是无意识地预测下一个词。理想状态下，我希望未来的预训练可以加入一个反思阶段。模型在读完一段文本后，能主动思考它与已有知识的关系，进行内在整合，再基于此生成新的理解。这类过程在人类中对应的是睡眠中的记忆重组与抽象，但在模型里尚无对应机制。</p> <p>当然，要实现这点非常困难。比如你可能会问：为什么不能简单地让模型自己生成思考样本再训练？问题在于这些样本虽然看起来合理，但其实会让模型退化。这听起来反直觉，但原因在于模型的输出分布是塌缩的（collapsed distribution）。也就是说，模型生成的样本只覆盖了整个可能思想空间中极小的一部分。看单个输出你可能觉得没问题，但总体上，它的思维分布高度集中、缺乏多样性。举个直观的例子：打开ChatGPT，让它讲个笑话。你会发现，它几乎只会那三五个笑话。它的输出分布早已塌缩，不再包含人类那种丰富的随机性和创造性。而人类正相反：他们的思维有噪声，但正因为有噪声，才保留了巨大的熵（entropy）和发散性。也就是说，人类的思考虽然不完美，却保持了开放的探索空间。因此，如何让模型在进行合成数据生成时，既避免塌缩、又保持高熵多样性，才是真正的研究难题。</p> <p>Dwarkesh Patel：我理解了。也就是说，塌缩的问题会直接影响合成数据的价值，因为我们希望模型能生成超出原有数据分布的新问题或新反思，而不是一遍遍重复自己。</p> <p>Andrej Karpathy： 对，正是如此。比如我让模型读完一本书的一章，然后请它思考一下。它会给出一个看似不错的回答——语言流畅、逻辑清晰。但如果你让它重复十次，你会发现十次几乎一模一样。所以你无法通过反思次数叠加来获得真正的新见解。它并不会像人类那样，在重复的思考中生成新的联系或洞见。换句话说，目前的模型无法在同样的提示下越想越深，只能重复同样的想法。</p> <p>从单个样本看，大模型生成的内容往往看起来没问题，但整体分布却是糟糕的。而且糟糕到什么程度呢？如果你继续让模型在自己的输出上训练太久，它就会发生彻底的坍缩（collapse）。我甚至觉得，这种现象可能没有真正的根本解决方案。更有趣的是，人类其实也会坍缩。这个类比听起来令人惊讶，但我认为它非常贴切。人类在一生中也会逐渐过拟合。孩子之所以让人惊叹，是因为他们还没有过拟合：他们的世界模型尚未被“训练死”。他们会说出一些让成年人震惊的话。听起来不合常理，却又在逻辑上有迹可循。那正是“未坍缩的思维”。而我们成年人，思维早已坍缩。我们不断重复自己，思考的路径越来越窄，学习率越来越低。时间越久，这种坍缩就越严重，直到一切都钝化。</p> <p>Dwarkesh Patel： 我看过一篇非常有趣的论文，它提出梦境其实是防止这种过拟合与坍缩的机制。梦的进化意义在于让我们进入那些极度陌生、非现实的场景，从而避免认知模型对日常经验的过度拟合。</p> <p>Andrej Karpathy：这是个很有意思的观点。确实，当你在脑中生成场景、并对它们进行注意（attend）时，本质上就是在训练自己的合成样本。如果这种自我训练持续太久，人也会跑偏，陷入自己的闭环，最终坍缩。因此，人类必须主动去寻求熵（entropy）。与他人交谈，就是获取外部熵的重要方式。它打破了我们内部的思维惯性。或许，大脑确实在进化中形成了一些增加内部熵的机制，用以打破这种自我坍缩的趋势。梦境，也许正是其中之一。</p> <p>Dwarkesh Patel： 我有个还没完全成型的想法，想请你聊聊。我们已知最好的学习者，也就是孩子，其实在记忆方面能力极差。人生早期的所有经历，他们几乎全部遗忘。但他们却能以惊人的速度学习语言、理解世界。相反，在另一个极端，像LLM这样的模型，在复述事实上无与伦比。它们可以逐字背出维基百科的段落，但在抽象学习与模式迁移方面，却远不如人类儿童。成年人则介于两者之间：他们记忆力较强，却失去了儿童那种灵活泛化的能力。也许这里面隐藏着某种深层机制？</p> <p>Andrej Karpathy： 我完全同意，这确实非常有趣。人类与LLM相比，的确更具“见林不见树”的能力。我们不擅长记忆，反而是一种优势。正因为我们无法逐字记忆，才迫使我们去抓取更高层次的模式。</p> <p>而LLM恰恰相反。它们是极端的记忆机器。它们能精准复现训练语料的文本。你甚至可以喂给它一串完全无意义的随机字符，只训练一两次，它就能原样背出。这是人类绝不可能做到的。但人类无法背诵随机字符串，反而是一种“认知特性”：我们被迫去提炼抽象规律，而不是死记细节。而LLM则被它们的完美记忆所困——这些海量记忆反而成为干扰，使它们难以形成真正的理解。</p> <p>这就是我之前提到的认知核心（cognitive core）概念。我希望未来的模型能减少这类记忆负担，让它们不再存储事实本身，而是只保留思考、推理与实验的算法结构。当模型需要知识时，它可以去查，而不是去背。</p> <p>Dwarkesh Patel： 这似乎也与防止模型坍缩有关？</p> <p>Andrej Karpathy： 我觉得这两个问题其实是不同维度的。记忆与坍缩确实都在影响智能系统的泛化能力，但机制不同。LLM的问题在于，它们太擅长记忆，以至于失去了探索空间。而人类在记忆上笨拙，反而保持了创造力和开放性：这是一个非常好的特性。</p> <p>Dwarkesh Patel： 那模型坍缩有没有什么解决方案？我能想到一些很朴素的思路，比如让模型的分布更宽一点、多样性更高一点之类的。但这些直觉式的方案通常会遇到什么问题？</p> <p>Andrej Karpathy：这是个很好的问题。理论上，你可以为模型加一个“熵正则化项”（entropy regularization），去鼓励输出分布更广、更具多样性。但从经验上看，这些方法效果都不太理想。一个核心原因是：我们目前要求模型执行的任务，本身就不需要太多多样性。这或许是最本质的答案。前沿实验室主要目标是让模型变得实用。从实用角度看，多样性其实反而会带来麻烦：输出更难评估，行为更不可控，价值密度反而下降。所以在很多任务上，输出的多样性被主动压制。</p> <p>Dwarkesh Patel： 对，而且在强化学习里，“过于有创意”其实还会被惩罚，对吧？</p> <p>Andrej Karpathy： 对，比如让LLM帮人写作。如果模型太有创意，往往反而是坏事。因为它会偏离你想要的内容，产出表面多样但本质相似的东西。也就是说，它的回答看起来不同，实际上都来自同一个坍缩的模式。我觉得问题的关键是：当前很多任务不要求多样性，所以模型也就不具备它。但从长远看，这反而在合成数据生成（synthetic generation）阶段成为瓶颈。我们在某种意义上亲手切断了模型的熵来源。我认为实验室应该在这方面做得更积极一些，保留并利用这种“熵”。</p> <p>Dwarkesh Patel：你刚才提到这可能是一个根本性问题，不太容易解决。你能具体解释一下你的直觉吗？</p> <p>Andrej Karpathy：我不确定这是不是根本性问题，也许我刚才说得太重了。我自己没做过大规模实验，但我确实认为，可以尝试用一些方法去提高输出熵，让模型产生更多解答。但问题在于如果你让它太“自由”，模型就会偏离训练分布，开始自创语言或使用极少见的词汇，输出就会变得不可控。换句话说，这是一场分布控制的平衡博弈：既要让模型发散，又不能让它漂移得太远。这件事不算不可能，但确实复杂，绝对不是调个超参就能解决的。</p> <p>Dwarkesh Patel：那如果要你大胆猜一下，你认为智能的最小核心（the optimal cognitive core）应该有多大？假设我们要把它放到一个冯·诺依曼探针（von Neumann probe）里，它需要多少比特？</p> <p>Andrej Karpathy：这个问题很有趣。回顾AI发展的历史，过去一度流行“scale-pilled”思维，也就是一切问题都靠更大的模型解决。我们造出了上万亿参数的模型（trillion-parameter models），但现在又出现了逆趋势：模型开始变小。我认为这些超大模型记忆太多、泛化太少。其实，我曾经提出过一个预测：我们最终可能可以把“认知核心”压缩到十亿参数级（~1 B），就能具备非常强的思考与推理能力。这样的模型可能无法直接回答百科式事实问题，但它知道自己不知道，并会主动去查。这就是智能的真正标志。它不再是“记忆体”，而是“思考体”。</p> <p>Dwarkesh Patel：但这挺出乎意料的。毕竟我们现在已经有十亿参数级的模型，它们看起来就很聪明了。</p> <p>Andrej Karpathy： 是的，但别忘了，目前的主流模型动辄上万亿参数。</p> <p>Dwarkesh Patel： 没错，但那些模型主要是在记忆而非“思考”。而以现在的进展速度，像GPT-OSS 20B这样的新模型已经比原版GPT-4（上万亿参数）更强了。照这个趋势发展，十年后“认知核心”只需要十亿参数？这也太保守了吧。难道不会更小，比如几千万、甚至几百万参数？</p> <p>Andrej Karpathy：不，我之所以认为“认知核心”至少要达到十亿参数规模，是因为训练数据本身太糟糕了。问题的根源在这里。我们今天训练模型所用的语料，是整个互联网。而互联网本身就是一个充满噪声的语料库。你以为的互联网，也许是《华尔街日报》那样的高质量内容，但现实完全不是这样。当你真正打开前沿实验室的预训练语料库，随机抽取一个互联网文档看看，几乎全是垃圾。要么是股票代码，要么是论坛碎片，或者毫无上下文的乱语。像《华尔街日报》这种高质量文本在整个语料中几乎微不足道。</p> <p>所以问题是：我们被迫用极差的数据去训练极大的模型。这些模型的大部分算力，其实都浪费在“压缩垃圾”上。它们的工作更多是记忆性压缩（memory work），而不是认知性学习（cognitive work）。理想情况下，我们希望保留的，是认知部分，而不是记忆部分。因此我们需要智能模型来帮助我们清洗、筛选预训练语料，把它提炼成真正有助于认知学习的数据。这样一来，模型规模自然可以变小。不过这类精炼数据集不会直接用于训练，而是会通过“蒸馏”（distillation）。从更大、更强的模型中提取精华，再用来训练小模型。</p> <p>Dwarkesh Patel：但我好奇的是既然数据都被蒸馏过了，为什么最终的蒸馏模型还要有十亿参数？</p> <p>Andrej Karpathy： 因为蒸馏的过程本身就非常有效。事实上，几乎所有小模型都是蒸馏而来的。没人会直接拿原始互联网去训练一个小模型。</p> <p>Dwarkesh Patel： 我明白。但为什么你觉得十年后，这个蒸馏后的“认知核心”还停留在十亿参数，而不会更小？</p> <p>Andrej Karpathy： 我认为再怎么压缩，智能至少需要“十亿个旋钮”（a billion knobs）才能做出点真正有趣的事。你希望它更小？</p> <h2 id="五未来十年ai不会变成人脑但会重建认知与生产架构">五、未来十年，AI不会变成”人脑”，但会重建认知与生产架构</h2> <h3 id="51-认知核心十亿参数就够了">5.1 认知核心：十亿参数就够了</h3> <p><strong>Dwarkesh Patel：</strong>是啊，从过去几年的趋势来看，模型规模已经在快速下探。我们从上万亿参数降到几十亿、二十亿，而且性能反而提升了。照这个趋势下去，智能核心或许还能继续缩小。就像费曼说的那句话，底部还有充足的空间。</p> <p>Andrej Karpathy： 哈，我本来以为我提出“十亿参数认知核心”已经够反主流了，没想到你还比我更激进。也许你说得对，也许我们还能再小一些。但我仍然觉得模型必须保留一定的知识基线（curriculum of knowledge）。它不需要百科式的细节知识，但也不能什么都去外部查。否则它会陷入认知分页状态。就像你思考时不停查资料，而不是在脑中推演。一个理想的智能体需要一定程度的内部知识缓冲，但不必记得太多“冷门事实”。</p> <p>Dwarkesh Patel： 所以我们刚讨论的，是“认知核心”可能的合理规模。那另一件事是：你怎么看未来前沿模型（frontier models）的总体规模趋势？过去几年，我们经历了从GPT-3到GPT-4.5的快速扩张，现在又出现了缩减或平台期。你认为接下来会怎样？模型还会变大，还是会继续变小？</p> <p>Andrej Karpathy： 我没有特别确定的预测，但我认为实验室们正在变得更务实。他们正在约束预算。事实证明，把算力和成本都砸在预训练阶段，并不是最划算的选择。这就是为什么新一代模型反而更小。预训练阶段变轻了。但他们会在中后期，比如强化学习（RL）或中间训练（mid-training）阶段，把算力“补回来”。换句话说，他们在调整投资结构。不是一味堆参数，而是在不同阶段寻找性价比最优的配置。所以我很难对趋势做精确预测，但我的直觉是人类对“更大模型的渴望不会消失。这几乎是AI发展的宿命。只是我们该怎么“变大”与“变聪明”，未来可能不再是同一件事。</p> <p>Dwarkesh Patel： 你认为接下来几年的进展，会延续过去两到五年的那种节奏吗？比如从NanoGPT到NanoChat之间的那些架构调整（architectural tweaks），是否就代表了未来的创新方向？还是你预期会出现更大的范式转变？</p> <p>Andrej Karpathy：我认为未来最大的变化其实会来自数据集质量的跃升。现在的数据集坦白说——糟糕透顶。糟糕到我甚至都不太明白模型是怎么在这种条件下学会任何东西的。如果你真的去看一个随机的训练样本，会发现里面充满了事实错误、逻辑混乱、无意义的内容。但当你把规模放得足够大时，这些噪声会被平均掉，剩下的信号就浮现出来。这其实是“统计规模的魔法”，而不是数据质量本身的功劳。</p> <p>所以未来数据集会变好很多。同时，硬件也在持续改进。NVIDIA 正在不断优化底层硬件，比如 TensorCores；配套的 kernel 算法也会继续被调优，以最大化算力利用率。算法层面同样会演进，从优化器到架构，再到训练流程中的每个组成部分，都会慢慢变得更高效、更统一。</p> <p>我过去十几年的经验是：没有哪一项单独的突破主导全部进步，而是每个环节都在稳步提升，大约“各项都+20%”，累积起来便是质变。</p> <p>Dwarkesh Patel： 有人提出过用不同方式来刻画我们距离“AGI”的进度，比如用“教育阶段”来比喻：从高中生，到大学本科生，最后再到博士。</p> <p>Andrej Karpathy：我不太喜欢这种说法。</p> <p>Dwarkesh Patel：也有人用“任务时间跨度”（horizon length）来定义：现在的AI能自主完成一分钟的任务，未来能完成一小时、一周、甚至更长期的任务。你认为用什么维度来衡量AI的进步才更合适？</p> <p>Andrej Karpathy： 我有两个回答。首先，我几乎想直接拒绝这个问题本身。因为我一直把AI视为计算机科学的自然延伸。那你要怎么量化计算的进步呢？自上世纪70年代以来，计算机的发展并没有统一的“Y轴”，对吧？从这个角度看，AI进展曲线这个提法本身就有点滑稽。</p> <p>不过，如果我们回到最早的定义。OpenAI刚成立时，我们对AGI的定义是：“一个系统能以人类水平或更高水平完成任何具备经济价值的任务。” 我其实一直坚持这个定义。后来人们陆续简化了它。比如去掉“物理任务”部分，只谈“数字化知识工作”（digital knowledge work）。这是一个重大让步。毕竟原始定义里包括人类能做的一切，比如搬运、操作、感知等。现在的AI显然还做不到。但即便只限定在知识工作，这个市场仍然巨大。我估计知识工作占整个经济体的10%到20%，单在美国，这也意味着数万亿美元的价值空间。</p> <p>Dwarkesh Patel：那按照这个定义，现在AI在多大程度上达到了经济可替代的水平？</p> <p>Andrej Karpathy：这很难直接量化。我们需要更细粒度地看任务而不是职位。因为社会会不断重构职位，将可自动化的部分剥离出来，剩下的由人类完成。举个例子，Hinton 曾预测放射科医生（radiologists）会被取代，结果完全相反。放射科医生的需求反而在增长。虽然计算机视觉技术已经足够强大，但实际工作中还包括病患沟通、报告撰写、跨学科协作等复杂环节，AI远未能胜任。相比之下，呼叫中心客服则是一个更容易被自动化的职业。它具备几个特征：任务简单、重复性高、交互模式固定、完全数字化。一个典型流程是10分钟通话、解决问题、修改数据库。这样的工作非常适合AI接管。</p> <p>但我并不认为AI会直接取代人。更现实的模式是出现一种自主滑块（autonomy slider），AI完成80%的工作量，剩下20%交给人类处理。未来的工作界面可能会变成这样：一个人类监督五个AI助手，管理他们执行重复性任务。企业也会诞生出新的“AI协调层”，专门帮助团队管理这些尚未完全可靠的AI员工。我认为，这种AI分层管理结构将会在未来十年广泛出现在经济的各个行业。毕竟，除了客服，大多数职位的复杂性远超接电话这一层。</p> <p>Dwarkesh Patel：我在想，也许放射科医生的情况可以类比早期自动驾驶的发展。最初自动驾驶上路时，前排必须坐着一个人，以防系统出错时能及时干预。即便是现在，像Waymo的Robotaxi虽然正式商用了，但车内依然有人类监控。我怀疑在放射科领域也可能出现类似情况——即便AI已经能自动化99%的工作，那最后1%依然需要人类来完成，而这 1% 的人类环节反而成为整个系统的瓶颈。</p> <p>更重要的是，如果这个“坐在前排的人”必须经过多年专业训练才能胜任，那他们的薪资理论上应该显著上升。因为他们承担着唯一且关键的不可替代角色。我感觉放射科医生的薪资上涨，部分原因就来自这种“瓶颈效应”：当人类成为最后1%的守门人时，他们的稀缺性被放大了。这种趋势或许也会在呼叫中心或其他岗位的薪资结构中重现：工资先下降90%，然后在最后阶段陡然上升，直到那最后1%被彻底替代。</p> <p>Andrej Karpathy： 这是个挺有趣的假设，但我觉得目前还没在放射科看到这样的趋势。说实话，我一直不太理解为什么Hinton当初点名放射科医生这个职业。因为这个行业本身极其复杂、混乱，远不止图像识别那么简单。相比之下，我更关注呼叫中心员工的情况。因为他们的工作高度重复，自动化潜力非常高。虽然我没有直接的数据，但我猜这一领域目前已经在引入部分AI替换人力。不过我也预计这种替换可能会被部分回调。或许一年或两年后，公司又会重新招聘一些人类员工。</p> <p>Dwarkesh Patel：事实上，有证据表明这种回调已经在发生。一些最早引入AI的公司，后来又重新雇用了人类员工，这其实挺令人意外的。另一个让我惊讶的现象是，按理说，如果我们真在通往AGI的路径上，AI应该能处理所有知识性工作（我们先不谈体力劳动）。那你会以为，这种替代会像逐步抽丝一样进行：先从顾问工作里替掉一小部分任务，再从会计工作中替掉一点点，慢慢地从每个职业里削走一块。但现实完全不是这样。AI的发展路径非常不均衡，几乎所有效率的提升都集中在程序员群体。</p> <p>如果你看大模型公司的收入结构，去掉面向消费者的Chat业务，单看API收入，几乎都是编程相关的应用在主导。这说明一个号称“通用”的系统（general-purpose system），在实际部署中却极度集中地服务于代码生成与软件开发。这真是一个反直觉的现象，AGI的早期形态，居然首先变成了程序员助手。</p> <p>Andrej Karpathy： 这一点其实挺有意思。我确实认为编程是LLM和Agent最完美的首个应用场景。原因在于，编程从一开始就是围绕文本展开的。程序员面对的是命令行和代码编辑器，一切交互都以文本为中心。而大语言模型天生就是在互联网上以文本为食的，它们最擅长的能力就是处理文本。再加上互联网上存在着海量的高质量代码数据，这种供需关系几乎是“天作之合”。</p> <p>除此之外，整个软件生态早已为代码操作构建了完备的基础设施。比如，开发者都有自己的IDE，像Visual Studio Code，而Agent可以直接嵌入进去执行修改。当Agent对代码做出变更时，系统会立刻生成“diff”文件，清晰展示修改的差异。也就是说，我们已经提前为AI自动化准备好了“展示、对比、验证”的可视化框架。</p> <p>但当你把这种模式搬到别的领域，比如幻灯片自动化，情况就完全不同。幻灯片不是文本，而是由空间布局与视觉元素组成的。AI要修改幻灯片，你该怎么呈现“diff”？没有任何通用工具能直观显示修改前后的差异。整个生态基础都不存在。所以结论是：代码是AI的理想宿主，而许多非文本领域目前并不具备这种可对接性。</p> <p>Dwarkesh Patel：但我不确定这能完全解释现象。因为我个人尝试过一些完全属于语言输入 - 语言输出的任务，比如把长访谈稿改写成短片段、根据文本生成视频摘要等等。理论上，这类任务正好是LLM的强项，但实际效果很有限。我们的共同朋友Andy Matuschak（AI科学家）也提到过类似的情况。他尝试了无数方法让模型为间隔重复（spaced repetition）生成学习卡片：这也是典型的“语言进-语言出”任务。他试过上下文示例、监督微调（supervised fine-tuning）、检索增强等五花八门的方式，但模型始终无法生成符合预期的结果。这让我觉得奇怪：即使在纯文本领域，除了编程以外，我们依然很难从LLM中挖掘出显著的经济价值。这背后的原因是什么？</p> <p>Andrej Karpathy：我觉得这确实合理。我并不是说只要是文本就容易处理，代码的结构性远强于自然语言。自然语言往往更“散”，充满模糊性与冗余信息，用统计学的说法，它的熵更高。而代码是高度规范化的，人们一旦理解了语法，就能直接检验输出是否正确。另外，编程本身是一项门槛较高的活动，这使得LLM哪怕只解决了一部分问题，用户也会感受到巨大的赋能。而在文本生成类任务中，人类的创造力基线已经很高，模型的增益显得不那么明显。所以我想说的不是所有文本任务都容易，而是代码刚好落在了LLM能力与人类需求的完美交集点：结构清晰、语料丰富、验证标准客观。也正因如此，编程成为AI最早实现规模化应用的领域。</p> <p>我们早已身处新一代工业革命中——超级智能不是突变，而是惯性 Dwarkesh Patel：你怎么看“超级智能（super intelligence）”？你觉得它会不会和人类或人类社会的智能有本质上的不同？</p> <p>Andrej Karpathy：我倾向于把它看作社会自动化的一种延续。从计算趋势来看，我认为未来会有越来越多的任务被逐步自动化。超级智能只是这个趋势的外推结果。我们会看到越来越多的自主实体（autonomous entities）完成数字化工作，甚至在更久之后扩展到物理层面。总体来说，我认为它仍然是“自动化”的范畴。</p> <p>Dwarkesh Patel：但自动化只是复制人类已经能做的事，而超级智能可能还能做出人类没做过的创新，对吧？</p> <p>Andrej Karpathy：对，不过我觉得“发明新事物”其实也属于自动化过程的一部分，如果这样说得通的话。</p> <p>Dwarkesh Patel：那从更感性的角度看，你觉得未来的文明会不会质上不同？比如超级智能的思考速度更快，能复制无数个自己、甚至融合这些副本，整体比人类聪明太多。这样的文明会不会让人类社会显得完全陌生？</p> <p>Andrej Karpathy：我觉得本质上它还是自动化，但外观上一定会“非常陌生”。就像你说的，它运行在巨大的计算集群上，速度极快、规模庞大。让我担忧的不是“AI接管世界”这种情节，而是一个更现实的场景：我们会逐渐失去理解和控制的能力。因为这些系统会被层层叠加在社会的每个角落，理解其运行机制的人会越来越少。结果就是一切仍在运作，但我们越来越不清楚自己身处的系统是怎么运作的。在我看来，这种“逐步丧失理解与控制”的情境，可能才是最可能发生的未来。</p> <p>Dwarkesh Patel：我想进一步探讨一下。对我来说，“失去控制”和“失去理解”并不是同一回事。比如一家公司的董事会——随便举例，像台积电、Intel——那些董事大多是八十岁的老先生，他们对公司运作其实理解不多，也不一定真正掌控全局。又或者更极端一点，美国总统这个例子，总统确实拥有巨大的权力，但他对系统运作的理解程度与他的控制权完全不同。</p> <p>Andrej Karpathy：对，我觉得这是个合理的反驳。你说得对。我猜我预期的是两者都会同时发生——既失去理解，也失去控制。我们已经在进入一个我自己都很难想象的领域。如果我写一部科幻小说，它的情节可能不是某个单一实体统治世界，而是出现多个逐渐具备自主性的实体，它们相互竞争，有的走向失控，有的被其他AI反制。那种情境更像是一个被完全自治系统填满的“沸腾生态池”，而我们已经把权力和任务都委托给了它们。</p> <p>Dwarkesh Patel：所以并不是因为它们比我们聪明，而导致我们失去了控制。或者说，不一定。真正导致失控的，是这些系统之间的竞争，以及竞争过程中涌现出的结果。</p> <p>Andrej Karpathy：是的，我基本上也这么想。我觉得这些系统中的很多其实仍然是人类使用的工具，只是有部分在代表特定人群或组织行动。所以可能从个体层面看，那些人依然“在控制”，但从整个社会层面看，我们可能已经整体失去了控制，因为最终的结果和社会希望的方向逐渐脱节。这是一种“整体意义上的失控”。</p> <p>Dwarkesh Patel：我们刚才谈到，现在做AI工程或AI研究时，这些模型更像是“编译器”，而不是“替代者”。可一旦真的出现所谓的AGI，它应该能做你在做的事。那如果有一百万个“你”的副本在并行工作，会不会极大加速AI的进步？也就是说，当真正的AGI出现时，我们会不会看到“智能爆炸”？我不是说现在的LLM，而是更远期的情况。</p> <p>Andrej Karpathy：我想会的，但这其实是“常态”。因为我们早已身处一场持续数十年的“智能爆炸”之中。看看GDP曲线，它本质上就是一个跨越各行业、指数加权的增长过程。一切都在逐步自动化，已经持续了几百年。工业革命就是自动化的开始，是物理劳动和工具制造的自动化。编译器是早期的软件自动化。</p> <p>所以我认为，人类早已在递归式地自我加速、自我改进。换个角度看，地球在生物演化前其实是个“无聊的星球”，从太空看几乎没什么变化。而现在我们正处在一场“烟火事件”的中段，只不过它是慢动作的爆炸。我确实认为，这场爆炸早已发生了很久。而AI，只是这场持续爆炸过程中的又一个阶段，并不是某种与过去断裂的新技术。</p> <p>Dwarkesh Patel：你认为这种“超指数增长趋势”会持续下去吗？</p> <p>Andrej Karpathy：这正是让我感兴趣的地方。我之前尝试在GDP数据中寻找AI的痕迹——我以为既然AI出现了，GDP曲线应该上升。但当我去看那些我认为同样“具有颠覆性”的技术，比如计算机、智能手机等，我发现你根本找不到它们在GDP中的突变痕迹。GDP曲线依旧保持着那条指数线。</p> <p>比如早期的iPhone，它刚发布时甚至还没有App Store，没有今天的各种功能。我们回看2008年觉得那是一次“地震级”的技术变革，但从经济数据上看，几乎没有突变。原因在于，这些技术的扩散是缓慢而分散的，最终都会被“平均”进那条稳定的指数增长曲线中。计算机也是如此——当计算机出现时，你并不会在GDP上看到一个“陡然上升的节点”。</p> <p>AI也是一样的。它只是进一步的自动化，让我们能编写一些以往做不到的程序。但归根结底，AI仍然是一种“程序”，是一种新的计算机系统，拥有自己的问题。它的影响也会慢慢渗透开来，最后仍旧汇入那条同样的指数曲线。不过，随着指数继续上升，它会变得越来越“垂直”，我们生活的世界也会显得越来越陌生。</p> <p>Dwarkesh Patel：你的意思是，从工业革命之前到现在，人类经济增长率经历了一个“超指数”的变化——比如几千年前是0%的增长，后来变成0.02%，而今天大约是2%。这就是“超指数”的过程。那你是在说，AI会让这个增长率从2%变成20%甚至200%？还是说，从过去300年的趋势来看，不论技术如何更迭，增长率其实一直稳定在2%，AI也不会改变这个规律？</p> <p>Andrej Karpathy：我认为增长率在过去两三百年确实保持相对稳定。但放在人类历史尺度上看，它的确经历了一次“爆炸式加速”——从几乎0%变成现在的2%。我之前确实试图在GDP曲线里找到AI的影响，但最后我相信这是一个误区。就算人们谈论“递归自我改进”，我也觉得这依旧是“常态”。当然AI会推动自我改进，但人类社会早就在不断自我加速了。比如LLM让工程师更高效地构建下一代LLM，越来越多的环节在被自动化、被优化。所有工程师能用Google搜索、能用IDE、能用自动补全、能在云端编程——这一切本身就是持续的加速。AI只是这条平滑曲线上的又一个环节，而非断点。</p> <p>Dwarkesh Patel：所以，你的意思是，增长率本身不会变化。即使出现所谓的“智能爆炸”，它也只会让我们继续维持在那条2%的增长曲线上，就像互联网帮助我们维持住那条2%的增长一样？</p> <p>Andrej Karpathy：对，我的预期是它仍会遵循同样的模式。</p> <p>Dwarkesh Patel：但如果从相反的角度看，我倾向于认为它可能会“爆炸”。我说的是真正的AGI——不是LLM编程助手，而是能在服务器上取代人类的那种智能体。它与以往的生产力工具不同，因为它本身就是“劳动力”。我们现在生活在一个“劳动力稀缺”的世界里。问任何创业者他们最需要什么，答案往往是“更多优秀的人”。如果突然多出几十亿个会创造、会协作、会自己组建公司的“智能个体”，那和发明一项新技术完全不同。这更像是——地球上突然多出了100亿个聪明人。</p> <p>Andrej Karpathy：我觉得这是个有趣的反驳，我也愿意被说服。但我想指出一点：计算本身就是劳动。计算机早就接管了大量人类的数字信息处理工作，许多岗位因此消失。换句话说，计算机已经是“劳动力”。自动驾驶也是如此——那就是计算机在从事劳动力活动。所以我认为，这种“AI做劳动”的现象其实已经在发生了。这仍然是“常态”。</p> <p>Dwarkesh Patel：但这一次，我们面对的机器可以更快地产生下一代技术——比如它不仅能造出下一辆自动驾驶汽车，还能造出下一个互联网。历史上我们确实经历过从0.2%增长跃升到2%的阶段，所以也许，这次的“机器产出机器”的循环，会让这种加速再度发生。</p> <p>Andrej Karpathy：我明白你的意思。但我觉得人们往往会有一种误解。他们假设，“好吧，我们已经把上帝装进了盒子，现在它什么都能做。”但事实不会是那样。AI会做成一些事情，也会在另一些事情上失败。它会逐步被引入社会，慢慢扩散，最终仍然呈现出同样的模式。这就是我的预测。因为这种假设——认为我们突然拥有一个完全智能、完全灵活、完全通用的人类模型，可以随意调配到社会的各个问题上——我认为那不会发生。这不会是一个“离散跳跃（discrete change）”，而是一个渐进的过程，技术会一点点渗透进各个行业。</p> <p>Dwarkesh Patel：我觉得这种讨论中常见的误导是，人们使用“智能（intelligence）”这个词，会让人误以为“超级智能”意味着某个单一的超级大脑坐在服务器里，像神一样思考所有发明与技术，从而引发一场爆炸式变革。而我想象的并不是那样。我设想的是可能会有数十亿个非常聪明、接近人类思维的“心智”同时存在。真正带来变化的不是某个超级个体，而是数量的巨大。这些“心智”就像数以亿计的高智商移民，他们会自己找到融入经济体系的方式——创业、发明、创造价值，而不需要我们“教他们该怎么做”。我们已经在现实中看到类似的例子：像香港、深圳这样的地区，在几十年里实现了10%以上的年经济增长。原因在于，他们有大量愿意充分利用资源的聪明人，经历了一段“追赶期”。我认为AI可能带来的情景，会非常类似。</p> <p>Andrej Karpathy：我明白你的意思，但我仍然觉得你在预设一种“突变”。就像某个被锁住的能力突然解封，然后我们马上就能在数据中心里拥有成千上万的天才。我认为这种离散式的跳跃在人类历史中几乎没有先例，我在任何数据中都找不到这样的模式，我也不认为它会发生。</p> <p>Dwarkesh Patel：但工业革命不就是一个这样的跳跃吗？我们从接近0%的增长变成了2%的增长。我只是说，也许AI会带来类似的跃迁。</p> <p>Andrej Karpathy：我对这点有点怀疑。我得去查查看，比如也许工业革命之前的经济记录并不精确，数据质量有限。我不排除你说的可能性，也许你是对的。你认为那是一个极其独特、几乎“魔法般”的事件，而现在我们可能会迎来另一次类似的时刻。一个打破既有范式的节点。</p> <p>Dwarkesh Patel：其实我觉得工业革命的关键就在于它并不神奇。你如果拉近看1780年或1870年，并不会发现某个决定性发明。变化是渐进的。但与此同时，经济确实进入了一个全新的增长轨道，增速更快、指数倍增长。我对AI的预期也类似：不会有某个“关键发明”的时刻，而是当积累到一定阈值时，我们突然意识到增长曲线已经彻底改变。</p> <p>Andrej Karpathy：是的，也许可以把它理解为某种“能量释放的临界点”。比如有时候出现一种新能源，或者在AI的情况下，是认知能力（cognitive capacity）被解锁。当这种潜在能力突破临界点时，就会释放出大量尚未完成的“认知工作储备（cognitive work overhang）”。而你认为AI正是填补这些工作缺口的新技术。</p> <p>Dwarkesh Patel：没错。毕竟，经济增长来自人们提出想法、执行想法、创造有价值的成果。而在人类历史的大部分时期，这些过程伴随着人口的快速增长。过去50年里，有人认为全球增长趋于停滞，尤其是发达国家的人口增长放缓。如果我们回顾过去的“超指数增长”，其实是人口和产出的共同增长驱动的。人口的指数增长带来了产出的超指数增长。</p> <p>Andrej Karpathy：我理解这种观点，但直觉上我并不完全认同。</p> <h2 id="六multi-agent的下一步产生文化与自我博弈的能力">六、Multi-Agent的下一步：产生”文化”与”自我博弈”的能力</h2> <h3 id="61-智能的演化一个极其罕见的事件">6.1 智能的演化：一个极其罕见的事件</h3> <p><strong>Dwarkesh Patel：</strong>你之前向我推荐过Nick Lane的书，我也因此去采访了他。我想顺着这个话题问一点关于智能与进化的问题。你做了二十年的AI研究，现在对”智能”是什么、怎样产生智能，应该有了更具体的理解。那你会不会因此更加惊讶进化竟然能”偶然”地演化出智能？</p> <p>Andrej Karpathy：我非常喜欢Nick Lane的书，刚才上来的路上我还在听他的新播客。关于智能及其演化，我确实认为它出现得相当晚——非常非常近期。老实说，我对它能演化出来感到惊讶。我常常会想象：如果宇宙中有上千颗像地球一样的行星，它们会是什么样子？Nick Lane提过，绝大多数星球上可能都会演化出类似的生命形式，比如细菌之类的东西；但从细菌到更复杂生命之间，有几个极难跨越的阶段。</p> <p>我直觉上觉得，“智能”的演化是极其罕见的事件。比如，细菌已经存在了二十多亿年，却没有产生更复杂的智能。真核生物的出现本身就非常艰难，因为细菌在地球演化早期就已经出现。再比如动物——多细胞动物大概存在了几亿年，也许是地球寿命的10%左右。从这个时间尺度上看，智能的出现似乎也不是不可能，但我仍觉得惊讶。直觉上，我原本更期待看到的，是一群在做“动物的事”的动物，而不是能创造文化、积累知识的生命体。这种突变太神奇了。</p> <p>Dwarkesh Patel：如果我们接受这种“突变性”的观点——也就是智能的核心其实是动物智能——那也许只要达到“松鼠”的智能水平，我们就离AGI不远了。松鼠的智能大概出现在寒武纪大爆发后六亿年前，那次变化似乎是由地球的“氧化事件”触发的。氧气一旦足够，真核生物就能形成复杂结构，随之智能算法似乎马上就出现了。这会不会说明：动物智能其实并不复杂，只是一次偶然的演化事件？</p> <p>Andrej Karpathy：是啊，这种事情真的太难判断了。我们或许可以根据“停滞的时长”去估计复杂性。Nick Lane在书里对这一点描述得很好——在细菌和古菌阶段，有长达二十亿年的瓶颈期：化学过程极其多样，却始终没有迈向动物形态。而在动物到智能之间，我们似乎还没看到那种持续数十亿年的“停滞”。</p> <p>另外，也许可以看智能是否曾经多次独立演化。如果这种“算法”在不同物种间多次出现，那说明它可能并非罕见。比如人类的类人猿智能是一种，但鸟类的智能（像乌鸦）也是极高的，而且它们的大脑结构与人类大不相同。这或许是智能多次独立演化的迹象。</p> <p>Dwarkesh Patel：有两位我之前采访过的学者，也就是Guern和Carl Schulman曾提到过一个有趣的观点：他们认为，人类与灵长类拥有的“可扩展智能算法（scalable algorithm）”其实也在鸟类中独立演化过，甚至可能在其他物种中也发生过。区别在于人类找到了一个能够奖励“边际智力提升”的生态位（evolutionary niche），并拥有一种可扩展的大脑结构，使得智力得以持续进化。</p> <p>相反，鸟类的大脑再聪明，也受制于体重和飞行。如果脑太大，它们就飞不起来；它们非常聪明，但生态位不会“奖励”更大的脑容量。海豚也类似，智商高，但受限于环境。而人类不同：我们有双手，能推动工具使用；我们能通过烹饪和社会分工外化消化过程，将更多能量供给大脑，于是这个正反馈循环就启动了。</p> <p>Andrej Karpathy：是的，而且还得有材料可用。比如，如果我是只海豚，那会更困难吧？因为在水里，你没办法点火，也不能做很多事。单从化学角度讲，水下世界能实现的反应和活动范围，比陆地要小得多。我同意刚才你说的“生态位激励”理论。也就是说，不同环境奖励不同的进化方向。但我仍然觉得这件事很神奇。直觉上，我原本更可能预期的是动物们一路往“肌肉更大”的方向进化，而不是走向智能。智能的出现是一个非常奇妙的分叉点。</p> <p>Dwarkesh Patel：我们换种方式说吧：这件事之所以困难，是因为它需要走在一条极其细微的界线上。如果某种能力足够重要，进化就会把它直接“硬编码”进DNA里；如果不够重要，那干脆就不学。智能之所以特别，就在于必须存在一种中间状态——它要让个体在生命周期中学习，但又不能完全预先写死。</p> <p>Andrej Karpathy：没错，你得“激励出”某种可适应性。理想情况下，你希望生物所处的环境高度不可预测，这样进化就没法提前把算法写进权重里。很多动物基本上是“预烘焙的（pre-baked）”，它们一出生，大脑连接就决定了行为。而人类则不同——人类必须在“测试时”（test time）重新学习世界。所以也许，真正促进智能出现的，是那些变化极快、难以预知的环境。因为只有这样，你才“不得不”进化出智能，用以实时解决问题。</p> <p>Dwarkesh Patel：Quentin Pope 有一篇有趣的博客文章，他认为AI不会出现“陡峭爆发（sharp takeoff）”，理由来自人类的历史。人类在6万年前已经拥有现代大脑结构，但直到1万年前才出现农业革命与文明。在那5万年里，人类其实是在慢慢建立“文化支架（cultural scaffold）”，一种能让知识跨世代累积的机制。</p> <p>但在AI训练里，这种机制是“天生存在的”：我们可以反复微调、重训模型，让它们继承彼此的参数或数据语料，而不需要像人类一样从零开始。因此，那种需要几万年才能形成的“文化循环”，对AI来说似乎是自带的。</p> <p>Andrej Karpathy：是，也不是。因为大语言模型其实没有“文化”，至少目前没有。也许我们现在给它们的输入太多了，反而让它们失去了创造文化的动力。但“文化”的核心在于情感、书写、传承，也就是能在个体之间留下记录、积累记忆。而LLM目前并没有这种机制。这实际上是一个限制。</p> <p>Dwarkesh Patel：那你觉得，“LLM文化”可能会是什么样的？</p> <p>Andrej Karpathy：最简单的形式，也许就是一个巨大的可编辑笔记本（scratchpad）。LLM在处理任务或阅读内容时，不断更新这个笔记本，为自己留下记录。为什么LLM不能为其他LLM写书呢？那该多有趣。为什么别的LLM不能读这本书，被启发、被震惊？目前完全没有这种机制——但这正是“文化”的萌芽。</p> <p>Dwarkesh Patel：你认为这种如多智能体系统（multi-agent systems）或AI的“独立文明”与“文化”的现象什么时候会开始出现？</p> <p>Andrej Karpathy：我认为在多智能体（multi-agent）领域有两个非常重要的方向，但这两件事目前都还没有被真正实现。第一个是“文化”——也就是让LLM为自身目的逐步积累知识的能力。第二个，是我认为极具潜力的“自我博弈（self-play）”。</p> <p>在我看来，自我博弈是一个非常强大的机制。进化的本质其实就是不断的竞争，而这种竞争推动了智能的诞生与进步。在算法层面，AlphaGo就是通过和自己对弈，不断提升棋力。但在LLM领域，目前还没有任何真正等价的“自我博弈”机制。我认为这迟早会出现。比如，为什么LLM不能自己生成问题，让另一个LLM去解决？然后这个LLM又尝试制造更难的问题，让对方挑战？</p> <p>这种机制完全可以设计出多种形式，这是一个值得深入研究的领域。但截至目前，还没有看到令人信服的成果，能够真正实现这两种“多智能体的协同进化”机制。我们仍然停留在“单一智能体”的阶段。文化也是类似的情况——我们甚至还没看到AI之间的“组织”或“群体结构”真正形成。正因如此，我认为我们仍处在非常早期的阶段。</p> <p>Dwarkesh Patel：那你觉得，目前阻碍LLM之间这种协作的核心瓶颈是什么？</p> <p>Andrej Karpathy：比如，一些小模型或“更笨”的模型，它们与人类认知阶段有奇妙的对应关系：像幼儿园、小学、初中、高中。但现在的模型总体上还没“毕业”。无论是我的Claude Code还是Codex，它们虽然能做博士级别的测验题，但在认知层面，仍然更像是小学甚至幼儿园的学生。它们是“天才儿童”——记忆力完美，能生成看起来漂亮的内容，但并不真正理解自己在做什么。它们还没形成跨领域的认知协调能力。正因为如此，我不认为它们现在具备创造“文化”的条件。它们还太稚嫩。</p> <h2 id="七被低估的技术扩张难点从demo到规模化的难度">七、被低估的技术扩张难点：从Demo到规模化的难度</h2> <h3 id="71-自动驾驶的教训每一个9都要用同样的代价换来">7.1 自动驾驶的教训：每一个”9”都要用同样的代价换来</h3> <p><strong>Dwarkesh Patel：</strong>你曾在特斯拉负责自动驾驶项目，从2017到2022年，亲眼见证了这项技术从”炫酷演示”到”数千辆车在路上自主驾驶”的转变。为什么这件事花了十年？这中间到底发生了什么？</p> <p>Andrej Karpathy：首先我要强调——这项技术现在还远远没完成。自动驾驶对我来说是非常重要的经验，因为我在这个领域工作了五年，这让我对AI发展的节奏形成了很强的直觉。事实上，自动驾驶的早期演示可以追溯到上世纪80年代。卡内基梅隆大学在1986年就展示过一辆能自己在路上行驶的卡车。</p> <p>我2014年左右加入特斯拉前，朋友带我看过Waymo的早期演示，那次驾驶几乎完美。当时我以为量产就在眼前，但结果又花了十年。这揭示了一个规律：有些任务存在巨大的“从demo到实际产品的落差”。做出一个演示很容易，但要成为真正可靠的产品则极其困难。尤其像自动驾驶这种，一旦失败，代价太高。</p> <p>在很多行业中，失败的代价并不大，但在这些高风险领域，开发周期会被极大拉长。比如软件工程也是类似的例子——vibe coding当然没问题，但若是要写生产级代码，哪怕一个小漏洞也可能造成安全问题或隐私泄露，后果灾难性。</p> <p>所以，我认为软件和自动驾驶共享同一个特征：高风险、高要求、零容错。更具体地说，这个过程是一场“九的征程（march of nines）”。每多提升一个“9”的可靠率，都需要同样的工作量。当系统达到90%的可靠性，那只是第一个“9”；接下来要95%、99%、99.9%、99.99%……每一步都同样艰难。在我在特斯拉的五年里，我们大概提升了两到三个“9”，但仍有更多要攻克。这就是为什么这样的项目要花十年甚至更久。</p> <p>也正因如此，我对demo从来毫不印象深刻。任何AI demo在我看来都只是起点。无论它多么惊艳，都还没触碰到真正的产品难题。真正的挑战，是当技术接触到现实世界后，如何在成千上万种异常情况下依然安全稳定地运行。自动驾驶教会了我一点：每一个“9”都要用同样的代价换来。AI的未来也是这样——demo令人振奋，但离成熟仍有漫长的路。</p> <p>Dwarkesh Patel：你刚才提到软件系统在安全保障方面的要求，和自动驾驶其实非常相似，这一点很有意思。人们常说自动驾驶之所以花这么久，是因为失败的代价太高。比如，一个人类司机平均每开40万英里或大约七年才出一次严重错误。如果你要发布一个“不能在七年内犯错”的代码智能体，那部署难度肯定巨大。但你的意思似乎是，如果一个编程Agent每七年就犯一次灾难性的错误，比如让关键系统崩溃，那其实也是不可接受的？</p> <p>Andrej Karpathy：是的，而且非常容易发生。</p> <p>Dwarkesh Patel：实际上，从“时间”角度看，这个周期甚至比七年更短。因为AI在不断地输出代码，相当于每个token都在冒风险。换算到时钟时间上，出错频率其实要高得多。</p> <p>Andrej Karpathy：对，从某种意义上说，这其实是一个更难的问题。自动驾驶只是人类成千上万种行为中的一种，是一个垂直领域。而软件工程则复杂得多，表面积更广，问题更多。</p> <p>Dwarkesh Patel：不过有人提出一个反驳：自动驾驶花很长时间的一个主要原因，是它必须先解决“基础感知”的问题——也就是视觉理解、物体识别、空间推理，以及当环境稍有变化时，如何具备“常识性泛化”。比如，当路边有人挥手时，系统能理解那意味着“停车”，而不需要针对这个动作单独训练。但现在我们拥有LLM或VLM（视觉语言模型），这些基础表征问题已经“被免费解决”了。所以从理论上讲，把AI部署到不同领域，就像让自动驾驶汽车去另一座城市运行——困难，但不需要再花十年。</p> <p>Andrej Karpathy：我不太确定我完全同意这种说法。我不确定我们到底“免费获得”了多少。我认为我们确实拥有了更通用的智能表征能力，但仍存在巨大的理解空缺。自动驾驶虽然是专用任务，但从另一个角度看，构建“专用智能”也有自己的难度，因为它不是从通用智能中自然涌现的产物，需要从零构建。因此，类比虽然有一定合理性，但我觉得并不完全贴切。LLM确实更“通用”，但它们依然充满漏洞与认知缺口，我们还没有真正获得“开箱即用的泛化”。</p> <p>另外，我想回到最开始提到的那点，那就是自动驾驶其实还远未完成。虽然我们已经能看到Waymo之类的车辆在路上运行，但规模依然很小。根本原因在于它们还不具备经济可行性。这些系统是“未来的产物”，现在运行的每一辆车都伴随着高昂的维护与资本开销。要让自动驾驶真正“划算”，仍需要长期的努力。</p> <p>还有一点，人们看到“无人驾驶”车辆时，往往会被表象误导。事实上，很多自动驾驶车背后都有庞大的远程操作中心，里面有人类操作员在“闭环干预”。我不知道具体程度，但可以肯定：人类仍在回路中，只是被“转移到看不见的地方”。在某种意义上，我们并没有移除人类，而是把人类藏了起来。</p> <p>因此，我仍然认为，自动驾驶要从一个环境推广到另一个环境，仍面临大量挑战。虽然现在的系统在体验上“看起来真实”，但背后仍有许多辅助。比如，Waymo的车辆其实不能行驶在所有城市区域——信号差的地方可能根本不行驶。当然，这些只是我的推测，我不了解他们的完整技术架构。</p> <p>图片 图片来源：Bloomberg</p> <p>Dwarkesh Patel：不过你毕竟在特斯拉带领自动驾驶团队干了五年。</p> <p>Andrej Karpathy：是的，不过我要澄清一下，我并不了解Waymo的具体实现。我其实非常喜欢Waymo，也经常乘坐它。我想表达的只是：人们对AI进展往往过于天真。我认为距离真正的成熟仍有很长的路。我更看好特斯拉的方向，因为他们采取了更具可扩展性的策略，我觉得团队做得非常出色，也更贴近我对未来的判断。特斯拉的系统依赖更通用的传感与学习，而不是为每个场景定制规则，这样的路线更有生命力。所以，我其实不太认同“自动驾驶花了十年才完成”这种说法——因为它还没完成。真正的自动驾驶革命，还没真正发生。</p> <p>Dwarkesh Patel：确实，从这个角度看，自动驾驶的起点应该是上世纪80年代，而不是十年前；而且结局还远没有到来。</p> <p>Andrej Karpathy：对，我同意。因为当我们谈论“自动驾驶”时，我脑海中的定义是规模化的自动驾驶，也就是普通人不需要再考驾照的那一天。离那个时刻，我们还有很长的路要走。</p> <p>Dwarkesh Patel：我想再延伸两个方向，因为我觉得这可能是当下世界上最重要的问题之：AI部署的速度与早期阶段的实际价值。如果我们试图预测2030年世界的样子，这就是最核心的问题。</p> <p>首先，一个显而易见的不同是“延迟要求”。自动驾驶模型的实时性要求极高，模型可能只有几千万参数，但必须毫秒级响应。而知识型工作并不一定受这个约束。当然，对计算机操作类任务或许仍有要求。</p> <p>第二个不同点，更关键的也许是资本开销。部署一辆自动驾驶车的成本极高，而AI模型则完全不同。虽然每次推理有额外的计算成本，但边际成本极低。训练一次大模型的投入可以摊薄到后续的无数次调用中。部署一个新的AI实例，远比制造一辆新车简单得多。整体上，AI的扩张经济性要好得多。</p> <p>Andrej Karpathy：我同意。凡是“以比特为核心”的事，都比“接触物理世界”的事容易上百万倍。比特是完全可重组、可任意复制、可即时更新的——这意味着AI产业的适应速度和扩张速度都会更快。至于你提到的另一个问题……</p> <p>Dwarkesh Patel：延迟需求。以及它对模型规模的影响。</p> <p>Andrej Karpathy：是的，我认为大体上你的判断是对的。当然，在大规模知识工作应用场景中，也会出现新的延迟瓶颈——因为要支撑这些任务，我们必须建立庞大的计算供应链与算力调度系统。不过，还有一个更容易被忽略的层面，我想简短地补充一下：那就是法律、社会、保险与伦理的层层结构。</p> <p>比如，在自动驾驶中，有人往Waymo车顶放一个路锥，这种行为也必须被纳入系统设计。那么，AI社会中它的等价物是什么？谁来承担责任？谁来定义“AI事故”？什么是“AI保险”？同样的问题也会出现在Agent与多模型协作中。所以，我觉得自动驾驶其实是一个极好的类比。几乎每个现实挑战，在AI社会里都会出现它的“镜像版本”：</p> <p>“放锥体”的人 → 故意干扰AI输入的用户；</p> <p>“远程操作员” → 隐形的人类在AI系统中的监管者；</p> <p>“交通法规” → 模型间交互的伦理与法律边界。</p> <p>Dwarkesh Patel：这很有趣。那这是否意味着——当前AI行业正在疯狂扩建算力，计划在一两年内增加十倍，十年内甚至百倍。如果AI的实际部署速度低于乐观预测，我们是否可能在“过度造计算力”？</p> <p>Andrej Karpathy：这让我想到历史上的类似情况，比如铁路泡沫，或者更准确地说，是上世纪电信行业的过度铺设。上世纪90年代末，互联网还没真正普及，但整个电信行业已经为“未来的互联网”提前铺满了光缆，于是出现了泡沫。</p> <p>Dwarkesh Patel：是的，电信泡沫。</p> <p>Andrej Karpathy：对，我理解我听起来有点悲观。但其实我是个乐观主义者。我认为AI最终一定会奏效——这是一个可解的问题，只是需要时间。我之所以语气显得悲观，是因为我每天刷推特时看到太多“脱离现实”的言论。很多都源于融资激励、流量变现或资本营销，而不是技术事实。我只是对这种“虚高”在做出反应。但总体上，我仍然非常看好这项技术的长期前景。</p> <p>过去一年，Claude Code、OpenAI Codex等产品几乎是凭空出现的奇迹，而ChatGPT的使用量也验证了庞大的市场需求。所以，我并不认为我们在“过度建算力”。我认为所有这些计算资源最终都会被消化。只是，我希望行业能校准预期。在过去十五年的AI历程中，我反复见到“知名人士误判AI进度”的情形，这次也不例外。而这种误判不仅影响投资与产业，还可能带来地缘政治层面的后果。因此，我们更需要脚踏实地地理解AI技术究竟是什么，不是什么。</p> <h2 id="八建立星际舰队学院用教育守住人类在ai时代的主动权">八、建立”星际舰队学院”：用教育守住人类在AI时代的主动权</h2> <h3 id="81-为什么选择教育而非继续ai研究">8.1 为什么选择教育而非继续AI研究？</h3> <p><strong>Dwarkesh Patel：</strong>我们来聊聊Eureka Labs吧。你现在也可以去重新创办一家AI实验室，继续攻克那些技术难题。但你选择了做教育，这让我挺好奇的。你现在主要在做什么？为什么不是继续从事AI研究？</p> <p>Andrej Karpathy：我想，也许可以这样解释——我对当下AI实验室的发展有一种“确定性”的感觉。我当然可以参与进去，也能起到一些作用，但那种作用未必是独特的。相反，我现在更担心的，是人类在这一切中的角色会逐渐被边缘化。我不仅关心AI将来能否建造“戴森球”、能否实现全自动化的未来，更关心那时的人类会怎样。我希望人类在那个未来依然能活得好，而不是被甩在AI进步的阴影之外。我觉得这正是我可以提供独特价值的地方——让人类能够在智能时代继续保持能动性与尊严，而不是去做前沿实验室里那种“边际的技术改进”。我最害怕的未来，其实有点像《机器人总动员（WALL·E）》或《蠢蛋进化论（Idiocracy）》那样——人类变成了旁观者，被智能系统“照顾”着、麻木着。所以，我认为教育是避免那种未来的关键路径。通过教育，人类才能在AI时代重新掌握主动权。</p> <p>Dwarkesh Patel：那你现在在Eureka具体在做什么？</p> <p>Andrej Karpathy：Eureka的目标，说得最简单一点，就是打造现实版的“星际舰队学院（Starfleet Academy）”。我不知道你有没有看过《星际迷航》。</p> <p>Dwarkesh Patel：我没看过，但听说过。</p> <p>Andrej Karpathy：在《星际迷航》里，Starfleet Academy是一所培养星际飞船驾驶员和科学官的精英学院——那是为探索未来、构建前沿技术而设立的学校。我想象的Eureka，就是这样的地方：一个极具前瞻性的技术教育机构，实时更新知识体系，培养真正能驾驭未来科技的人。</p> <p>Dwarkesh Patel：我接下来想问你一类更具体的问题。你是全世界最擅长讲授技术和科学内容的人之一。你能谈谈你认为“如何才能把技术内容讲好”吗？我很好奇你在YouTube上的教学理念，以及在Eureka中，这种理念会有什么不同？</p> <p>Andrej Karpathy：关于Eureka，我觉得最让我着迷的是——AI将彻底改变教育，而我们必须重新设计整个系统。我认为现在的阶段还非常早期。很多人会从最直接的方式入手，比如让LLM当成问答助手、讲解内容、出题练习，这些当然有用，但对我来说，这还太粗糙了。我希望能“做对”，而不是“做出来”。目前AI的能力还远远达不到我想要的那种“理想导师体验”。举个例子：我最近在学韩语。一开始我是在网上自学，后来去韩国上了小班课，大概十个人左右，再后来换成了一对一的导师。这段经历让我真正意识到好老师的标准有多高。比如我的韩语老师——她只通过几分钟的交流，就能精准判断我的学习阶段、我掌握了什么、哪里模糊。她能提出恰到好处的问题，探测我的知识模型。而今天的任何LLM，都做不到这一点，甚至离那还差得很远。当她真正了解我的程度后，她能为我定制出完全匹配的学习内容。这就是理想的学习状态——永远被恰当地挑战（appropriately challenged）：既不太难，也不太简单。在那种教学中，我感觉自己是唯一的学习瓶颈。知识永远准确、节奏刚好、解释充分——唯一的限制就是我的记忆力与专注度。我希望能让每个人都拥有这种体验。教育不该是知识的瓶颈，而是能力的放大器。这正是Eureka想要重构的未来教育模式。</p> <p>Dwarkesh Patel：你如何自动化这个过程呢？</p> <p>Andrej Karpathy：这是个非常好的问题——“如何自动化这种导师体验”？就以当前的AI能力来说，答案是：做不到。也正因为如此，我认为现在其实还不是构建“AI导师”产品的最佳时机。虽然这无疑是一个有价值、并且许多人都会尝试去做的方向，但以我想要达到的标准而言，目前的模型能力还远远不够。哪怕今天推出一个AI教学产品，它在教育价值上仍是巨大的，但当我回想起自己和那位韩语导师的体验时，我会觉得这个门槛实在太高——高到让我感到敬畏，甚至觉得“暂时还造不出来”。</p> <p>Dwarkesh Patel：但你确实已经在做了，对吧？</p> <p>Andrej Karpathy：是的，但也不是以那种“AI导师”的形式。任何真正体验过优秀导师的人，都会问自己：“你怎么可能复制出这样的存在？”所以我目前更多是在等待那种能力的到来。在AI行业里，我其实常常扮演“劝退”的角色。以前我做计算机视觉咨询时，我的最大价值有时恰恰是告诉客户不要用AI。因为并不是所有问题都适合用AI解决。教育领域现在也是类似的情况：我想要的那种AI导师体验还没到时机。所以我现在构建的Eureka，看起来会更“传统”一些——既有实体部分，也有数字部分。但我很清楚未来的形态会是什么样，只是我们还没到那一步。</p> <p>Dwarkesh Patel：那你能透露一下，你计划今年或明年推出的产品是什么吗？</p> <p>Andrej Karpathy：我现在正在开发Eureka的第一门课程。我希望它能成为“学习AI的最先进目的地”，一个毫无疑问的行业标杆。AI是我最熟悉的领域，所以我认为它是理想的起点。课程名为LLM 101N，其中的压轴项目是一个叫 NanoChat 的作品——它是课程的核心部分，也是学生的最终项目。我现在正在补充中间环节的教学模块，并准备组建一个小型助教团队，真正把整门课搭建成一个系统的学习体验。</p> <p>我想补充的是：很多人谈教育时，往往强调“知识传播”的那一面，也就是把已有的知识扩散出去。但在我看来，教育其实是一项极具技术挑战性的工程问题。对我来说，教育的本质是“为学习者搭建通往知识的坡道（ramps to knowledge）”。NanoChat就是这样一个坡道，它是一个极简但完整的AI项目。只要学生阅读它、拆解它、理解它，他们就能在极短时间内获得大量“灵光乍现”的时刻。我称之为“Eurekas per second”，即每秒顿悟率。这就是我想要的教育体验：高密度的理解时刻。Eureka的使命，就是要设计出这种“坡道体系”，让学习者永远不会卡关，不会觉得太难或太易，而是被持续地、精确地推动向前。</p> <p>Dwarkesh Patel：所以你现在的设想是，在短期内，由学生自己去培养“自我探测理解力”来取代AI导师的那种提问方式。只要他们有足够的自省能力，就能在TA、LLM和参考答案之间找到正确路径。听起来，真正的核心不是AI自动化本身，而是你把“如何解释AI”这件事系统化并编码进课程结构，这才是最大的创新。</p> <p>Andrej Karpathy：我认为这确实是关键——要永远校准自己与现实的AI能力之间的差距。很多人会说“直接去问ChatGPT就行了”，但如果你真的让它“教你AI”，它只会生成一些杂乱的东西——远远称不上教学内容。当前的AI还写不出NanoChat这样的教学作品，而NanoChat正是我认为“当前能力边界下最有用的形态”。我依然在与AI合作，它帮我生成素材、起草框架、处理繁琐的部分，让我能更快地构建课程。</p> <p>我过去曾在斯坦福开设CS231N课程——那是斯坦福第一门深度学习课程。而现在在开发LLM 101N时，我能明显感到两者的差别：如今的AI极大地提升了我的生产力。我可以让LLM帮我写示例代码、整理幻灯片、生成解释结构，我几乎以数倍速度推进课程开发。但内容的创造性设计、概念提炼、以及教学逻辑仍需要我本人完成。AI是辅助，但还不是替代。真正的挑战在于——你是否能正确地定位AI的角色，让它在恰当的时机做恰当的事。</p> <p>Dwarkesh Patel：所以，当你设想Eureka在几年后能够提供的内容时，看起来最大的瓶颈其实是在各个学科里找到像你这样的“Karpathy”，能够把自己对知识的理解转化为通向知识的“坡道”，对吗？</p> <p>Andrej Karpathy：我认为这会是一个逐步演变的过程。现阶段，Eureka的重点还是招聘教师，与AI及教学团队协作，共同打造最前沿的课程体系。而随着时间推移，部分助教可能会逐渐被AI替代。比如，对于一些基础性问题，AI完全可以通过掌握全部课程资料，为学生提供即时、准确的解答，成为高质量的自动化助教。不过，教师仍然是关键的，他们负责课程整体架构的设计与协调，确保内容体系科学、连贯、层层递进。我预想的教育演变路径是：从“人类主导、AI协作”逐渐过渡到“AI设计、人类监督”。也许未来某个阶段，AI会在课程设计上比我做得更好，而那时我可能反而变得不再必要。但这需要一个长期的演化过程。</p> <p>Dwarkesh Patel：那你设想的教学体系是由不同领域的专家各自贡献课程内容，还是说你希望保持自己在教学风格和理念上的主导？比如Sal Khan在Khan Academy里，几乎所有课程视频都是他本人讲解的。你会采取类似的方式吗？</p> <p>Andrej Karpathy：不会那样。我会聘请各领域的专家教师。因为有许多学科我并非专家，只有真正的领域专家才能为学生提供最前沿的学习体验。我当然会继续主导AI方向的课程，但在更广阔的教育体系中，专业化分工是必须的。 总体而言，我设想的Eureka会比人们想象的更“传统”一些。Starfleet Academy在我心中首先是一所实体学校——学生能全职学习，从头到尾与教师、同学共同完成课程内容，确保真正理解。而在此之下，我还会推出一个数字化版本，它可能包含AI教学助手或LLM互动系统，但属于一个相对“次一级”的体验。虽然不如线下沉浸式学习那样完整，但它能让全世界八十亿人都有机会接触到优质教育。</p> <p>Dwarkesh Patel：听起来你其实是在以当代工具重新发明“大学”——基于AI、网络和交互技术，从第一性原理出发构建教育体系，并吸引那些真正有动力、愿意深入学习的人。</p> <p>Andrej Karpathy：是的，我认为未来不仅需要“教育”，还需要“再教育（re-education）”。我非常希望能在这方面有所贡献，因为我相信工作的形态将发生巨大变化。举个例子，现在已经有大量人开始学习AI技能、尝试提升自己——这恰恰是一个非常好的教学起点。在动机层面上，我常把教育分成两个阶段：AGI 之前与 AGI 之后。在AGI出现之前，学习的动机很简单：人们希望赚到钱，而掌握AI技能就是当下最直接的路径。但在AGI之后，这个问题就变得更有趣了。如果一切都实现了自动化，人类不再需要工作，那人们为什么还要上学？</p> <p>我常说，“AGI之前的教育是有用的，AGI之后的教育是有趣的。”就像今天人们去健身房一样。我们并不再需要体力去搬重物——机器早已替代了这部分功能。可人们依然健身，因为这很有趣、能让身体健康、还能让你看起来更好。换句话说，健身已成为一种心理与文化上的吸引力，而不再是生存的需求。我相信教育的演化也会类似。未来的人们上学，就像他们现在去健身房一样。</p> <p>目前，大多数人觉得学习困难，是因为教材要么太难、要么太简单，导致他们反复“卡关”或失去兴趣。只有少部分人能克服这种阻力。而我认为，这其实是一个可以被技术解决的工程问题。如果我们能像我学习韩语时那位导师那样，打造出真正理解学生状态、能持续适配的AI导师系统，学习就会变得轻松而愉快。那样的话，人们将会为了乐趣而学习，因为学习不再痛苦，而是即时满足、不断“顿悟”的体验。到那时，任何人都能轻松掌握多门语言，熟悉大学的所有基础课程，只因为“为什么不呢？”</p> <p>Dwarkesh Patel：我明白你的意思了。也就是说，在AGI之后，人们一方面会把教育当作娱乐、当作自我提升的方式；但另一方面，你似乎也认为教育有助于让人类保持对AI的掌控，对吗？这两者听起来有些不同。你觉得教育的作用是两者并存吗？</p> <p>Andrej Karpathy：可以这么说。我确实认为教育在短期内是一种赋能手段，让人类能跟上AI的步伐。但从长期来看，我得承认，这可能是一场“逐渐失利的游戏”。AI的能力终将超越人类，不过在那之前，我们还远远没有触及人类潜力的极限。</p> <p>目前的问题是，大多数人之所以没能“走得更远”，是因为学习系统效率太低。教材太难或太浅，人们总是被卡在错误的难度区间。如果我们能让教育像精准的训练计划一样被定制化，人类完全可以达到前所未有的广度与深度。未来，每个人都可能会说五种语言、掌握多学科知识，仅仅因为那变得非常轻松。</p> <p>Dwarkesh Patel：这真的很有趣——你这个愿景和健身文化完全对应。想想看，一百年前几乎没人会有一副健美的身材，也没人能轻松卧推两片杠铃。现在呢？系统训练的理念普及后，普通人都能做到。你设想的“教育健身房”模式，本质上就是让学习像健身一样被系统化、量化和普及——让人类在任何知识领域中都能更快、更深、更强。</p> <p>Andrej Karpathy：没错，这正是我想看到的未来。我觉得我其实是在押注“人性的不变性”。无论技术如何发展，人类依然会渴望探索、思考、学习这些事情，而这种渴望是跨越时代的。我相信这种追求会继续存在——就像几千年来人们始终向往智慧与力量一样。</p> <p>如果你回顾历史，其实也能找到一些“后AGI时代的雏形”。比如古希腊的学者圈、或者某些贵族社会——在那些物质充裕、劳动不再是生存必要条件的小型社会里，人们往往会把时间投入到身体和精神的“繁荣”中：学习、运动、艺术、哲学。我相信这种人类本能会延续下去。</p> <p>当然，如果我错了，我们最终迎来一个像《机器人总动员》或《蠢蛋进化论》那样的未来——那将是最糟糕的结局。就算我们建起了戴森球、拥有无限能量，我也不会认为那是胜利。因为那样的世界失去了人类的主体性和尊严。我真正关心的，是人类的成长与卓越——每个人都应该以某种方式成为“超级人类”。</p> <p>Dwarkesh Patel：我理解，但那样的世界似乎依然是一种“文化式的存在”——人类依旧活着，但无法再凭个人的劳动或思维去改变技术的轨迹。你可能还能对AI的决策提出批准或否决意见，但那不再是因为你自己发明了什么、创造了什么新设计而影响未来。</p> <p>Andrej Karpathy：或许吧。但我认为我们仍会经历一个过渡阶段——在这个阶段里，人类依然能通过自身理解力和创造力参与其中，推动科技向前。长期来看，这种能力可能会逐渐被AI取代，但在人类与AI共存的时期，我们仍能有所贡献。</p> <p>甚至我觉得，到了那个阶段，这种“参与”可能会演化成一种运动或竞赛。就像今天的力量举运动员不断挑战身体极限，那么在未来的“认知时代”，或许会有人类去挑战知识与思维的极限。这会成为一场“智力奥运会”。</p> <p>想象一下：当每个人都能拥有一个完美的AI导师，人类的潜能会被彻底释放。我们今天所称的“天才”，可能只是人类心智能力的冰山一角。未来，人类可能真的能靠认知训练变得“超人”。这并不是神话，而是我觉得非常现实的方向。</p> <p>Dwarkesh Patel：我太喜欢这个愿景了。事实上，我觉得这种未来最“对口”的人可能就是我——我的工作本身就要求我每周都要学习不同的领域知识。如果你真的能把学习做到像你说的那样，我会是第一个报名的人。</p> <p>Andrej Karpathy：在这点上我其实跟你挺像。很多人都讨厌上学，迫不及待地想离开学校，但我恰恰相反——我非常喜欢学习、热爱学校，甚至希望能一直读下去。我一路读到了博士，最后是因为学校“实在不让我再待了”，我才去了工业界。对我而言，学习本身既是一种乐趣，也是一种赋能的过程。它让我变得更有用、更有生产力。</p> <p>Dwarkesh Patel：我觉得你刚刚其实提到了一点很微妙的东西，我想把它明确说出来。到目前为止，网络课程之所以没能让“每个人都学到所有知识”，问题并不是内容本身，而是动机门槛太高。这些课程缺乏清晰的“上坡道”，学习者太容易被卡住，学不动、也坚持不下去。如果我们能像你说的那样，做出一个真正像“优秀导师”一样的系统，它能即时调整节奏、匹配难度，这在动机层面上会是一次彻底的“解锁”。</p> <p>Andrej Karpathy：我完全同意。因为“卡关”的体验真的很糟糕。你投入了时间和精力，却发现学不下去——要么太难，要么太无聊。那种心理上的“负反馈”会让人迅速失去兴趣。而一旦我们能做到真正的个性化匹配，学习其实是愉快的。对我来说，这完全是一个技术问题——我们要解决的，不是“人为什么不想学”，而是“系统为什么不够聪明”。 在未来一段时间内，教育的最佳状态大概是人类教师与AI协同。AI帮我们调整节奏、提供反馈，人类负责激发兴趣、理解情境。最终，也许有一天AI可以单独胜任这一切，但那还需要时间。</p> <p>Dwarkesh Patel：我想问一些关于“教学方法”的问题。如果你要给其他领域的教育者一些建议——尤其是在那些无法像编程那样用代码验证理解的学科——你会怎么告诉他们，如何制作像你那样的教学内容？</p> <p>Andrej Karpathy：这是个很宽泛的问题。我可能有十几条“半下意识”的小技巧，但从高层来看，我的教学思维很大程度上受益于我的物理学背景。我一直认为，物理是最能“启动人类大脑”的学科。基础教育的目标不应是为未来某个行业储备技能，而是让你的大脑学会思考。而物理学能让你学会建模、抽象与近似——这些思维习惯对任何领域都极其宝贵。</p> <p>举个例子：物理学家常说“假设有一头球形的牛”，很多人会笑，但这其实是极其聪明的思维方式。它教你抓住一阶近似（first-order term），去理解事物的核心结构，再逐步补上二阶、三阶的复杂细节。这种分层建模的思维，是理解世界的通用框架。</p> <p>我很喜欢Geoffrey West那本《Scale》，他用物理的方法研究生物尺度定律：动物体型越大，心跳越慢；散热能力取决于表面积（平方增长），而能量产生取决于体积（立方增长）。这些规律让你看到——哪怕是复杂的生命系统，也能用物理近似捕捉它的核心。</p> <p>所以，在教学中我总是去寻找“最重要的一阶项”：这个体系中，真正决定性的是哪一部分？我能否用最简洁的方式把它展示出来。举个我自己的例子：我写过一个叫 MicroGrad 的开源项目。只有一百行Python代码，却完整实现了神经网络的前向与反向传播。这一百行代码里，包含了神经网络学习的全部核心思想：链式求导、梯度传播、参数优化。当然，它不高效，缺少并行化与内存优化。但那些只是“效率工程”，而非“智识核心”。MicroGrad让人能在极简模型里真正“看懂”深度学习是怎么工作的。</p> <p>我热爱这种“把知识解耦与重排”的过程。教育的本质就是构建知识的上坡道（ramps to knowledge），让每个概念都依赖于前一个概念，不让学生被卡在中间。这其实是一个非常高智力的工作：你得不断拆解、简化、重组复杂系统，让知识从混乱的网络，变成一条可攀登的阶梯。对我而言，这种“理清知识的过程”本身就是最有趣的智力活动之一。</p> <p>图片 图片来源：The Nvidia Patterns</p> <p>Dwarkesh Patel：这样的教学方式真的能极大提升学习动机。比如你在讲解 Transformer 的视频里，从最简单的 bigram 模型开始——一个“前一个词 → 下一个词”的查表过程，简直就是一张最原始的词表。然后你再一步步引出 Transformer，每一层、每一个结构的添加都有明确动机：为什么需要这一部分？它解决了前一步的什么问题？这比直接记公式、背注意力机制的数学表达式要有意义得多。</p> <p>Andrej Karpathy：对，先让学生感受到“痛点”，再呈现解决方案。这其实非常巧妙。学习的过程应该是一段“渐进式的推理体验”。我总是希望学生能跟着这个节奏走。他们先遇到一个问题，然后我带他们看见为什么要发明下一个方法。这样的教学充满“参与感”，学生也会更投入。还有一些我认为非常重要的小技巧，很多优秀教师都会用。比如：不要直接给出答案。一定要先问学生，“你会怎么解决？”哪怕他们想错了也没关系，因为这个过程能让他们明确“问题空间”是什么。直接给答案其实是一种“偷走学习体验”的行为，有点不厚道。</p> <p>Dwarkesh Patel：对，因为当学生自己先尝试时，他们就能更清楚地理解——这个问题的行动空间是什么，目标是什么，为什么只有这个方案能满足目标。</p> <p>Andrej Karpathy：没错。只有在你尝试过、思考过之后，再看到正确答案，你才会对它产生真正的“理解与敬意”。那种“啊，原来如此”的瞬间会让学习效率最大化——每获得一个新知识点，就能带来最大的信息增量。</p> <p>Dwarkesh Patel：非常同意。那你觉得，为什么很多真正的专家——那些在领域里非常厉害的人——往往反而讲不好课？</p> <p>Andrej Karpathy：这其实是一个非常普遍的现象——知识的诅咒。当你成为专家后，你会下意识地假设很多基础是“理所当然”的，以至于你很难再站在初学者的角度思考。我自己也常常犯这个错。有一次，有人给我看一篇生物学论文，我完全看不懂。我满脑子都是“愚蠢的问题”，于是我干脆用ChatGPT，把论文放进上下文窗口里，开始问它这些问题。AI 会帮我理清最基础的概念，等我弄懂后，我就把这整段对话发回给那位研究员——他是那篇论文的作者。我觉得这其实特别有帮助。因为当一个专家看到别人是如何困惑的，他才能重新理解自己的知识体系里哪些地方“默认”太多了。所以我其实很希望，大家能把自己和 ChatGPT 的“傻问题对话”分享给我。那样我能看到学生真正在哪些地方卡壳，从而改进教学。</p> <p>Dwarkesh Patel：太有道理了。其实我觉得，这个问题在写作上也一样——无论是论文、博客还是产品公告。几乎在100%的情况下，如果你把作者的正式文本换成他们“午餐时的口头解释”，内容都会变得更清晰、更准确，甚至更科学。因为正式写作时，人们总喜欢把语言变得抽象、堆满术语，还要在开头铺垫几段“清嗓子式”的废话。而面对面交流时，你被迫直接“讲明白”，直接进入主题。这种“把话说清楚”的冲动，反而让知识的表达更真实。</p> <p>Andrej Karpathy：对，我看到那条推文的时候真的觉得写得太好了，我还转发给了好几个人。这种情况我见过太多次了。最典型的例子就是我还在读博士、做研究的时候，你读别人发表的论文，反复琢磨、费劲地想明白他们到底在做什么。然后过几个月在学术会议上碰到那位作者，大家一起喝啤酒聊天，你随口问一句：“嘿，你那篇论文到底是讲什么的？”结果他三句话就把核心思想讲得清清楚楚——那三句话完美地捕捉了论文的精髓，让你完全明白整篇论文在做什么，而根本不用读完那几十页。就像只有在酒桌上随意聊时，他才会这么说：“哦，其实就是把这个想法和那个想法结合一下，做了个实验试试看。”一句话直击本质。你就会想，为什么正式论文里不能这么写？</p> <p>Dwarkesh Patel：刚才我们一直在讨论“如何让解释者更好地讲清楚一个概念”。那从学习者的角度来说呢？如果一个学生没有像你这样的 Karpathy 来帮他讲透一篇论文——他只是自己读书、读论文——你会给他们什么建议？在非专业领域自学时，你是怎么做的？</p> <p>Andrej Karpathy：老实说，我也没有什么“独门秘籍”，这其实是一个挺“痛苦”的过程。但我确实有几个心得。</p> <p>第一，我发现“按需学习（learning on demand）”特别有效。也就是，当你正在做一个项目、要解决一个实际问题时，你带着明确目标去学习，这种学习是有“即时奖励”的。而相对的，学校教育里很多是“广度式学习”。老师告诉你：“先学这个，将来会用到的”，然后你就学了，但其实你当下感受不到任何价值。我喜欢的是那种“学即所用”的学习。你因为要做成一件事，被迫去理解知识，这时候吸收效率极高。</p> <p>第二个我觉得非常重要的是：教别人是最好的学习方式。每当我试图向别人解释某个概念时，我就会立刻暴露出自己理解的漏洞。你一讲就发现：“糟糕，我其实没搞懂这一点。” 这种不舒服的意识逼迫你回去查、去补，直到你能用自己的语言清楚讲出来。这就是为什么我喜欢不断地重讲、重写，哪怕只是给AI、给学生、或者写成推文。解释知识的过程会强迫你重新整理、操控并真正“拥有”它。</p> <p>Dwarkesh Patel：是的，这真是一个完美的收尾。Andrej，非常精彩，谢谢你。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">相关文章推荐：</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/2025-01-15-xiaohanding-paper-suggestion/"></a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/anthropic-claude-agent-methodology/">Anthropic 研究员详解：构建高效 Claude 智能体的完整方法论</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/claude-code-knowledge-management/">Claude Code自定义命令在知识管理与内容创作中的系统化应用研究</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/habit/">18个改变人生的习惯：科学证据支持的长期主义指南</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/openai-gpt5-researcher-vision/">OpenAI双巨头首次详解GPT-5：不是下一代GPT，终极形态是AI研究员</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/kimi-yang-dialogue/">KIMI创始人杨植麟深度访谈：攀登无限之山</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jason-wei-ai-insights/">Jason Wei：理解2025年AI进展的三种关键思路</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/anthropic-pretraining-nick-joseph/">Nick Joseph访谈：Anthropic预训练的核心思考与实践</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/xuyangsheng-CUHK-SZ/">徐扬生院士：人工智能时代的教育</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/sutton-llm-is-wrong-way/">Sutton：大语言模型走错了路，不符合「苦涩教训」精神</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Chao Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-\u4e2d\u6587\u7b80\u4ecb",title:"\u4e2d\u6587\u7b80\u4ecb",description:"",section:"Navigation",handler:()=>{window.location.href="/chinese/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-",title:"",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/2025-01-15-xiaohanding-paper-suggestion/"}},{id:"post-anthropic-\u7814\u7a76\u5458\u8be6\u89e3-\u6784\u5efa\u9ad8\u6548-claude-\u667a\u80fd\u4f53\u7684\u5b8c\u6574\u65b9\u6cd5\u8bba",title:"Anthropic \u7814\u7a76\u5458\u8be6\u89e3\uff1a\u6784\u5efa\u9ad8\u6548 Claude \u667a\u80fd\u4f53\u7684\u5b8c\u6574\u65b9\u6cd5\u8bba",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/anthropic-claude-agent-methodology/"}},{id:"post-claude-code\u81ea\u5b9a\u4e49\u547d\u4ee4\u5728\u77e5\u8bc6\u7ba1\u7406\u4e0e\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u7cfb\u7edf\u5316\u5e94\u7528\u7814\u7a76",title:"Claude Code\u81ea\u5b9a\u4e49\u547d\u4ee4\u5728\u77e5\u8bc6\u7ba1\u7406\u4e0e\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u7cfb\u7edf\u5316\u5e94\u7528\u7814\u7a76",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/claude-code-knowledge-management/"}},{id:"post-18\u4e2a\u6539\u53d8\u4eba\u751f\u7684\u4e60\u60ef-\u79d1\u5b66\u8bc1\u636e\u652f\u6301\u7684\u957f\u671f\u4e3b\u4e49\u6307\u5357",title:"18\u4e2a\u6539\u53d8\u4eba\u751f\u7684\u4e60\u60ef\uff1a\u79d1\u5b66\u8bc1\u636e\u652f\u6301\u7684\u957f\u671f\u4e3b\u4e49\u6307\u5357",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/habit/"}},{id:"post-openai\u53cc\u5de8\u5934\u9996\u6b21\u8be6\u89e3gpt-5-\u4e0d\u662f\u4e0b\u4e00\u4ee3gpt-\u7ec8\u6781\u5f62\u6001\u662fai\u7814\u7a76\u5458",title:"OpenAI\u53cc\u5de8\u5934\u9996\u6b21\u8be6\u89e3GPT-5\uff1a\u4e0d\u662f\u4e0b\u4e00\u4ee3GPT\uff0c\u7ec8\u6781\u5f62\u6001\u662fAI\u7814\u7a76\u5458",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/openai-gpt5-researcher-vision/"}},{id:"post-kimi\u521b\u59cb\u4eba\u6768\u690d\u9e9f\u6df1\u5ea6\u8bbf\u8c08-\u6500\u767b\u65e0\u9650\u4e4b\u5c71",title:"KIMI\u521b\u59cb\u4eba\u6768\u690d\u9e9f\u6df1\u5ea6\u8bbf\u8c08\uff1a\u6500\u767b\u65e0\u9650\u4e4b\u5c71",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/kimi-yang-dialogue/"}},{id:"post-jason-wei-\u7406\u89e32025\u5e74ai\u8fdb\u5c55\u7684\u4e09\u79cd\u5173\u952e\u601d\u8def",title:"Jason Wei\uff1a\u7406\u89e32025\u5e74AI\u8fdb\u5c55\u7684\u4e09\u79cd\u5173\u952e\u601d\u8def",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/jason-wei-ai-insights/"}},{id:"post-nick-joseph\u8bbf\u8c08-anthropic\u9884\u8bad\u7ec3\u7684\u6838\u5fc3\u601d\u8003\u4e0e\u5b9e\u8df5",title:"Nick Joseph\u8bbf\u8c08\uff1aAnthropic\u9884\u8bad\u7ec3\u7684\u6838\u5fc3\u601d\u8003\u4e0e\u5b9e\u8df5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/anthropic-pretraining-nick-joseph/"}},{id:"post-\u5f90\u626c\u751f\u9662\u58eb-\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\u7684\u6559\u80b2",title:"\u5f90\u626c\u751f\u9662\u58eb\uff1a\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\u7684\u6559\u80b2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/xuyangsheng-CUHK-SZ/"}},{id:"post-andrej-karpathy\u6df1\u5ea6\u5bf9\u8bdd-agent\u7684\u5341\u5e74\u5f81\u7a0b\u4e0eai\u7684\u5e7d\u7075\u672c\u8d28",title:"Andrej Karpathy\u6df1\u5ea6\u5bf9\u8bdd\uff1aAgent\u7684\u5341\u5e74\u5f81\u7a0b\u4e0eAI\u7684\u5e7d\u7075\u672c\u8d28",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/karpathy-agent-ten-years/"}},{id:"post-sutton-\u5927\u8bed\u8a00\u6a21\u578b\u8d70\u9519\u4e86\u8def-\u4e0d\u7b26\u5408-\u82e6\u6da9\u6559\u8bad-\u7cbe\u795e",title:"Sutton\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u8d70\u9519\u4e86\u8def\uff0c\u4e0d\u7b26\u5408\u300c\u82e6\u6da9\u6559\u8bad\u300d\u7cbe\u795e",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/sutton-llm-is-wrong-way/"}},{id:"post-patrick-hsu-evo2\u534e\u4eba\u79d1\u5b66\u5bb6-\u865a\u62df\u7ec6\u80de\u8fc8\u5411gpt-2\u9636\u6bb5-\u5408\u6210\u751f\u7269\u5b66\u5c06\u6df1\u523b\u6539\u53d8\u4e16\u754c",title:"Patrick Hsu-Evo2\u534e\u4eba\u79d1\u5b66\u5bb6\uff1a\u865a\u62df\u7ec6\u80de\u8fc8\u5411GPT-2\u9636\u6bb5\uff0c\u5408\u6210\u751f\u7269\u5b66\u5c06\u6df1\u523b\u6539\u53d8\u4e16\u754c",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/patrick-hsu-evo2/"}},{id:"post-anthropic\u5185\u90e8ai\u4ee3\u7801\u9769\u547d-\u73b0\u5b9e\u8fd8\u662f\u7092\u4f5c-\u5f00\u53d1\u8005\u793e\u533a\u7684\u6df1\u5ea6\u8d28\u7591",title:"Anthropic\u5185\u90e8AI\u4ee3\u7801\u9769\u547d\uff1a\u73b0\u5b9e\u8fd8\u662f\u7092\u4f5c\uff1f\u5f00\u53d1\u8005\u793e\u533a\u7684\u6df1\u5ea6\u8d28\u7591",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/claude-code-sofaware/"}},{id:"post-\u59da\u987a\u96e8-ai\u4e0eagent\u7814\u7a76\u89c2\u70b9\u96c6",title:"\u59da\u987a\u96e8\uff1aAI\u4e0eAgent\u7814\u7a76\u89c2\u70b9\u96c6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yao-shunyu-agent-research/"}},{id:"post-\u59da\u987a\u96e8ai\u4e0eagent\u7814\u7a76\u89c2\u70b9\u96c6",title:"\u59da\u987a\u96e8AI\u4e0eAgent\u7814\u7a76\u89c2\u70b9\u96c6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yao-shunyu-ai-agent-insights/"}},{id:"post-\u6768\u690d\u9e9f\u89c2\u70b9\u7cbe\u534e-ai\u65f6\u4ee3\u7684\u6280\u672f\u54f2\u5b66\u4e0e\u5b9e\u8df5",title:"\u6768\u690d\u9e9f\u89c2\u70b9\u7cbe\u534e\uff1aAI\u65f6\u4ee3\u7684\u6280\u672f\u54f2\u5b66\u4e0e\u5b9e\u8df5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yang-zhilin-ai-philosophy/"}},{id:"post-openai\u59da\u987a\u96e8\u6df1\u5ea6\u8bbf\u8c08-ai\u4e0b\u534a\u573a\u7684agent\u9769\u547d",title:"OpenAI\u59da\u987a\u96e8\u6df1\u5ea6\u8bbf\u8c08\uff1aAI\u4e0b\u534a\u573a\u7684Agent\u9769\u547d",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/openai-yao-dialogue/"}},{id:"post-deepmind\u79d1\u5b66\u8d1f\u8d23\u4eba\u6df1\u5ea6\u8bbf\u8c08-\u5982\u4f55\u7b5b\u9009\u5e76\u653b\u514b\u53d8\u9769\u6027\u6311\u6218",title:"DeepMind\u79d1\u5b66\u8d1f\u8d23\u4eba\u6df1\u5ea6\u8bbf\u8c08\uff1a\u5982\u4f55\u7b5b\u9009\u5e76\u653b\u514b\u53d8\u9769\u6027\u6311\u6218",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/deepmind-kohli-dialogue/"}},{id:"post-church-nature-ai\u9a71\u52a8\u86cb\u767d\u8d28\u8bbe\u8ba1-\u9769\u547d\u6027\u8303\u5f0f\u7684\u5168\u6d41\u7a0b\u89e3\u6790",title:"Church @ Nature\uff1aAI\u9a71\u52a8\u86cb\u767d\u8d28\u8bbe\u8ba1\uff0c\u9769\u547d\u6027\u8303\u5f0f\u7684\u5168\u6d41\u7a0b\u89e3\u6790",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/nature-ai-driven-protein-design/"}},{id:"news-one-offline-rl-paper-accepted-by-aaai-2024-sparkles-sparkles",title:'One Offline RL Paper accepted by **AAAI 2024**. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-two-papers-llama-excitor-videodistill-are-accepted-by-cvpr-2024-sparkles-sparkles",title:'Two Papers(LLaMA-Excitor, VideoDistill) are accepted by **CVPR 2024**. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-one-llm-safety-survey-paper-accepted-by-naacl-2024-sparkles-smile",title:'One LLM safety survey paper accepted by **NAACL 2024**. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-one-paper-accepted-by-ijcai-2024-survey-track-sparkles-sparkles",title:'One Paper accepted by **IJCAI 2024** Survey Track. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-robocodex-is-accepted-by-icml-2024-sparkles-sparkles",title:'RoboCodeX is accepted by **ICML 2024**. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-three-papers-emulated-disalignment-structured-reasoning-multi-objective-dpo-are-accepted-by-acl-2024-sparkles-sparkles",title:'Three Papers (Emulated Disalignment, Structured Reasoning, Multi-Objective DPO) are accepted by **ACL 2024**.<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">...',description:"",section:"News"},{id:"news-mm-safetybench-a-benchmark-for-safety-evaluation-of-multimodal-large-language-models-is-accepted-by-eccv-2024-project-page-https-isxinliu-github-io-project-mm-safetybench-sparkles-sparkles",title:"MM-SafetyBench (A Benchmark for Safety Evaluation of Multimodal Large Language Models) is accepted...",description:"",section:"News"},{id:"news-inference-time-language-model-alignment-via-integrated-value-guidance-is-accepted-by-emnlp-2024-arixv-link-https-arxiv-org-pdf-2409-17819-sparkles-sparkles",title:"Inference-Time Language Model Alignment via Integrated Value Guidance is accepted by **EMNLP 2024**....",description:"",section:"News"},{id:"news-weak-to-strong-search-align-large-language-models-via-searching-over-small-language-models-is-accepted-by-neurips-2024-neurips-link-https-proceedings-neurips-cc-paper-files-paper-2024-file-088d99765bc121c6df215da7d45bc4e9-paper-conference-pdf-sparkles-sparkles",title:"Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models is...",description:"",section:"News"},{id:"news-we-proposal-a-new-law-ai-45-law-toward-trustworthy-agi-arxiv-link-https-arxiv-org-abs-2412-14186-sparkles",title:'We proposal a new law, **AI 45\xb0-Law** toward trustworthy AGI! [Arxiv Link](https://arxiv.org/abs/2412.14186) <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">...',description:"",section:"News"},{id:"news-icml2025-emergent-response-planning-in-llm-https-arxiv-org-abs-2502-06258-and-c-3po-compact-plug-and-play-proxy-optimization-to-achieve-human-like-retrieval-augmented-generation-https-arxiv-org-abs-2502-06205-are-accepted-by-icml2025-sparkles",title:"[**ICML2025**] [Emergent Response Planning in LLM](https://arxiv.org/abs/2502.06258) and [C-3PO: Compact Plug-and-Play Proxy Optimization to...",description:"",section:"News"},{id:"news-our-paper-adversarial-preference-learning-for-robust-llm-alignment-is-accepted-by-acl2025-arxiv-link-https-arxiv-org-abs-2505-24369-sparkles",title:"Our paper Adversarial Preference Learning for Robust LLM Alignment is accepted by **ACL2025**....",description:"",section:"News"},{id:"news-we-find-patches-from-harmful-content-enabling-them-to-bypass-data-moderation-and-generate-dangerous-responses-when-encountering-the-full-image-or-related-text-vlms-can-aggregate-scattered-training-patchesk-https-arxiv-org-abs-2506-03614-sparkles",title:"We find patches from harmful content, enabling them to bypass data moderation and...",description:"",section:"News"},{id:"news-big-project-release-we-introduce-safework-r1-a-cutting-edge-multimodal-reasoning-model-that-demonstrates-the-coevolution-of-capabilities-and-safety-safework-r1-https-arxiv-org-abs-2507-18576-rocket-sparkles",title:"\ud83c\udf89 **Big Project Release!** We introduce **SafeWork-R1**, a cutting-edge multimodal reasoning model that...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%61%6E%67%63%68%61%6F [%41%54] %70%6A%6C%61%62 [%44%4F%54] %6F%72%67 [%44%4F%54] %63%6E","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=https://scholar.google.com/citations?hl=en&user=5KRbHPMAAAAJ&view_op=list_works&sortby=pubdate","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emigmo","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/00/5867-26.html","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@chaoyang4587","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>