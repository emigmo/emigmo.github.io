<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 李飞飞:从文字到世界,空间智能是AI的下一个前沿 | Chao Yang </title> <meta name="author" content="Chao Yang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emigmo.github.io/blog/2025/drfeifei-spatial-intelligence/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chao</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/chinese/">中文简介 </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">李飞飞:从文字到世界,空间智能是AI的下一个前沿</h1> <p class="post-meta"> Created in November 10, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/blog"> <i class="fa-solid fa-tag fa-sm"></i> blog</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="李飞飞从文字到世界空间智能是ai的下一个前沿">李飞飞:从文字到世界,空间智能是AI的下一个前沿</h1> <p><em>作者:李飞飞 (Fei-Fei Li)</em> <em>职位:斯坦福大学教授、World Labs 联合创始人</em> <em>发布时间:2025年11月</em> <em>来源:World Labs 官方博客</em></p> <hr> <h2 id="前言">前言</h2> <blockquote> <p><strong>“空间智能将改变我们创造和交互现实与虚拟世界的方式——彻底革新叙事、创造力、机器人学、科学发现,以及更多领域。这,正是AI的下一个前沿。”</strong></p> <p><strong>“今天的AI是’黑暗中的文字匠’:能言善辩,却无经验;知识丰富,却缺乏根基。”</strong></p> </blockquote> <p>1950年,当计算机还只是自动化算术和简单逻辑时,艾伦·图灵提出了一个至今仍回荡的问题:机器能思考吗?</p> <p>他能看到别人尚未看到的未来,需要非凡的想象力——那就是:智能或许有一天可以被”构建”,而非”诞生”。</p> <p>今天,以大语言模型(LLM)为代表的前沿AI技术,已经开始改变人类获取与处理抽象知识的方式。然而,它们仍然是”黑暗中的文字匠”:能言善辩,却无经验;知识丰富,却缺乏根基。</p> <p>李飞飞教授在这篇文章中,系统阐述了空间智能的重要性、世界模型的构建原则,以及这项技术将如何深刻重塑创造力、具身智能与人类进步。</p> <hr> <h2 id="目录">目录</h2> <ul> <li><a href="#%E4%B8%80%E5%BC%95%E8%A8%80%E4%BB%8E%E5%9B%BE%E7%81%B5%E4%B9%8B%E9%97%AE%E5%88%B0%E7%A9%BA%E9%97%B4%E6%99%BA%E8%83%BD">一、引言:从图灵之问到空间智能</a></li> <li> <a href="#%E4%BA%8C%E7%A9%BA%E9%97%B4%E6%99%BA%E8%83%BD%E4%BA%BA%E7%B1%BB%E8%AE%A4%E7%9F%A5%E7%9A%84%E8%84%9A%E6%89%8B%E6%9E%B6">二、空间智能:人类认知的脚手架</a> <ul> <li><a href="#21-%E8%BF%9B%E5%8C%96%E7%9A%84%E5%9F%BA%E7%9F%B3%E6%84%9F%E7%9F%A5-%E8%A1%8C%E5%8A%A8%E5%BE%AA%E7%8E%AF">2.1 进化的基石:感知-行动循环</a></li> <li><a href="#22-%E6%97%A5%E5%B8%B8%E7%94%9F%E6%B4%BB%E4%B8%AD%E7%9A%84%E7%A9%BA%E9%97%B4%E6%99%BA%E8%83%BD">2.2 日常生活中的空间智能</a></li> <li><a href="#23-%E4%BA%BA%E7%B1%BB%E5%88%9B%E9%80%A0%E5%8A%9B%E7%9A%84%E6%A0%B9%E5%9F%BA">2.3 人类创造力的根基</a></li> <li><a href="#24-%E6%8E%A8%E5%8A%A8%E6%96%87%E6%98%8E%E8%BF%9B%E6%AD%A5%E7%9A%84%E5%8A%9B%E9%87%8F">2.4 推动文明进步的力量</a></li> <li><a href="#25-%E5%BD%93%E5%89%8Dai%E7%9A%84%E7%A9%BA%E9%97%B4%E6%99%BA%E8%83%BD%E7%BC%BA%E9%99%B7">2.5 当前AI的空间智能缺陷</a></li> </ul> </li> <li> <a href="#%E4%B8%89%E6%9E%84%E5%BB%BA%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8Bai%E7%9A%84%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%8D%81%E5%B9%B4">三、构建世界模型:AI的下一个十年</a> <ul> <li><a href="#31-%E4%BB%80%E4%B9%88%E6%98%AF%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B">3.1 什么是世界模型</a></li> <li><a href="#32-%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%89%E5%A4%A7%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B">3.2 世界模型的三大核心能力</a></li> <li><a href="#33-%E6%8A%80%E6%9C%AF%E6%8C%91%E6%88%98%E4%B8%8E%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91">3.3 技术挑战与研究方向</a></li> <li><a href="#34-world-labs%E7%9A%84%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95">3.4 World Labs的研究进展</a></li> </ul> </li> <li> <a href="#%E5%9B%9B%E7%94%A8%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E6%9B%B4%E7%BE%8E%E5%A5%BD%E7%9A%84%E4%B8%96%E7%95%8C">四、用世界模型构建更美好的世界</a> <ul> <li><a href="#41-%E5%88%9B%E9%80%A0%E5%8A%9B%E4%B8%BA%E5%8F%99%E4%BA%8B%E6%B3%A8%E5%85%A5%E8%B6%85%E8%83%BD%E5%8A%9B">4.1 创造力:为叙事注入超能力</a></li> <li><a href="#42-%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E7%9A%84%E5%AE%9E%E8%B7%B5">4.2 机器人:具身智能的实践</a></li> <li><a href="#43-%E6%9B%B4%E9%95%BF%E8%BF%9C%E7%9A%84%E5%9C%B0%E5%B9%B3%E7%BA%BF%E7%A7%91%E5%AD%A6%E5%8C%BB%E7%96%97%E4%B8%8E%E6%95%99%E8%82%B2">4.3 更长远的地平线:科学、医疗与教育</a></li> </ul> </li> <li><a href="#%E4%BA%94%E7%BB%93%E8%AF%AD%E6%9E%84%E5%BB%BA%E4%B8%8E%E4%B8%96%E7%95%8C%E5%A5%91%E5%90%88%E7%9A%84%E6%99%BA%E8%83%BD">五、结语:构建与世界契合的智能</a></li> </ul> <hr> <h2 id="一引言从图灵之问到空间智能">一、引言:从图灵之问到空间智能</h2> <h3 id="11-图灵的愿景与ai的征程">1.1 图灵的愿景与AI的征程</h3> <p>1950年,当计算机还只是自动化算术和简单逻辑时,艾伦·图灵提出了一个至今仍回荡的问题:<strong>机器能思考吗?</strong></p> <p>他能看到别人尚未看到的未来,需要非凡的想象力——那就是:智能或许有一天可以被”构建”,而非”诞生”。</p> <p>这一洞见催生了一场持续至今的科学征程——人工智能(AI)。在我投身AI研究的25年中,图灵的愿景依然不断启发着我。但我们距离那一愿景有多近?答案并不简单。</p> <h3 id="12-当前ai的成就与局限">1.2 当前AI的成就与局限</h3> <p>今天,以大语言模型(LLM)为代表的前沿AI技术,已经开始改变人类获取与处理抽象知识的方式。</p> <p>它们展示了曾被认为不可能的能力:生成连贯的文本、成山的代码、逼真的图像,甚至短视频。AI是否会改变世界?——从任何合理的定义来看,它已经做到了。</p> <p><strong>然而,它们仍然是”黑暗中的文字匠”</strong>:</p> <ul> <li>能言善辩,却无经验</li> <li>知识丰富,却缺乏根基</li> <li>无法真正理解物理世界</li> </ul> <h3 id="13-为什么空间智能是下一个前沿">1.3 为什么空间智能是下一个前沿</h3> <p><strong>仍有大量潜能未被触及:</strong></p> <ul> <li>自动化机器人的愿景依然诱人却遥远</li> <li>疾病治疗、新材料发现、粒子物理等领域的研究加速梦仍未实现</li> <li>真正能够理解并赋能人类创作者的AI仍未到来</li> </ul> <p><strong>空间智能将改变一切:</strong></p> <p>空间智能(spatial intelligence)将改变我们创造和交互现实与虚拟世界的方式——彻底革新叙事、创造力、机器人学、科学发现,以及更多领域。这,正是AI的下一个前沿。</p> <h3 id="14-我的北极星">1.4 我的”北极星”</h3> <p>自我进入这一领域以来,对视觉与空间智能的探索就一直是我的”北极星”:</p> <ul> <li>构建了 <strong>ImageNet</strong>——首个大规模视觉学习与评测数据集</li> <li>与神经网络算法、现代算力(如GPU)一道,成为孕育现代AI的三大关键要素</li> <li>在斯坦福实验室将计算机视觉与机器人学习相结合</li> <li>与联合创始人创建 <strong>World Labs</strong>,第一次真正实现这一可能性</li> </ul> <p>在这篇文章中,我将解释:</p> <ol> <li>什么是空间智能</li> <li>为什么它重要</li> <li>如何构建能够解锁它的”世界模型”</li> </ol> <hr> <h2 id="二空间智能人类认知的脚手架">二、空间智能:人类认知的脚手架</h2> <h3 id="21-进化的基石感知-行动循环">2.1 进化的基石:感知-行动循环</h3> <p>视觉长期以来是人类智能的基石,但它的力量源自更为根本的东西。</p> <p><strong>进化的起点:</strong></p> <p>早在动物能筑巢、抚育后代、用语言交流或建立文明之前,那看似简单的”感知行为”:</p> <ul> <li>感受到一缕光</li> <li>触到一种质感</li> </ul> <p>就已经悄然点燃了通向智能的进化旅程。</p> <p><strong>从感知到智能的桥梁:</strong></p> <p>这种从外部世界汲取信息的能力,在感知与生存之间搭建起一座桥梁,而这一桥梁在漫长的进化中变得愈发复杂。</p> <p>神经元层层叠加,形成能解释世界、协调生物与环境互动的神经系统。因此,许多科学家认为,<strong>“感知—行动”循环成为了智能进化的核心机制</strong>,也是自然孕育出我们这个物种的根基——一个能感知、学习、思考与行动的终极存在。</p> <h3 id="22-日常生活中的空间智能">2.2 日常生活中的空间智能</h3> <p>空间智能在我们与物理世界的互动中扮演着基础性的角色。每天,我们都在依赖它完成最平常的动作:</p> <p><strong>日常场景示例:</strong></p> <ul> <li>停车时想象车尾与路缘的距离</li> <li>接住被抛来的钥匙</li> <li>在人群中穿行而不碰撞</li> <li>半睡半醒间准确地将咖啡倒进杯中</li> </ul> <p><strong>极端情况:</strong></p> <ul> <li>消防员穿行于坍塌建筑与浓烟之中,瞬间判断稳定性与生死抉择</li> <li>通过肢体语言和本能默契沟通——这些都无可言传</li> <li>婴儿则在学会语言前的漫长时光里,通过玩耍与环境互动来认识世界</li> </ul> <blockquote> <p><strong>这一切都在直觉中、自然而然地发生——一种机器至今未能获得的流畅能力。</strong></p> </blockquote> <h3 id="23-人类创造力的根基">2.3 人类创造力的根基</h3> <p>空间智能同样是我们想象力与创造力的基石。</p> <p><strong>从古至今的视觉媒介:</strong></p> <p>讲故事的人在脑中构建出丰富的世界,并用各种视觉媒介将之传达给他人:</p> <ul> <li>原始洞穴壁画</li> <li>现代电影</li> <li>沉浸式电子游戏</li> </ul> <p><strong>虚拟世界的构建:</strong></p> <p>无论是孩子在沙滩上筑城堡,还是在电脑上玩《我的世界》,这种以空间为根基的想象构成了人与虚拟世界交互体验的基础。</p> <p><strong>工业应用:</strong></p> <p>在工业应用中,对物体、场景与动态交互环境的模拟则支撑着:</p> <ul> <li>工业设计</li> <li>数字孪生</li> <li>机器人训练</li> </ul> <p>等无数关键场景。</p> <h3 id="24-推动文明进步的力量">2.4 推动文明进步的力量</h3> <p>历史上那些塑造文明的关键时刻中,空间智能往往扮演着核心角色。</p> <p><strong>埃拉托色尼测量地球周长(古希腊):</strong></p> <p>通过对阴影的几何化思考完成了惊人的壮举:</p> <ul> <li>在亚历山大测得太阳影子形成的7度角</li> <li>与赛恩(Syene)”正午无影”的现象进行对比</li> <li>从而计算出了地球的周长</li> </ul> <p><strong>哈格里夫斯的”珍妮纺纱机”:</strong></p> <p>源于空间洞察:</p> <ul> <li>意识到只需将多个纺锤并列安装在一个机架上</li> <li>一个工人就能同时纺出多股线</li> <li>生产效率因此提高了八倍</li> </ul> <p><strong>沃森与克里克揭示DNA结构:</strong></p> <p>依赖于他们亲手搭建的三维分子模型:</p> <ul> <li>用金属板与铁丝不断调整、拼接</li> <li>直到碱基对的空间排布完美契合</li> </ul> <blockquote> <p><strong>在这些案例中,空间智能都推动了文明的进步——当科学家与发明家需要操纵物体、想象结构、在物理空间中推理时,这些能力是纯文字永远无法承载的。</strong></p> </blockquote> <h3 id="25-当前ai的空间智能缺陷">2.5 当前AI的空间智能缺陷</h3> <p>虽然我们大多数人并不会每天像埃拉托色尼那样发现新的真理,但我们几乎时时刻刻都以同样的方式在思考:</p> <ul> <li>通过感官去理解这个复杂世界</li> <li>依托对物理与空间规律的直觉认知,使其变得可理解</li> </ul> <p><strong>遗憾的是,当今的AI还无法以这样的方式思考。</strong></p> <h4 id="多模态大语言模型的进展">多模态大语言模型的进展</h4> <p>过去几年确实取得了巨大进步。多模态大语言模型(MLLMs),在文本之外又引入了大量多媒体数据进行训练,初步具备了空间感知能力:</p> <ul> <li>可以分析图像、回答与之相关的问题</li> <li>甚至生成超写实的图像与短视频</li> </ul> <p>同时,借助传感器与触觉技术的突破,最先进的机器人已经能在严格受限的环境中开始操控物体与工具。</p> <h4 id="根本性局限">根本性局限</h4> <p>然而,坦率地说,AI的空间能力依然远未接近人类水平。其局限也显而易见:</p> <p><strong>基本空间理解的失败:</strong></p> <ul> <li>最先进的MLLM在估计距离、方向、大小等任务上,表现往往不比随机猜测好多少</li> <li>无法”心智旋转”物体——即从新角度再现同一对象的形状</li> <li>不会在迷宫中导航、识别捷径,或预测基本的物理规律</li> </ul> <p><strong>视频生成的连贯性问题:</strong></p> <ul> <li>生成的视频虽然新奇炫目,却常在几秒钟后失去连贯性</li> </ul> <p><strong>根本性的脱节:</strong></p> <p>如今的顶级AI擅长阅读、写作、检索与模式识别,但当涉及对物理世界的表征或交互时,却存在根本性局限。</p> <p>我们人类理解世界的方式是整体性的:</p> <ul> <li>不仅仅看到”眼前的东西”</li> <li>还理解它们在空间上的关系</li> <li>在语义上的意义</li> <li>以及在现实中的重要性</li> </ul> <blockquote> <p><strong>而这种通过想象、推理、创造与交互来理解世界的能力,正是空间智能的力量。</strong></p> </blockquote> <h4 id="缺乏空间智能的后果">缺乏空间智能的后果</h4> <p>缺乏它,AI就与它所试图理解的物理现实脱节。它将:</p> <ul> <li>无法真正安全地驾驶汽车</li> <li>无法在家庭与医院中引导机器人</li> <li>无法创造全新的沉浸式学习与娱乐体验</li> <li>也无法加速材料科学与医学的发现</li> </ul> <h4 id="超越语言的前沿">超越语言的前沿</h4> <p>哲学家维特根斯坦曾写道:<strong>“语言的边界就是我世界的边界”</strong>。</p> <p>我不是哲学家,但我知道,对AI而言,世界不止于语言。空间智能代表着超越语言的前沿。</p> <p>它连接想象、感知与行动,为机器真正提升人类生活打开了新的可能:</p> <ul> <li>从医疗到创造力</li> <li>从科学发现到日常辅助</li> </ul> <hr> <h2 id="三构建世界模型ai的下一个十年">三、构建世界模型:AI的下一个十年</h2> <h3 id="31-什么是世界模型">3.1 什么是世界模型</h3> <p>那么,我们该如何打造拥有空间智能的AI?</p> <p>如何让模型具备:</p> <ul> <li>像埃拉托色尼那样的空间推理能力</li> <li>像工业设计师那样的工程精度</li> <li>像讲故事的人那样的创造性想象力</li> <li>以及像应急救援人员那样与环境流畅互动的能力?</li> </ul> <p><strong>答案:世界模型(World Models)</strong></p> <p>要实现这样的AI,我们需要比LLM更具雄心的体系:世界模型(World Models)。</p> <p>这是一种全新的生成式模型,其在理解、推理、生成与交互方面的能力,将超越当今LLM所能触及的极限。它能够在语义、物理、几何与动态层面上,理解并生成复杂的虚拟或真实世界。</p> <p><strong>领域现状:</strong></p> <p>这一领域尚处于萌芽阶段,现有方法从抽象推理模型到视频生成系统不等。</p> <p>World Labs成立于2024年初,正是基于这样一种信念:<strong>基础性方法仍在形成之中,而这将成为未来十年人工智能的决定性挑战。</strong></p> <h3 id="32-世界模型的三大核心能力">3.2 世界模型的三大核心能力</h3> <p>在这个新兴领域中,最重要的是确立指导发展方向的核心原则。对于空间智能而言,我将”世界模型”定义为具备以下三项核心能力的系统:</p> <h4 id="能力1生成性generative">能力1:生成性(Generative)</h4> <p><strong>世界模型能够生成具有感知、几何与物理一致性的世界</strong></p> <p>要实现空间理解与推理,世界模型必须能够生成自身的模拟世界。</p> <p><strong>核心要求:</strong></p> <ul> <li>能在语义或感知指令的引导下,生成无限多样、变化丰富的虚拟世界</li> <li>同时保持几何、物理与动态上的一致性</li> <li>无论这些世界是现实的还是虚拟的</li> </ul> <p><strong>表征形式的探索:</strong></p> <p>研究界目前正在探索,这些世界应当以隐式(implicit)还是显式(explicit)的几何结构形式表示。</p> <p><strong>我的观点:</strong></p> <p>除了强大的潜在表征(latent representations)之外,我认为通用世界模型的输出还应当:</p> <ul> <li>允许生成显式、可观测的世界状态</li> <li>以便适应不同的应用场景</li> </ul> <p><strong>时间连贯性:</strong></p> <p>尤其重要的是,模型对当下世界的理解必须与其过去的状态保持连贯一致——<strong>理解当前,就是理解它是如何演化而来的。</strong></p> <h4 id="能力2多模态multimodal">能力2:多模态(Multimodal)</h4> <p><strong>世界模型在设计上就是多模态的</strong></p> <p>正如人类与动物一样,世界模型应能处理多种形式的输入。在生成式AI领域中,这些输入被称为”提示词(prompts)”。</p> <p><strong>多样化的输入:</strong></p> <p>面对不完整的信息——无论是:</p> <ul> <li>图像</li> <li>视频</li> <li>深度图</li> <li>文本指令</li> <li>手势</li> <li>还是动作</li> </ul> <p>世界模型都应能预测或生成尽可能完整的世界状态。</p> <p><strong>处理要求:</strong></p> <p>这要求模型:</p> <ul> <li>既要以真实视觉的精度处理图像输入</li> <li>又能以同样的灵活性理解语义性指令</li> </ul> <p><strong>交互方式:</strong></p> <p>如此一来,无论是智能体还是人类,都能:</p> <ul> <li>通过多样的输入形式与模型就”世界”进行交流</li> <li>以多样的方式接收输出</li> </ul> <h4 id="能力3交互性interactive">能力3:交互性(Interactive)</h4> <p><strong>世界模型能根据输入动作输出下一个状态</strong></p> <p>最后,当动作(actions)和/或目标(goals)作为输入提示的一部分时,世界模型的输出必须包含世界的下一个状态。</p> <p><strong>状态表征:</strong></p> <p>这一状态可以是隐式的,也可以是显式的。</p> <p><strong>一致性要求:</strong></p> <p>当输入仅包含一个动作(有无目标皆可)时,世界模型应能生成与以下内容相一致的输出:</p> <ul> <li>世界先前状态</li> <li>预期目标状态(如有)</li> <li>其语义意义</li> <li>物理规律</li> <li>动态行为</li> </ul> <p><strong>未来展望:</strong></p> <p>随着空间智能世界模型在推理与生成能力上不断增强,我们可以想象,未来模型不仅能预测世界的下一个状态,还将能够基于该状态预测下一步行动。</p> <h3 id="33-技术挑战与研究方向">3.3 技术挑战与研究方向</h3> <p>这一挑战的规模,超越了AI以往所面临的一切。</p> <h4 id="挑战的复杂性">挑战的复杂性</h4> <p>语言是人类认知中纯粹生成的现象,而”世界”遵循的规则则复杂得多。</p> <p><strong>在地球上,例如:</strong></p> <ul> <li>重力决定运动</li> <li>原子结构决定光的颜色与亮度</li> <li>无数物理定律约束着一切交互</li> </ul> <p><strong>即使是最奇幻、最具创造性的世界:</strong></p> <ul> <li>也由遵守物理与动态规律的空间对象与智能体构成</li> </ul> <p><strong>核心难题:</strong></p> <p>要在模型中一致地协调这些——语义、几何、动力学与物理层面——需要全新的方法论。因为”世界”的维度远比语言这种一维的序列信号复杂得多。</p> <h4 id="world-labs的研究方向">World Labs的研究方向</h4> <p>要实现像人类一样具备普适空间智能的世界模型,必须跨越若干巨大的技术壁垒。</p> <p>在World Labs,我们的研究团队正致力于这一目标的基础性突破。以下是我们当前研究的几个方向示例:</p> <p><strong>方向1:新的通用训练任务函数</strong></p> <p>在世界模型研究中,一个长期目标是定义一种像LLM中”下一个token预测”一样简洁优雅的通用任务函数。</p> <p><strong>挑战:</strong></p> <ul> <li>世界模型输入与输出空间的复杂性使这一函数的设计更加困难</li> </ul> <p><strong>要求:</strong></p> <ul> <li>这一目标函数及其对应表征必须符合几何与物理规律</li> <li>忠实体现世界模型在想象与现实之间的”落地表征”本质</li> </ul> <p><strong>方向2:大规模训练数据</strong></p> <p>训练世界模型所需的数据远比文本复杂。</p> <p><strong>好消息:</strong></p> <ul> <li>我们已经拥有了庞大的数据资源</li> <li>互联网上规模宏大的图像与视频集合为训练提供了丰富的素材</li> </ul> <p><strong>挑战:</strong></p> <ul> <li>如何让算法从二维图像或视频帧(RGB)中提取更深层次的空间信息</li> </ul> <p><strong>关键问题:</strong></p> <ul> <li>过去十年的研究揭示了语言模型中数据量与模型规模的scaling law</li> <li>对于世界模型,关键在于构建能够在相似规模上有效利用视觉数据的架构</li> </ul> <p><strong>补充数据源:</strong></p> <ul> <li>高质量的合成数据</li> <li>额外模态(如深度、触觉)</li> <li>在训练过程的关键阶段起到补充作用</li> </ul> <p><strong>未来发展取决于:</strong></p> <ul> <li>更先进的传感系统</li> <li>更稳健的信号提取算法</li> <li>以及更强大的神经仿真方法</li> </ul> <p><strong>方向3:新的模型架构与表征学习</strong></p> <p>世界模型研究将不可避免地推动模型架构与学习算法的革新,特别是超越当下的多模态LLM与视频扩散模型(video diffusion)。</p> <p><strong>当前模型的局限:</strong></p> <p>这些模型通常将数据编码为一维或二维序列,使得简单的空间任务变得异常困难:</p> <ul> <li>在短视频中数清不同的椅子</li> <li>记住一小时前房间的样子</li> </ul> <p><strong>新的架构思路:</strong></p> <p>或许能改进这一点,例如:</p> <ul> <li>具备3D或4D感知能力的token化</li> <li>上下文与记忆机制</li> </ul> <p><strong>World Labs的RTFM模型:</strong></p> <p>在World Labs,我们最近开发了一种基于帧的实时生成模型——<strong>RTFM(Real-Time Generative Frame-based Model)</strong>。</p> <p>它的特点:</p> <ul> <li>以空间为基础的帧(spatially-grounded frames)作为空间记忆形式</li> <li>实现了高效实时生成的同时</li> <li>保持了生成世界的持续性与一致性</li> </ul> <h3 id="34-world-labs的研究进展">3.4 World Labs的研究进展</h3> <p>显然,在完全释放空间智能的潜力之前,我们仍面临艰巨的挑战。但这项研究不仅仅是理论工作,<strong>它正成为新一代创造性与生产力工具的核心引擎</strong>。</p> <h4 id="marble首个世界模型产品">Marble:首个世界模型产品</h4> <p>在World Labs的进展令人鼓舞。我们最近向部分用户展示了 <strong>Marble</strong> 的早期版本:</p> <p><strong>Marble的特点:</strong></p> <ul> <li>全球首个可通过多模态输入生成并保持一致性3D环境的世界模型</li> <li>让用户与创作者能够探索、交互并在其中继续构建他们的创意世界</li> </ul> <p><strong>当前状态:</strong></p> <ul> <li>我们正全力以赴,努力尽快将其向公众开放</li> </ul> <h4 id="未来展望">未来展望</h4> <p>Marble只是我们的第一步。随着研究的加速,科研人员、工程师、用户与商业领袖们都开始意识到这一方向的巨大潜能。</p> <p><strong>下一代世界模型将:</strong></p> <ul> <li>使机器在空间智能上达到全新的层次</li> <li>开启AI迄今仍普遍缺乏的核心能力</li> <li>真正让人工智能进入理解与创造世界的时代</li> </ul> <hr> <h2 id="四用世界模型构建更美好的世界">四、用世界模型构建更美好的世界</h2> <h3 id="41-人工智能的发展动机">4.1 人工智能的发展动机</h3> <p>人工智能的发展动机至关重要。作为推动现代AI时代到来的科学家之一,我的动机始终十分明确:<strong>AI应当增强人类的能力,而非取而代之。</strong></p> <p>多年来,我一直致力于让AI的开发、部署与治理与人类需求保持一致。</p> <p><strong>务实的立场:</strong></p> <p>当下关于”技术乌托邦”与”世界末日”的极端叙事比比皆是,但我依然持一种更务实的立场:</p> <ul> <li>AI是由人开发、被人使用、并由人治理的</li> <li>它必须始终尊重人的自主性与尊严</li> <li>它的”魔力”在于拓展我们的能力,让我们变得更具创造力、更紧密相连、更高效并更有成就感</li> </ul> <p><strong>空间智能的愿景:</strong></p> <p>空间智能正体现了这一愿景——一种能赋能人类创造者、照护者、科学家与梦想家的AI,使他们实现曾经不可能的目标。</p> <blockquote> <p><strong>这一信念,正是我将空间智能视为AI下一个伟大前沿领域的根本原因。</strong></p> </blockquote> <h3 id="42-应用的时间尺度">4.2 应用的时间尺度</h3> <p>空间智能的应用横跨不同的时间尺度:</p> <p><strong>当下(短期):</strong></p> <ul> <li>创作工具正在当下出现</li> <li>World Labs的 Marble 已经让创作者与讲故事的人能够亲手掌握这种能力</li> </ul> <p><strong>中期:</strong></p> <ul> <li>机器人领域代表着中期的雄心目标</li> <li>我们正致力于完善感知与行动之间的闭环</li> </ul> <p><strong>长期:</strong></p> <ul> <li>最具变革意义的科学应用可能需要更长时间</li> <li>但它们将深刻地促进人类的福祉</li> </ul> <p><strong>集体努力的需要:</strong></p> <p>在所有时间线中,有几个领域的潜力尤其突出,足以重塑人类的能力。要实现这些潜力,需要集体努力:</p> <ul> <li>远超任何一个团队或公司的能力范围</li> <li>需要整个AI生态系统的参与:研究者、创新者、创业者、企业家,乃至政策制定者</li> <li>共同朝着一个愿景努力</li> </ul> <p>而这个愿景,值得我们追求。以下是未来的图景:</p> <h3 id="43-创造力为叙事注入超能力">4.3 创造力:为叙事注入超能力</h3> <blockquote> <p><strong>“创意,是智慧的乐趣。”</strong> ——爱因斯坦</p> </blockquote> <h4 id="故事的力量">故事的力量</h4> <p>在人类发明文字之前,我们就会讲故事:</p> <ul> <li>把故事画在洞穴壁上</li> <li>代代相传</li> <li>并以共享的叙事建立文化</li> </ul> <p>故事是人类:</p> <ul> <li>理解世界</li> <li>跨越时空连接彼此</li> <li>探索”人之为人”的方式</li> <li>也是我们在生活与爱中寻找意义的途径</li> </ul> <h4 id="彻底变革叙事方式">彻底变革叙事方式</h4> <p>今天,空间智能有潜力彻底变革我们创作与体验叙事的方式,从娱乐到教育,从设计到建造,赋予它们更深远的影响力。</p> <p><strong>Marble平台的能力:</strong></p> <p>World Labs的Marble平台将前所未有的空间表达能力与编辑控制权交到创作者手中:</p> <ul> <li>电影人</li> <li>游戏设计师</li> <li>建筑师</li> <li>及各类讲述者</li> </ul> <p><strong>无需传统3D设计软件的繁复流程:</strong></p> <ul> <li>快速创造、迭代、探索完整的三维世界</li> </ul> <p><strong>人类仍是核心:</strong></p> <ul> <li>创造的行为依然是人类的核心活动</li> <li>AI只是放大并加速创意实现的过程</li> </ul> <h4 id="三大应用方向">三大应用方向</h4> <p><strong>1. 多维叙事体验</strong></p> <p>电影人和游戏设计师可以利用 Marble 构建整个世界:</p> <ul> <li>不受预算或地理限制</li> <li>探索传统制作流程中无法实现的场景与视角</li> </ul> <p><strong>媒介融合:</strong></p> <p>随着媒介与娱乐的界限模糊化,我们正接近一种全新的互动体验形态:</p> <ul> <li>融合艺术、模拟与游戏的个性化世界</li> <li>让任何人(而不仅仅是大型工作室)都能创造并进入自己的故事</li> </ul> <p><strong>2. 以设计讲述空间故事</strong></p> <p>几乎所有被制造的物品或建造的空间,都必须在物理实现之前经过虚拟3D设计——这一过程往往耗费大量时间与成本。</p> <p><strong>空间智能的加速:</strong></p> <ul> <li>建筑师可以在数分钟内可视化并漫游尚不存在的建筑</li> <li>工业或时装设计师可以即时将想象转化为形态</li> <li>探索物体与人体及空间的交互</li> </ul> <p><strong>3. 全新的沉浸与互动体验</strong></p> <p>人类体验的最深层方式之一,就是创造意义的体验本身。</p> <p><strong>历史限制:</strong></p> <p>在整个人类历史上,我们只共享一个三维世界:物理世界。直到近几十年,通过游戏与早期虚拟现实(VR),我们才得以初步窥见”自造世界”的可能。</p> <p><strong>技术融合:</strong></p> <p>如今,空间智能结合VR、XR(扩展现实)头显与沉浸式显示设备,将这种体验提升到前所未有的高度。</p> <p><strong>未来愿景:</strong></p> <ul> <li>人们”走进”多维世界将如同打开一本书般自然</li> <li>空间智能让造世界的权力从专业团队扩展到每一位拥有愿景的创作者、教育者与普通人</li> </ul> <h3 id="44-机器人具身智能的实践">4.4 机器人:具身智能的实践</h3> <p>从昆虫到人类,动物都依赖空间智能来理解、导航并与世界交互。机器人也不会例外。</p> <h4 id="长久以来的梦想">长久以来的梦想</h4> <p>自该领域诞生以来,”具备空间感知的机器”就是人类的梦想,包括我在斯坦福研究实验室与学生、合作者共同进行的研究。</p> <blockquote> <p><strong>正因如此,我对用 World Labs 构建的模型实现这一愿景感到异常兴奋。</strong></p> </blockquote> <h4 id="三大应用方向-1">三大应用方向</h4> <p><strong>1. 通过世界模型扩展机器人学习</strong></p> <p>机器人的学习进步取决于可扩展的训练数据方案。</p> <p><strong>核心需求:</strong></p> <p>要让机器人具备理解、推理、规划与交互的能力,它们需要覆盖极为庞大的状态空间。</p> <p><strong>当前共识:</strong></p> <p>许多研究者认为,三者结合是实现可泛化机器人的关键:</p> <ul> <li>互联网数据</li> <li>合成仿真数据</li> <li>人类演示的真实采集</li> </ul> <p><strong>数据稀缺性:</strong></p> <p>然而,与语言模型不同,如今机器人的训练数据极为稀缺。</p> <p><strong>世界模型的作用:</strong></p> <p>世界模型将在此发挥决定性作用。随着其感知精度与计算效率的提高:</p> <ul> <li>世界模型生成的输出将迅速缩小模拟与现实之间的差距</li> <li>从而让机器人能在数不清的状态、互动与环境中学习</li> </ul> <p><strong>2. 人机协作伙伴</strong></p> <p>无论是:</p> <ul> <li>实验室中协助科学家的研究助理机器人</li> <li>还是陪伴独居老人的家用助理</li> </ul> <p>机器人都可以扩展劳动力并提升社会生产力。</p> <p><strong>核心要求:</strong></p> <p>但要做到这一点,机器人必须具备空间智能:</p> <ul> <li>能感知、推理、规划、行动</li> <li>并且最重要的是:保持对人类目标与行为的同理一致</li> </ul> <p><strong>应用示例:</strong></p> <ul> <li> <strong>实验室机器人</strong>:可以替代科学家完成仪器操作,让人专注于需要推理的部分</li> <li> <strong>家庭助理机器人</strong>:可以帮助老人做饭,而不剥夺他们的乐趣与自主性</li> </ul> <p><strong>关键能力:</strong></p> <p>真正具备空间智能的世界模型能够:</p> <ul> <li>预测下一个状态</li> <li>甚至推断与之匹配的下一步行动</li> <li>是实现这一愿景的关键</li> </ul> <p><strong>3. 扩展的具身形态</strong></p> <p>人形机器人只是我们为自身世界打造的一个形式。</p> <p><strong>真正的创新红利:</strong></p> <p>将来自更加多样的设计:</p> <ul> <li>输送药物的纳米机器人</li> <li>穿行狭窄空间的软体机器人</li> <li>以及为深海或外太空而造的机器</li> </ul> <p><strong>通用要求:</strong></p> <p>无论形态如何,未来的空间智能模型都必须将环境与机器人自身的感知、运动一体化建模。</p> <p><strong>关键挑战:</strong></p> <p>开发这些机器人面临的关键挑战在于:缺乏多样化形态的训练数据。</p> <p><strong>世界模型的作用:</strong></p> <p>世界模型将在这一过程中发挥关键作用:</p> <ul> <li>为仿真数据提供支持</li> <li>为训练环境提供支持</li> <li>为评测任务提供支持</li> </ul> <h3 id="45-更长远的地平线科学医疗与教育">4.5 更长远的地平线:科学、医疗与教育</h3> <p>除了创造性与机器人应用外,”空间智能”的深远影响还将延伸至更多能够增强人类能力、拯救生命、加速发现的领域。</p> <p>以下我将重点介绍三个具有深刻变革潜力的方向。当然,空间智能的应用远不止于此,它的影响范围几乎遍及所有行业。</p> <h4 id="1-科学研究模拟实验与假设验证">1. 科学研究:模拟实验与假设验证</h4> <p><strong>核心能力:</strong></p> <p>在科学研究中,具备空间智能的系统可以:</p> <ul> <li>模拟实验</li> <li>并行验证假设</li> <li>探索人类无法亲临的环境——从深海到遥远的行星</li> </ul> <p><strong>变革领域:</strong></p> <p>这项技术有望彻底变革:</p> <ul> <li>气候科学</li> <li>材料研究</li> <li>等领域的计算建模方式</li> </ul> <p><strong>关键优势:</strong></p> <p>通过将多维度模拟与真实世界数据采集相结合,这些工具能:</p> <ul> <li>显著降低计算壁垒</li> <li>拓展每一个实验室可观察与理解的边界</li> </ul> <h4 id="2-医疗领域从实验室到病床">2. 医疗领域:从实验室到病床</h4> <p>在医疗领域,空间智能将重塑从实验室到病床的全过程。</p> <p><strong>我的经验:</strong></p> <p>在斯坦福,我与学生及合作者多年来一直与医院、养老机构以及居家患者合作。这些经验让我深信空间智能在医疗领域的变革潜力。</p> <p><strong>三大应用方向:</strong></p> <p><strong>药物研发:</strong></p> <ul> <li>通过多维建模加速药物研发</li> </ul> <p><strong>诊断辅助:</strong></p> <ul> <li>通过辅助放射科医生识别影像中的模式来提升诊断质量</li> </ul> <p><strong>环境感知式监护:</strong></p> <ul> <li>支持环境感知式监护系统</li> <li>在不取代人类关怀的前提下</li> <li>为患者与护理人员提供持续支持</li> </ul> <p><strong>机器人辅助:</strong></p> <ul> <li>更不用说机器人在不同场景中帮助医护人员和患者的巨大潜力</li> </ul> <h4 id="3-教育领域沉浸式学习体验">3. 教育领域:沉浸式学习体验</h4> <p>空间智能能够实现沉浸式学习:</p> <ul> <li>让抽象或复杂的概念变得可感知</li> <li>创造出符合人类大脑与身体学习方式的迭代体验</li> </ul> <p><strong>AI时代的学习需求:</strong></p> <p>在AI时代,更快速、更高效的学习与技能重塑对于儿童与成人都至关重要。</p> <p><strong>三大应用场景:</strong></p> <p><strong>学生:</strong></p> <ul> <li>可以以多维方式探索细胞机器</li> <li>或”亲历”历史事件</li> </ul> <p><strong>教师:</strong></p> <ul> <li>可借助互动环境进行个性化教学</li> </ul> <p><strong>专业人士:</strong></p> <ul> <li>外科医生、工程师等专业人士则能在高度逼真的仿真环境中安全地练习复杂技能</li> </ul> <h4 id="统一的目标">统一的目标</h4> <p>跨越这些领域,可能性是无限的,但目标始终如一:</p> <blockquote> <p><strong>让AI成为增强人类专长、加速人类发现、放大人类关怀的力量——而不是取代那份属于人的判断力、创造力与共情力。</strong></p> </blockquote> <hr> <h2 id="五结语构建与世界契合的智能">五、结语:构建与世界契合的智能</h2> <h3 id="51-ai成为全球现象">5.1 AI成为全球现象</h3> <p>过去十年间,人工智能已成为全球现象,在科技、经济乃至地缘政治层面都带来了转折。</p> <h3 id="52-图灵精神的延续">5.2 图灵精神的延续</h3> <p>然而,作为一名研究者、教育者和创业者,最令我振奋的仍是图灵七十五年前那道问题背后的精神。</p> <blockquote> <p><strong>我依然与他共享那份好奇与惊叹——正是这份好奇,让我每天都为探索空间智能的挑战而充满动力。</strong></p> </blockquote> <h3 id="53-历史性的时刻">5.3 历史性的时刻</h3> <p>人类历史上第一次,我们正站在这样一个时刻:<strong>有望构建出与物理世界高度契合的机器,让它们成为我们应对重大挑战的真正伙伴。</strong></p> <p><strong>应用前景:</strong></p> <p>无论是:</p> <ul> <li>加速疾病研究</li> <li>革新故事叙述方式</li> <li>还是在病痛、受伤或衰老的脆弱时刻给予支持</li> </ul> <p>我们都正处于一场技术变革的门槛上,它将提升我们最珍视的生命价值。</p> <h3 id="54-关于生活的愿景">5.4 关于生活的愿景</h3> <blockquote> <p><strong>这是一个关于更深刻、更丰富、更有力量的生活的愿景。</strong></p> </blockquote> <p>距自然在原始动物中首次显现空间智能的曙光已近五亿年,而我们有幸成为这一代技术创造者:</p> <ul> <li>可能即将赋予机器同样能力的人类</li> <li>也有幸能将此能力用于全人类的福祉</li> </ul> <h3 id="55-我的北极星">5.5 我的”北极星”</h3> <p>若没有空间智能,我们关于”真正智能机器”的梦想将永远不完整。</p> <blockquote> <p><strong>这场探索,是我的”北极星”。邀请你一同追寻它。</strong></p> </blockquote> <hr> <p><strong>作者简介:</strong> 李飞飞(Fei-Fei Li)是斯坦福大学教授、ImageNet创始人、World Labs联合创始人。她是计算机视觉和人工智能领域的先驱,致力于构建具备空间智能的AI系统。</p> <p><strong>来源:</strong> World Labs 官方博客 <strong>整理时间:</strong> 2025年11月10日</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">相关文章推荐：</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/2025-01-15-xiaohanding-paper-suggestion/"></a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/anthropic-claude-agent-methodology/">Anthropic 研究员详解：构建高效 Claude 智能体的完整方法论</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/claude-code-knowledge-management/">Claude Code自定义命令在知识管理与内容创作中的系统化应用研究</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/habit/">18个改变人生的习惯：科学证据支持的长期主义指南</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/openai-gpt5-researcher-vision/">OpenAI双巨头首次详解GPT-5：不是下一代GPT，终极形态是AI研究员</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/kimi-yang-dialogue/">KIMI创始人杨植麟深度访谈：攀登无限之山</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jason-wei-ai-insights/">Jason Wei：理解2025年AI进展的三种关键思路</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/anthropic-pretraining-nick-joseph/">Nick Joseph访谈：Anthropic预训练的核心思考与实践</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/xuyangsheng-CUHK-SZ/">徐扬生院士：人工智能时代的教育</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/karpathy-agent-ten-years/">Andrej Karpathy深度对话：Agent的十年征程与AI的幽灵本质</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Chao Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-\u4e2d\u6587\u7b80\u4ecb",title:"\u4e2d\u6587\u7b80\u4ecb",description:"",section:"Navigation",handler:()=>{window.location.href="/chinese/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-",title:"",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/2025-01-15-xiaohanding-paper-suggestion/"}},{id:"post-\u674e\u98de\u98de-\u4ece\u6587\u5b57\u5230\u4e16\u754c-\u7a7a\u95f4\u667a\u80fd\u662fai\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf",title:"\u674e\u98de\u98de:\u4ece\u6587\u5b57\u5230\u4e16\u754c,\u7a7a\u95f4\u667a\u80fd\u662fAI\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/drfeifei-spatial-intelligence/"}},{id:"post-anthropic-\u7814\u7a76\u5458\u8be6\u89e3-\u6784\u5efa\u9ad8\u6548-claude-\u667a\u80fd\u4f53\u7684\u5b8c\u6574\u65b9\u6cd5\u8bba",title:"Anthropic \u7814\u7a76\u5458\u8be6\u89e3\uff1a\u6784\u5efa\u9ad8\u6548 Claude \u667a\u80fd\u4f53\u7684\u5b8c\u6574\u65b9\u6cd5\u8bba",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/anthropic-claude-agent-methodology/"}},{id:"post-claude-code\u81ea\u5b9a\u4e49\u547d\u4ee4\u5728\u77e5\u8bc6\u7ba1\u7406\u4e0e\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u7cfb\u7edf\u5316\u5e94\u7528\u7814\u7a76",title:"Claude Code\u81ea\u5b9a\u4e49\u547d\u4ee4\u5728\u77e5\u8bc6\u7ba1\u7406\u4e0e\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u7cfb\u7edf\u5316\u5e94\u7528\u7814\u7a76",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/claude-code-knowledge-management/"}},{id:"post-18\u4e2a\u6539\u53d8\u4eba\u751f\u7684\u4e60\u60ef-\u79d1\u5b66\u8bc1\u636e\u652f\u6301\u7684\u957f\u671f\u4e3b\u4e49\u6307\u5357",title:"18\u4e2a\u6539\u53d8\u4eba\u751f\u7684\u4e60\u60ef\uff1a\u79d1\u5b66\u8bc1\u636e\u652f\u6301\u7684\u957f\u671f\u4e3b\u4e49\u6307\u5357",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/habit/"}},{id:"post-openai\u53cc\u5de8\u5934\u9996\u6b21\u8be6\u89e3gpt-5-\u4e0d\u662f\u4e0b\u4e00\u4ee3gpt-\u7ec8\u6781\u5f62\u6001\u662fai\u7814\u7a76\u5458",title:"OpenAI\u53cc\u5de8\u5934\u9996\u6b21\u8be6\u89e3GPT-5\uff1a\u4e0d\u662f\u4e0b\u4e00\u4ee3GPT\uff0c\u7ec8\u6781\u5f62\u6001\u662fAI\u7814\u7a76\u5458",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/openai-gpt5-researcher-vision/"}},{id:"post-kimi\u521b\u59cb\u4eba\u6768\u690d\u9e9f\u6df1\u5ea6\u8bbf\u8c08-\u6500\u767b\u65e0\u9650\u4e4b\u5c71",title:"KIMI\u521b\u59cb\u4eba\u6768\u690d\u9e9f\u6df1\u5ea6\u8bbf\u8c08\uff1a\u6500\u767b\u65e0\u9650\u4e4b\u5c71",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/kimi-yang-dialogue/"}},{id:"post-jason-wei-\u7406\u89e32025\u5e74ai\u8fdb\u5c55\u7684\u4e09\u79cd\u5173\u952e\u601d\u8def",title:"Jason Wei\uff1a\u7406\u89e32025\u5e74AI\u8fdb\u5c55\u7684\u4e09\u79cd\u5173\u952e\u601d\u8def",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/jason-wei-ai-insights/"}},{id:"post-nick-joseph\u8bbf\u8c08-anthropic\u9884\u8bad\u7ec3\u7684\u6838\u5fc3\u601d\u8003\u4e0e\u5b9e\u8df5",title:"Nick Joseph\u8bbf\u8c08\uff1aAnthropic\u9884\u8bad\u7ec3\u7684\u6838\u5fc3\u601d\u8003\u4e0e\u5b9e\u8df5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/anthropic-pretraining-nick-joseph/"}},{id:"post-\u5f90\u626c\u751f\u9662\u58eb-\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\u7684\u6559\u80b2",title:"\u5f90\u626c\u751f\u9662\u58eb\uff1a\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\u7684\u6559\u80b2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/xuyangsheng-CUHK-SZ/"}},{id:"post-andrej-karpathy\u6df1\u5ea6\u5bf9\u8bdd-agent\u7684\u5341\u5e74\u5f81\u7a0b\u4e0eai\u7684\u5e7d\u7075\u672c\u8d28",title:"Andrej Karpathy\u6df1\u5ea6\u5bf9\u8bdd\uff1aAgent\u7684\u5341\u5e74\u5f81\u7a0b\u4e0eAI\u7684\u5e7d\u7075\u672c\u8d28",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/karpathy-agent-ten-years/"}},{id:"post-sutton-\u5927\u8bed\u8a00\u6a21\u578b\u8d70\u9519\u4e86\u8def-\u4e0d\u7b26\u5408-\u82e6\u6da9\u6559\u8bad-\u7cbe\u795e",title:"Sutton\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u8d70\u9519\u4e86\u8def\uff0c\u4e0d\u7b26\u5408\u300c\u82e6\u6da9\u6559\u8bad\u300d\u7cbe\u795e",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/sutton-llm-is-wrong-way/"}},{id:"post-patrick-hsu-evo2\u534e\u4eba\u79d1\u5b66\u5bb6-\u865a\u62df\u7ec6\u80de\u8fc8\u5411gpt-2\u9636\u6bb5-\u5408\u6210\u751f\u7269\u5b66\u5c06\u6df1\u523b\u6539\u53d8\u4e16\u754c",title:"Patrick Hsu-Evo2\u534e\u4eba\u79d1\u5b66\u5bb6\uff1a\u865a\u62df\u7ec6\u80de\u8fc8\u5411GPT-2\u9636\u6bb5\uff0c\u5408\u6210\u751f\u7269\u5b66\u5c06\u6df1\u523b\u6539\u53d8\u4e16\u754c",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/patrick-hsu-evo2/"}},{id:"post-sam-altman-\u4e30\u6c9b\u667a\u80fd-\u6df1\u5ea6\u89e3\u6790-ai\u57fa\u7840\u8bbe\u65bd\u7684\u5b8f\u5927\u613f\u666f\u4e0e\u6218\u7565\u8def\u5f84",title:"Sam Altman\u300a\u4e30\u6c9b\u667a\u80fd\u300b\u6df1\u5ea6\u89e3\u6790:AI\u57fa\u7840\u8bbe\u65bd\u7684\u5b8f\u5927\u613f\u666f\u4e0e\u6218\u7565\u8def\u5f84",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/sam-altman-abundant-intelligence/"}},{id:"post-anthropic\u5185\u90e8ai\u4ee3\u7801\u9769\u547d-\u73b0\u5b9e\u8fd8\u662f\u7092\u4f5c-\u5f00\u53d1\u8005\u793e\u533a\u7684\u6df1\u5ea6\u8d28\u7591",title:"Anthropic\u5185\u90e8AI\u4ee3\u7801\u9769\u547d\uff1a\u73b0\u5b9e\u8fd8\u662f\u7092\u4f5c\uff1f\u5f00\u53d1\u8005\u793e\u533a\u7684\u6df1\u5ea6\u8d28\u7591",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/claude-code-sofaware/"}},{id:"post-\u59da\u987a\u96e8-ai\u4e0eagent\u7814\u7a76\u89c2\u70b9\u96c6",title:"\u59da\u987a\u96e8\uff1aAI\u4e0eAgent\u7814\u7a76\u89c2\u70b9\u96c6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yao-shunyu-agent-research/"}},{id:"post-\u59da\u987a\u96e8ai\u4e0eagent\u7814\u7a76\u89c2\u70b9\u96c6",title:"\u59da\u987a\u96e8AI\u4e0eAgent\u7814\u7a76\u89c2\u70b9\u96c6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yao-shunyu-ai-agent-insights/"}},{id:"post-\u6768\u690d\u9e9f\u89c2\u70b9\u7cbe\u534e-ai\u65f6\u4ee3\u7684\u6280\u672f\u54f2\u5b66\u4e0e\u5b9e\u8df5",title:"\u6768\u690d\u9e9f\u89c2\u70b9\u7cbe\u534e\uff1aAI\u65f6\u4ee3\u7684\u6280\u672f\u54f2\u5b66\u4e0e\u5b9e\u8df5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yang-zhilin-ai-philosophy/"}},{id:"post-openai\u59da\u987a\u96e8\u6df1\u5ea6\u8bbf\u8c08-ai\u4e0b\u534a\u573a\u7684agent\u9769\u547d",title:"OpenAI\u59da\u987a\u96e8\u6df1\u5ea6\u8bbf\u8c08\uff1aAI\u4e0b\u534a\u573a\u7684Agent\u9769\u547d",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/openai-yao-dialogue/"}},{id:"post-deepmind\u79d1\u5b66\u8d1f\u8d23\u4eba\u6df1\u5ea6\u8bbf\u8c08-\u5982\u4f55\u7b5b\u9009\u5e76\u653b\u514b\u53d8\u9769\u6027\u6311\u6218",title:"DeepMind\u79d1\u5b66\u8d1f\u8d23\u4eba\u6df1\u5ea6\u8bbf\u8c08\uff1a\u5982\u4f55\u7b5b\u9009\u5e76\u653b\u514b\u53d8\u9769\u6027\u6311\u6218",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/deepmind-kohli-dialogue/"}},{id:"post-church-nature-ai\u9a71\u52a8\u86cb\u767d\u8d28\u8bbe\u8ba1-\u9769\u547d\u6027\u8303\u5f0f\u7684\u5168\u6d41\u7a0b\u89e3\u6790",title:"Church @ Nature\uff1aAI\u9a71\u52a8\u86cb\u767d\u8d28\u8bbe\u8ba1\uff0c\u9769\u547d\u6027\u8303\u5f0f\u7684\u5168\u6d41\u7a0b\u89e3\u6790",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/nature-ai-driven-protein-design/"}},{id:"news-one-offline-rl-paper-accepted-by-aaai-2024-sparkles-sparkles",title:'One Offline RL Paper accepted by **AAAI 2024**. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-two-papers-llama-excitor-videodistill-are-accepted-by-cvpr-2024-sparkles-sparkles",title:'Two Papers(LLaMA-Excitor, VideoDistill) are accepted by **CVPR 2024**. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-one-llm-safety-survey-paper-accepted-by-naacl-2024-sparkles-smile",title:'One LLM safety survey paper accepted by **NAACL 2024**. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-one-paper-accepted-by-ijcai-2024-survey-track-sparkles-sparkles",title:'One Paper accepted by **IJCAI 2024** Survey Track. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-robocodex-is-accepted-by-icml-2024-sparkles-sparkles",title:'RoboCodeX is accepted by **ICML 2024**. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-three-papers-emulated-disalignment-structured-reasoning-multi-objective-dpo-are-accepted-by-acl-2024-sparkles-sparkles",title:'Three Papers (Emulated Disalignment, Structured Reasoning, Multi-Objective DPO) are accepted by **ACL 2024**.<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">...',description:"",section:"News"},{id:"news-mm-safetybench-a-benchmark-for-safety-evaluation-of-multimodal-large-language-models-is-accepted-by-eccv-2024-project-page-https-isxinliu-github-io-project-mm-safetybench-sparkles-sparkles",title:"MM-SafetyBench (A Benchmark for Safety Evaluation of Multimodal Large Language Models) is accepted...",description:"",section:"News"},{id:"news-inference-time-language-model-alignment-via-integrated-value-guidance-is-accepted-by-emnlp-2024-arixv-link-https-arxiv-org-pdf-2409-17819-sparkles-sparkles",title:"Inference-Time Language Model Alignment via Integrated Value Guidance is accepted by **EMNLP 2024**....",description:"",section:"News"},{id:"news-weak-to-strong-search-align-large-language-models-via-searching-over-small-language-models-is-accepted-by-neurips-2024-neurips-link-https-proceedings-neurips-cc-paper-files-paper-2024-file-088d99765bc121c6df215da7d45bc4e9-paper-conference-pdf-sparkles-sparkles",title:"Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models is...",description:"",section:"News"},{id:"news-we-proposal-a-new-law-ai-45-law-toward-trustworthy-agi-arxiv-link-https-arxiv-org-abs-2412-14186-sparkles",title:'We proposal a new law, **AI 45\xb0-Law** toward trustworthy AGI! [Arxiv Link](https://arxiv.org/abs/2412.14186) <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">...',description:"",section:"News"},{id:"news-icml2025-emergent-response-planning-in-llm-https-arxiv-org-abs-2502-06258-and-c-3po-compact-plug-and-play-proxy-optimization-to-achieve-human-like-retrieval-augmented-generation-https-arxiv-org-abs-2502-06205-are-accepted-by-icml2025-sparkles",title:"[**ICML2025**] [Emergent Response Planning in LLM](https://arxiv.org/abs/2502.06258) and [C-3PO: Compact Plug-and-Play Proxy Optimization to...",description:"",section:"News"},{id:"news-our-paper-adversarial-preference-learning-for-robust-llm-alignment-is-accepted-by-acl2025-arxiv-link-https-arxiv-org-abs-2505-24369-sparkles",title:"Our paper Adversarial Preference Learning for Robust LLM Alignment is accepted by **ACL2025**....",description:"",section:"News"},{id:"news-we-find-patches-from-harmful-content-enabling-them-to-bypass-data-moderation-and-generate-dangerous-responses-when-encountering-the-full-image-or-related-text-vlms-can-aggregate-scattered-training-patchesk-https-arxiv-org-abs-2506-03614-sparkles",title:"We find patches from harmful content, enabling them to bypass data moderation and...",description:"",section:"News"},{id:"news-big-project-release-we-introduce-safework-r1-a-cutting-edge-multimodal-reasoning-model-that-demonstrates-the-coevolution-of-capabilities-and-safety-safework-r1-https-arxiv-org-abs-2507-18576-rocket-sparkles",title:"\ud83c\udf89 **Big Project Release!** We introduce **SafeWork-R1**, a cutting-edge multimodal reasoning model that...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-\u674e\u98de\u98de-\u7a7a\u95f4\u667a\u80fd\u662fai\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf",title:"\u674e\u98de\u98de:\u7a7a\u95f4\u667a\u80fd\u662fAI\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf",description:"\u4ece\u6587\u5b57\u5230\u4e16\u754c,World Labs\u5f00\u542f\u7a7a\u95f4\u667a\u80fd\u65b0\u65f6\u4ee3",section:"Projects",handler:()=>{window.location.href="/projects/2_spatial_intelligence/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%61%6E%67%63%68%61%6F [%41%54] %70%6A%6C%61%62 [%44%4F%54] %6F%72%67 [%44%4F%54] %63%6E","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=https://scholar.google.com/citations?hl=en&user=5KRbHPMAAAAJ&view_op=list_works&sortby=pubdate","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emigmo","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/00/5867-26.html","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@chaoyang4587","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>