# 目录

- [01 序](#01-序)
  - [1.1 人：“我一直有这个非共识，我想要去做Agent”](#11-人一直有这个非共识我想要去做agent)
- [02 系统](#02-系统)
  - [2.1 机器的手：“人最重要的affordance是手，AI呢？”](#21-机器的手人最重要的affordance是手ai呢)
  - [2.2 任务的设定：“我们对简单任务的robustness没有重视”](#22-任务的设定我们对简单任务的robustness没有重视)
  - [2.3 泛化的工具：“语言是人为了泛化而发明出来的工具”](#23-泛化的工具语言是人为了泛化而发明出来的工具)
  - [2.4 奖励的机制：“当AI玩一个语言游戏，要怎么定义内在激励？”](#24-奖励的机制当ai玩一个语言游戏要怎么定义内在激励)
- [03 吞噬的边界](#03-吞噬的边界)
  - [3.1 双刃剑，“创业公司最大机会是：设计不同的interface”](#31-双刃剑创业公司最大机会是设计不同的interface)
  - [3.2 对Agent创业者的思索：“这世界是相互抄的关系，而不是单向抄的关系”](#32-对agent创业者的思索这世界是相互抄的关系而不是单向抄的关系)
  - [3.3 既单极又多元的世界：“这个世界不是单方压倒另一方，双方都有自己的力量”](#33-既单极又多元的世界这个世界不是单方压倒另一方双方都有自己的力量)
  - [3.4 环境是记忆层级中最外层的部分，“这很哲学”。](#34-环境是记忆层级中最外层的部分这很哲学)
  - [3.5 Chatbot系统会演化成Agent系统：“人和Agent交互的方式是什么样？”](#35-chatbot系统会演化成agent系统人和agent交互的方式是什么样)
- [04 人类的全局](#04-人类的全局)
  - [4.1 人与系统，Agent要不要像人？“是一个效用问题”](#41-人与系统agent要不要像人是一个效用问题)
  - [4.2 OpenAI的抉择时刻：“如果你没有different bet，很难超越前面的霸主”](#42-openai的抉择时刻如果你没有different bet很难超越前面的霸主)
  - [4.3 假若你是一个CEO：“首先我肯定会学习”](#43-假若你是一个ceo首先我肯定会学习)
- [05 快问快答式结尾](#05-快问快答式结尾)

## 01 序

### 1.1 人："我一直有这个非共识，我想要去做Agent"

张小珺：我们今天的嘉宾是OpenAI姚顺雨，他的研究方向是Agent。前段时间顺雨写了一篇有名的博文《The Second Half》，告诉大家AI游戏已进入下半场。

这次节目我们第一次尝试有两位主持人，除了我还有大家熟悉的广密。

顺雨，我看了你的资料和你写的文字，从你的语言里读到一种反叛精神，我对你这个人很感兴趣。你能不能先给大家做一个自我介绍，聊聊你的经历？

姚顺雨：你说反叛精神？这很有意思。

我感觉我是个非常乖的学生。从小到大就是按部就班的学习。

本科从合肥考到清华，读姚班。在姚班大家会告诉你去美国读PhD，我就去美国读PhD，我在普林斯顿读PhD。读PhD之后很自然，OpenAI是做research（研究）最好的地方，就加入OpenAI——感觉我前28年的人生，非常的乖。

张小珺：你是15-19年在清华姚班，19-24年在Princeton，24年毕业进OpenAI。你在本科学的不是AI，是怎么进入AI领域，继而又进入Agent领域？

姚顺雨：姚班的传统偏理论计算机科学，但我可能有一点反叛精神吧。

当时，我觉得很多重要理论问题已经解决得差不多，比如将某个图算法的复杂度从n的2.83次方优化到n的2.82次方，这种改进在现实中意义不大。

我在2016年上李建老师的一门课，看到一个multi-modal embedding（多模态嵌入）的demo，展示了embedding（向量表示/嵌入）一个非常神奇的例子：比如用"king"的embedding减去"man"，再加上"woman"，结果接近"queen"的embedding——这让我第一次意识到，深度学习在语义表示上居然能做到这么惊艳的计算。

"king（国王）"的向量 − "man（男人）"的向量 + "woman（女人）"的向量 ≈ "queen（王后）"的向量。
当时清华，尤其姚班，在Deep Learning（深度学习）的老师和资源还比较有限。2018年，我按照姚班传统去海外交流，去了MIT，师从吴佳俊学长，我才真正系统性开始做Deep Learning。

最初我做的是Computer Vision（计算机视觉），但渐渐意识到Vision很难实现通用人工智能。我的直觉告诉我：Language是一个更核心、更有潜力的方向，于是读博后转向语言模型研究。

张小珺：你是怎么进入Agent方向的？

姚顺雨：也算是某种机缘巧合吧。我的导师之前做过一些研究，探讨怎么在一个简单的语言游戏环境中训练智能体（Agent）。大概2016或2017年的工作。

那个项目是用一个基础RNN模型，在一个很小规模的文字游戏里，训练模型进行一些简单动态交互。比如，模型可以学会，“过桥之后就可以到河对岸”——这样简单的常识或逻辑推理。

我读博，本来是被计算机视觉（Computer Vision）录取，但我已经不太想做视觉了，主动去找语言（NLP）老师聊。

我遇到现在的导师Karthik Narasimhan（普林斯顿计算机科学副教授），开始一起头脑风暴项目点子。我当时说：现在的语言模型，比如GPT-2，已经比你们当年用的模型强太多，它们玩游戏是不是表现也会更好？

他说，maybe that's a good idea。我们就开始做了。

从那以后，我就一直做智能体相关工作，到现在6年了。

张小珺：Agent或Language最吸引你的是什么？

姚顺雨：是它的可泛化性（generalizable）。绝大多数事，你都可以用语言表达。

我当时隐隐约约有个直觉：你如果真想去实现AGI（通用人工智能）——那时还没人提“AGI”这个词——但如果你真的想做一个非常通用的系统（general system），你就得去构建一个智能体。

回头看AI历史，很久很久以前，从Herbert Simon（赫伯特·西蒙）在1960年代开始，大家最早的想法就是要做一个Agent。当时大家的野心很大——想用一个夏天搞定视觉，再用另一个夏天搞定语言，拼在一起，去做一个Agent，他就应该比人还聪明。

但这事太难了。慢慢地，AI变得非常碎片化。大家研究的问题越来越小。比如，有的人研究视觉一小部分问题，有的人研究语言某个子任务，越来越细分，越来越垂直。

但到2015年之后，开始出现Scaling Law（扩展规律），包括很多研究突破，历史上一些关键时刻也在提示我们：也许我们应该从这种“垂直式思维（vertical thinking）”重新回到更“通用式思维（general thinking）”，再去尝试构建真正通用的系统。

张小珺：当你进入Agent系统做研究，要让语言模型真正行动起来，你意识到最重要的几件事是什么？

姚顺雨：第一年最大收获是：要用GPT，不要用BERT。

BERT：“来自Transformer的双向编码表示”，由Google AI在2018年发布的一种NLP预训练模型。

可能现在很多人不知道BERT，当时语言领域最火的模型叫BERT。想法是：我有一句话，通过某种方式学到这句话的一个表示，通过这个表示做很多下游任务，比如做一些单选题，或者基于选择的任务。

当时95%的人做BERT，只有5%的人做GPT。这也是因为当时NLP的主要任务都是一些：我有一句话，这句话是积极的还是不积极的；我很讨厌这个电影，这是一个负面的句子。都是非常简单的事。这种事BERT确实效果更好。

但你会发现，如果你要做一个language Agent，你需要的不只是选择能力，而是去自由产生新动作的能力。

当然如果你在玩围棋，或者视频游戏，选择有限。如果你玩马里奥兄弟，他就是上、下、左、右。但如果你玩基于语言的游戏，动作是自由的。比如我在这个游戏可以用剑杀怪兽，或者我可以去第三个房间，或者我可以用金色钥匙打开第一个房间的门。BERT永远做不到。

世界的本质就是，你的行为空间是open-ended（开放）的，这种在开放空间决策的能力BERT永远做不到。我发现这个之后，就再也没用过BERT。

第二个learning是：任务或环境非常重要。

当你有一个非常差的任务，你永远不可能学到非常好的东西。当时有很多人在做：这个句子是正面的还是负面的？a这句话能不能导致b这句话成立？当时这些任务看上去很难，现在看非常简单。

首先你要找一个足够有挑战的任务，这个任务能做出本质的新方法。当时你想去做Agent或语言Agent，实际上没什么选择，只能去做文字游戏。

Zork是个非常经典的文字游戏。你在一个基于文字的世界里，有点像一个互动脚本，可以往下走、往上走，可以去各个房间，可以做各种各样的事。

但你会发现，这个环境还是有很多缺陷，能学到的局限在这个环境，这个环境还是不够大。而且你如果用RL学这个环境，就会像用RL学传统的视频游戏，可以把这个游戏打通关，但对于其他任务没有迁移作用。你可以把围棋下得特别好，但对世界上其他事情没有价值。

我们需要一个更好的环境。

张小珺：你博士期间的研究工作：语言智能体（Language Agent）、ReAct（浏览维基百科进行推理）、Reflextion（反思）、Tree of Thoughts（思维树）、digital automation（数字自动化）、WebShop（网上购物）——这些研究跨度很大，它们的共性问题是什么？你是怎么按着兴趣一步一步延伸的？

姚顺雨：从我的角度，是非常自然的过程。当我意识到环境有问题，我第一个比较重要的工作是WebShop，首先要解决环境问题。如果没有一个好的任务或环境，把这个游戏刷得再高，没有意义。

2015年有一个非常好的工作叫World of Bits（比特世界）。当时想法是，我们应该把电脑或互联网作为一个环境，这个环境比游戏更exciting（令人兴奋）。但因为各种技术限制，没有做得特别好。到2021年，我和导师讨论，觉得这时可能是一个自然的时间点重新去做。

我当时也觉得，技术还没完全成熟，大多数人还在研究一些比较标准的任务：a能不能导致b，或者翻译，或者从一篇文章回答问题。那个阶段想做互联网上的Agent，技术还没ready（准备好）。但也正因为技术没成熟，反而是一个好的时间点开始做。到2022年，我们就做了WebShop这个环境。

2022年，GPT-3.5发布，还有后来Chain of Thought（思维链）出现，带来新的方法层面上的机会。我们就做了ReAct这个工作。我现在还是觉得，我自己最喜欢的工作是ReAct。

ReAct: Synergizing Reasoning and Acting in Language Models，在语言模型中协同推理与行动，是一种让大语言模型在与外部环境交互时，同时进行“推理”和“行动”的方法框架。

之后，基于这两个方向：一方面做更多方法（method），一方面做更多任务（task）。

但总体来说，我的研究有两个核心：

一是怎么去做一些有价值、和现实世界更相关的任务和环境；
二是怎么去做一些简单、但又通用的方法。
张小珺：ReAct的提出标志了范式的变化吗？

姚顺雨：这需要5年或10年以后再去看。

当时学术界还不太能接受，我去做一个prompting（提示工程），把它作为research（研究）。传统意义上，你需要提出一些fancy（花哨）的东西——需要提出一些数学公式，训练一个模型，证明很多理论，或者做很多工程上的事。但如果你只是去用一个模型，感觉太软了。

不过，当时最有价值的，就是去研究怎么使用模型。如果你想训练模型，会落后OpenAI或这些公司好几年。你做的很有可能几年前别人已经发现了。如果你想做不一样的，可能怎么去使用模型更有价值。

张小珺：为什么你做这件事情比大部分人都早？

姚顺雨：有幸运的部分，我PhD做的第一个事就是基于语言模型做Agent。当时做的人很少，因为它太难了，或者不是一个共识类的事情。当时共识类任务是做问答，做翻译，或者做一些已经被社区接受的任务。

我一直有这个非共识：我想要去做Agent。

另一点是，我一直想做简单且通用的东西。我不想做一个很复杂、但只能在一个领域奏效的东西。这个方向在传统意义上很难被接受，大家习惯了做AI的方式：把问题不停细分，做很多细分方法。

可能并没有多少人想做一个简单且通用的系统，或者认为这是可能的——尤其20年之内。

## 02 系统

### 2.1 机器的手：“人最重要的affordance是手，AI呢？”

张小珺：今天我们的话题是Agent和强化学习，我们很好奇你会怎么定义Agent？
姚顺雨：这是一个很好的问题。要结合讨论背景看。

从自然语言处理的角度，Agent是相对于一个只会生成文章或对话的系统而言。它能和外界交互，比如使用计算器、互联网，或调用各种工具。也就是说，不仅能生成内容，还能操作和互动。

但从更广义的AI背景看，Agent是一个非常古老的概念。

任何能进行自我决策、与环境交互，并试图optimize reward（优化奖励）的系统，都可以被称为Agent。

从这个角度出发，今天我们讲的Agent更多是指：怎么基于语言模型这样的foundation model（基础模型）去做具备自我决策能力的系统，而不是传统意义上基于规则或仅在某个领域用强化学习（RL，Reinforcement Learning）训练出来的Agent。

因为“Agent”这个词在不同时代有不同定义——你可以说AlphaGo是Agent，也可以说Waymo是Agent，甚至可以说机器人是Agent。这个词的意义很依赖具体情境。

张小珺：你研究的“Language Agent”（语言智能体）和传统Agent，存在本质区别吗？

姚顺雨：本质区别是可以推理，因为推理才可以泛化。

举个简单的例子，我做ReAct一个很强的动机是：我做完第一个工作之后，在思考一个问题——为什么我可以一下子去玩一个新的游戏，但现在这些系统或AI需要几十万步甚至几百万步训练，才能完成类似的事？

我发现，是因为我可以思考。我看到一个全新的环境，会想：这个灯是黑的，那可能有危险，基于常识可能有怪兽；我现在最重要的是点亮灯。基于之前的上下文（Context），灯在我后面，那我应该先向后走。

如果没有这样的思考能力，而是直接从复杂语言去预测“我要往后走”，就很难——没有推理做不到。

最大区别在于，语言模型提供了一个足够强的先验（prior），这个先验让你可以推理，而推理又可以在不同的环境间泛化。

所以核心是推理能力，推理才能带来泛化。

张小珺：从你的视角看，Agent是一个怎样的演变历程？它是怎么一步步发展到今天的？

姚顺雨：我可以说一下自己的理解，可能不完整，或者有一些错误。

最早的AI，我们称为Good Old-Fashioned AI（符号主义AI），想法很简单：我注重的是推理，我怎么想，就把这些规则写出来，让AI也这么做。比如，如果温度高于30度，空调就应该降温。这种基于规则的AI，可以造出很多早期智能体，比如最早的机器人、最早证明数学定理的系统，很多是这么做出来的。

但很快，1980年代，大家发现这个东西有瓶颈——你不管写多少规则，还是很难覆盖这个世界上所有可能发生的情况。

那时符号主义走向极致，大家开始做专家系统：找很多专家，把这世界上所有可能的规则都写下来，是不是就能得到AGI？或者一个通用的、有用的系统？

但后来发现，无论你写多少规则，还是有很多特殊情况无法处理。这些规则只能用于一个任务。比如你写了一个诊断心脏病的系统，写了很多规则，但人千变万化，你没办法处理所有情况，这个系统也没法处理肺病。导致了第一次AI寒冬。

后来我们有了新的神经网络（Neural Network），也就是第二波Agent兴起，标志是Deep Reinforcement Learning（深度强化学习）。典型事件是DeepMind玩视频游戏、做AlphaGo，OpenAI玩机器手、打Dota。

这一波核心是：我有一个虚拟环境，可以无限次尝试，有奖励机制，还有通用网络架构，我就像黑盒一样去学怎么maximize reward（最大化奖励），它就变强了。

这个方向取得了很多成功，最有名的是AlphaGo。但还是有老问题：每做一个新环境，都要做很多Environment-Specific（环境特定）工程。比如做Dota，要调很多参数（parameter tuning），做很多基于这个环境的工程。最大问题是：这些方法没法泛化。

你学了一个围棋Agent，没办法玩别的游戏。你在一个环境里学到的东西，没办法迁移到另一个环境。这肯定是不理想的。而且，如果你所有能解决的问题都在虚拟环境里，或者是像游戏那样可以无限次玩的环境，你就没法找到很好的真实世界应用。

第三波Agent是从大语言模型开始的。我们发现它可以做推理，而基于推理，就能进入一些新的环境，比如编程、互联网、各种数字环境。这些数字环境有一个共性：大多数都是基于语言的，需要推理。

这一次Agent的核心区别有两点：一方面是方法上，我们使用语言模型，用推理去构建能处理各种问题的Agent；另一方面是环境本身也发生了进化，从早期符号主义环境（比如数学定理），到下围棋、打游戏，再到今天互联网、编程、电脑操作这些更接近真实世界的数字环境。

所以这是两条线：一条是方法线，一条是任务线。

大家可能更多注意到方法线，容易忽视任务线。但这两条线是相辅相成的。

张小珺：我一直有一个基础疑问。OpenAI提出的大模型能力分级从Level 1到Level 5，很多人都很熟悉了：

Level 1 是聊天机器人（Chatbot）

Level 2 是推理者（Reasoner）

Level 3 是智能体（Agent）

Level 4 是创新者（Innovator）

Level 5 是组织者（Organizer）

但这个五级划分的内在逻辑是什么？为什么是先有聊天机器人、推理者，然后才是Agent？Level 4和Level 5又是怎么来的？它们之间是递进关系吗，还是各自独立发展？

姚顺雨：逻辑是，首先你要有语言的先验知识。基于语言的先验知识，最早能做出来的应用是Chatbot（L1）。接下来，基于语言先验，你需要具备推理能力，这是Reasoner（L2）。

当你既有语言知识，又具备推理能力，才可能进一步做各种Agent（L3），尤其是能泛化的Agent。也就是说，Agent建立在Chatbot和Reasoner能力之上。

很明显，今天Agent发展最关键的两个方向：

一个是让它拥有自己的reward（奖励），能自己探索；

另一个是Multi-Agent（多智能体），让它们之间能形成组织结构。

这两个方向，我觉得是正交，它们可以并行发展。

谁是Level 4，谁是Level 5，我不确定。但这两个事情是显然的下一步。

张小珺：从Level 2到Level 3，也就是你做的这一步——从训练模型到使用模型，是一个很重要的跨越。

姚顺雨：或者说，是从单纯做推理，到把推理应用在Agent上，用它去和环境交互。

张小珺：Agent目前有哪些主流架构？形成共识了吗？

姚顺雨：我的感觉是，大多数时候大家用的还是类似ReAct架构。你能够去推理，然后你可以产生action（行动）。这是最简单的一种形式。但最简单的反而是效果最好的。

当然，基于不同任务，大家会设计很多workflow（工作流）或更specific（特定）的方法。但如果说最通用、适配性最强的方案，我还是觉得是类似ReAct的方法。

李广密：提升Agent能力，你自己最看重的是哪几个关键能力？

之前有人提到Context（上下文）、Long-Context Reasoning（长上下文推理）、Tool Use（工具调用）或Instruction Following（指令遵循）。你刚才一直强调Reasoning（推理），那如果要提升Agent的能力，你最在意哪些能力维度？

姚顺雨：这是个很好的问题。我觉得现在没有一个特别成熟的taxonomy（能力分类体系），或划分系统。每个人都有自己的理解方式。

有些人会按照工具划分，比如coding（编程）能力、上网能力、使用计算机的能力，这是一种划分方法；另一种是按照模型自身的能力划分，比如多模态处理、长上下文处理、推理能力——这两种划分都有道理。

但就我现在看，我最看重的是Context（上下文）处理能力，或Memory（记忆）能力。因为只有在这个基础上，才能进一步实现Lifelong Learning（终身学习）或Online Learning（在线学习）的能力。

李广密：你刚才一直在提环境，你认为code代码是一个实现AGI最重要的环境吗？它可以支持多轮的强化学习（RL）、提供闭环反馈，也可以验证结果。如果我们在代码这个环境上构建Agent，会不会发展更快？

姚顺雨：毫无疑问，这是最重要的环境之一。

Code有点像人的手。

它某种程度上，是AI最重要的affordance（环境给予行动者的可能性）。

对于物理世界，人最重要的affordance是手——我们围绕它制造各种工具，比如锤子、笔、筷子。但对AI、对Digital Agent（数字智能体）来说，最重要的affordance可能就是code。

因为其他东西，都是给人定义的。比如网页、小说、视频，是为人类设计的；但code是一个天然就给机器使用的表达形式。

我2022年一直在想：做Coding Agent明明是很重要的事，为什么没人做？

我们当时做了一个工作叫InterCode。大家都在做的是：给一个coding task（编程任务）模型生成一段代码，然后你去evaluate（评估）它。但我们就在想：为什么不把执行结果反馈给模型？

我们可以让它变成一个多轮Agent task（智能体任务），构造成一个环境，而不是单次完成的任务。基于这个，我们后来做了SWE-bench、SWE-Agent。

SWE-bench是一个真实世界的软件工程基准，用GitHub上的issue和修复代码来评测模型的代码修复能力。
SWE-Agent是一个基于大语言模型的智能体，能在SWE-bench上自主阅读代码库、修改代码并运行测试来解决问题。
有时候，很有意思的一点：一个东西明明非常重要，但就是没人做。如果你是一个研究员，觉得你做的事很重要，但别人不觉得、也没人做，并不是坏事——可能它真的很重要，只是大家还没开始。

李广密：这里有个很强的非共识：有的人觉得code是这一轮技术革命最大的价值体现，但也有人觉得可以泛化到更多任务里，在电脑、手机、数字世界中都可以实现，Agent操作人能做的95%、99%任务。

你对从code到数字世界这一步的跨越，或者它的泛化，是有信心的吗？

姚顺雨：更广义说，你可以认为API也是code的一部分。任何基于code的接口，都属于code环境的一部分。

有个非常经典的debate（争论）：最终的AGI，是基于API或code的？还是基于GUI（图形界面）？或者是为人定义的前端环境？还是它是一个混合体（mix）？

这个问题有点像：你是想改造你的车让它适应所有路，还是改造所有路让它适应现在的车？

很多时候，现实中并没有现成的API，只有GUI。但你可以人为为它构造一个API。

当然，最终结果很可能是meet in the middle（在中间相遇），两边都会做，而且这个事情可能没那么难。

现在看，让一个Agent既能使用code，又能操作人类界面的screenshot（截屏）、前端，两者兼顾也没那么困难。从这个角度说，让Agent像车一样能适配各种路，比起要改造所有路让它们都有API，要容易很多。

Coding肯定很重要，但如果让Agent也能操作GUI，最终Agent很可能是“什么都能做”的。

### 2.2 任务的设定：“我们对简单任务的robustness没有重视”

张小珺：你4月发布博文《The Second Half》（下半场），你是怎么想到the second half这个idea的？受了什么启发吗？

姚顺雨：我是受邀去斯坦福一门课做talk，当时想，能讲点什么？没法讲太技术，只能讲更哲学的内容，就想到这个话题。

这个想法来自我在OpenAI的工作经验，以及之前做research的感悟。大家过去往往更关注模型训练、方法设计，但我觉得现在的bottleneck（瓶颈）已经转移了：变成怎么去定义好的任务，怎么去定义好的环境。

张小珺：现在是处在那个转折点吗？从上半场到下半场。

姚顺雨：主线正从“上半场”转向“下半场”。我说的主线是基于语言的智能体。当然你也可以说，在Audio（音频）、Multimodal（多模态）、Robot（机器人）这些方向，还有很多未解的问题。

但我觉得，从语言出发，去定义Reasoning（推理）、定义Agent，我们终于有了一个非常general（通用）的方法，而且这个方法是可泛化的——我们实现了一个基点时刻。

这带来一个本质变化：以前我面对很多怪兽，需要造出各种不同武器去打它们；现在我有了一把通用武器，比如机关枪，我不需要再为每个怪兽单独造武器。接下来要思考的问题就变成：我该朝哪个方向开枪？

现在方法的问题已基本解决，真正重要的是——我们要用这个通用方法，解决什么问题？

李广密：怎么设定任务？怎么定义问题？关于这个，你在探索过程中有什么思考吗？

姚顺雨：不同的人有不同的flavor（风格），我从很早就有一个偏好：我想定义一个基于结果的reward（奖励），而不是基于过程的；而且这个reward应该是基于规则、可计算的，而不是来自人的偏好、模型的偏好，或者一些黑盒指标。

我们做WebShop的时候，最困难的一点是，怎么定义reward。我觉得做任何RL（强化学习）任务最难的不是建环境，而是怎么设计reward。你当然可以把Amazon或Facebook模拟出来，工程上确实很难，但总是可以做。但最难的，是怎么设计一个既有难度，又有实际价值，同时又有一个好的reward的任务。

我希望这个reward是不noisy（不噪声大）的，是可解释的，是白盒的（white-box），不是那种黑盒的东西（black-box）。

事实证明，这也是现在RL成功的关键。像math（数学）和coding（编程）这种任务，之所以能做出来，核心就是：

Reward是基于结果，不是基于过程；

Reward是白盒的、基于规则的，不是基于人的偏好或模型的偏好。

比如，一个数学题答案是3，它就是3——只要你得出的是3，就是对的；不是3，就是错的。

但如果你reward是基于过程，就会出现hacking（投机取巧）。你去优化人的偏好、模型的偏好，也会出现hacking。比如你生成一段非常优美的代码，但它并不解决实际问题。

我后面做的很多task，也都是用同样的filter（筛选标准）。

比如SWE-bench这类工作：

第一，它是结果导向，而不是过程导向；
第二，它的reward是基于规则、白盒的，而不是来自人或模型的主观偏好。
张小珺：就像上面说的，OpenAI有5个分级。如果从任务定义出发，是不是也可以做出一套产品能力的分级？随着模型能力溢出，我们开始使用这些能力，Agent能力可以怎么分级，你脑海中有没有一个初步的框架？

姚顺雨：我现在倾向于认为，不同类型应用会带来不同challenge（挑战）。这些挑战是正交的，很难说哪个更难、哪个更简单。

人类也有这个问题——洛克菲勒和爱因斯坦谁更厉害？很难定义；成为一家大公司CEO和成为一个数学家，哪个更难？只是不同的挑战类型。

而对于Agent，另一点是：人觉得很简单或难的事情，对Agent可能不是那样。

人觉得做客服比做软件工程师简单很多，工资也低、文凭要求也低。但现在反而做软件工程对Agent更容易。因为软件工程有更好的环境、更清晰的reward、更大的数据量，等等。但你想做一个特别robust（健壮）或reliable（可靠）的客服，反而更难。它涉及复杂的reliability challenge（可靠性挑战）。

我们当然可以把人类工作分成不同的category（类别）。但对AI来说，人类觉得难或不难的任务划分，不一定直接映射到AI的能力上。

张小珺：整体来说，什么样的任务适合Agent做？什么样的任务适合人和Agent一起做？什么样的任务适合人做？

姚顺雨：我现在感觉任务大概可以分成几类。

一类任务更注重reliability（可靠性）。你做客服，重要的是：100次里你需要99次甚至更多不能出错。你只有85次让用户满意，还有15次不满意，可能被炒鱿鱼。这种任务比较简单，但需要极高稳定性。Agent就需要特别强调reliability。

另一类任务更注重creativity（创造力）。你去证明黎曼猜想，或者写一个复杂程序，或者创作一部文学剧本。这类任务允许你失败很多次，只要有一次做得特别好，就算成功了。这是非常不一样的挑战。

还有一种划分方式是：看任务的深度和广度。

有些任务像Cursor（一个代码编辑工具），是非常短的loop（循环）。我只需要把一个文件改一下，可能3秒就完成。但也有一些任务需要30分钟、3小时，甚至3天。这种任务需要的是Long-Term Memory（长期记忆）的能力。

再比如，从任务的广度看，我只是去解决一个具体bug，这是比较窄的问题。但如果我要从0搭建一个像Windows这样的操作系统，这是一个非常广的任务。你可以说这是一个人能做的事情，一个小组能做的事情，还是一个公司才能做的事情？从这个角度，我们也需要做更多motivation research（动机建模研究）。

张小珺：哪些任务对于Agent是相对更好定义的？从易到难的顺序应该是什么？

姚顺雨：我们可以平行做很多不同事情。有一个简单的设计评估指标（metric）方法。

在coding任务中，我们传统有一个评估指标叫Pass@k，意思是：你对同一个代码生成任务，最多尝试k次，其中起码有一次的成功概率是多少？你可以想象，当这个k越来越大，系统被使用的成功概率也会变大。

很多时候做coding相关研究，它会report（报告）的是Pass@100，也就是：同一个任务你跑100次，起码成功一次的概率是多少？

但我们2024年发了一个研究，叫TAU-Bench（Tool-Agent-User Benchmark，工具–智能体–用户基准测试），想法是：对于另一类任务，比如客服，我们需要一个刚好相反的指标，我们把这个指标定义为Pass^k。也就是：每一次都成功的概率是多少，或者失败一次的概率是多少？

有些任务我们需要优化的是Pass@k（多次尝试中至少成功一次），而另一些任务，比如客服，我们需要优化的是Pass^k（每次都成功），或者我们最关心的是Pass@1（一次就要成功）。

但是，现在我们对于简单任务的robustness（稳健性）并没有特别重视——这是因为大家做AI还是在做一些benchmark（基准任务），而不是实际应用。

但如果你接受了这个mindset（思维）转变，很自然你就会意识到：有些应用是需要特别强调robustness的，那你就需要去优化它的robustness。

现在大家还没完全意识到这件事；但我相信，如果大家意识到这个转变，会带来很大进步。

### 2.3 泛化的工具：“语言是人为了泛化而发明出来的工具”

张小珺：你有一句非常high level（抽象）的总结：语言通过智能体中的推理实现了泛化。这里的泛化是一个已经被证实的，还是一种推断？

姚顺雨：为什么语言非常独特？因为它是人在这个世界完成各种各样事情的工具。

语言也是人类发明的工具，像火或笔一样。但它之所以特殊，是因为它是一个帮助你解决任何事情的通用性（general-purpose）或泛化性（generalizable）的工具。

当你学会了这门工具，你可以去做很多新任务。比如你学会了攀岩，它帮不了你完成新任务。但你学会了语言，你可以通过语言和人交流，学习、思考、推理。

2020年以前，大家没把这个事想清楚，觉得语音、文字、图像、视频都是一些数据，没什么区别。但我觉得最大区别是：语言是人为了实现泛化而发明出来的工具，这一点比其他东西更本质。

张小珺：这里说的是语言具有泛化能力，那么强化学习终于具备了泛化能力，这是一种推断还是一种结论？

姚顺雨：可以说是我个人观点，当然很多人在讨论。泛化与否，本质上是一个spectrum（谱系）问题，是一个相对概念，不是绝对的0和1。

我之所以这么说，是因为在此前，如果你在一个特定环境上训练，模型只能在这个环境表现良好，不能轻易迁移到其他环境。但现在，你在一个环境上训练，模型可以适应更多不同环境，这才是最本质的区别。

DeepSeek大家觉得一个有趣结果是：你在数学和编程领域用强化学习训练模型，但它在创意写作上也变得更强。

这体现了本质区别：AlphaGo只能下围棋，不能下象棋；而现在你学会数学，也能提高创意写作。

李广密：我读你的文章，印象最深的也是，你提到RL终于泛化了，是真的泛化吗？——你刚才也说，有很多先验知识已经train（训练）到model（模型）里头了，有什么迹象让你感觉是真的泛化了，而不是training data（训练数据）里面就包含这些数据？

姚顺雨：对，我觉得是有可能的。如果你的Pre-Training（预训练）已经包含了所有事情，那么 RL（强化学习）只是激发出这些能力的skill（技能）。

事后想起可能是Ilya（OpenAI前首席科学家），还是谁，说过一句话，意思是：Maybe the ultimate generalization（也许最终的泛化），就是你去overfit（过拟合）现实。如果你能把剩下的所有事情都做完，那么讨论它是过拟合还是泛化就不重要了。

但我觉得，它还是泛化的。原因是它能够推理。当你能在一个环境学到如何思考的技能，并且这种思考能力能迁移到新环境，这才是泛化的本质原因。

李广密：训练某一类游戏变强，能泛化到其他游戏也都很强吗？比如，一个模型打Dota（多人在线战术竞技游戏）非常强，是不是在所有游戏里都很强？

姚顺雨：不好说。即使是推理，它在不同环境的泛化能力也可能不一样。比如，基于逻辑的推理，可能从数学到编程的迁移更容易；基于人情世故的推理，可能在另一类任务上迁移得更好。

但重要的是，现在终于有可能出现一个单一模型能够做所有任务。之前认为这不太可能，但现在是有可能的——你可以在很多不同任务上做强化学习，而且它能迁移到更多任务。

当然，如果只考虑任务与任务之间的迁移，迁移程度和任务本身的性质有关系。

李广密：代码和数学之所以容易泛化，你有想过背后的原因吗？是因为他们有思考过程？

姚顺雨：只是因为它是最早开始做的。它之所以最早开始做，是因为它相对简单，有一个很好的reward（奖励信号），不需要复杂环境，它本身就是推理。

现在看，很多其他任务也是可泛化的。只是我们一开始做的是这个任务，所以，大家对这个方向的讨论比较多。

### 2.4 奖励的机制：“当AI玩一个语言游戏，要怎么定义内在激励？”

张小珺：基于基础模型往上长，Agent生态树在你脑海中，会是一个怎样的结构？

姚顺雨：一个方向是：fundamental research（基础研究）怎么演变？或者说，方法怎么演变？

另一个方向是：应用，或者它的交互方式（interaction）有怎样的演变？

这两个方向之间有关联。但它们需要不同的人去探索不同的方向。比如Cursor并没有在fundamental research上做什么创新，但做了交互方式上的创新。

在fundamental research上，比较重要的有三方面：

一个是Memory（记忆），

一个是Intrinsic Reward（内生奖励机制），

还有一个是Multi-Agent（多智能体系统）。

这也跟OpenAI提出的Innovator（L4、创新者）和Organization（L5、组织者）框架很像。

你作为一个Innovator，首先你需要一个Long-Term Memory（长期记忆）。

比如，我是Wiles（安德鲁·怀尔斯，数学家），我研究费马大定理，可能花了20年。我就需要一个长期记忆。

我有这个长期记忆还不够，还需要有内在的reward。因为在你真正证明那件事之前，没有任何外部奖励（Extrinsic Reward）——你没有获奖，没有做成任何“可交付”的事情，也没人给你feedback（反馈）。你需要自己给自己反馈。

这是所有Innovator最重要的。无论你是艺术家、科学家、文学家，还是任何类型的创作者，对吧？

另一方面，作为一个Organization（组织），你需要解决的问题是：Agent和Agent之间怎么协作？怎么让Multi-Agent（多智能体）协作scale（规模化）？

现在的Agent就像一个普通大学生，做一个数字化的实习生。或者说，AGI就是一个普通一本大学生在电脑上能做所有事情的一个能力。

但是，人类社会的边界是什么？这当然覆盖80%或90%的人。但我们最崇拜的人，是哪两种？

一种是创造新东西，在认知或审美上开创新领域的人：爱因斯坦、高更、梵高、贝多芬；
另一种是能创造新组织、伟大组织的人：伊隆·马斯克、乔布斯。
很自然，个体的创造力和组织的协作能力——都非常重要。

张小珺：为什么OpenAI分级的最后一级是组织者（L5）？

姚顺雨：我一开始是认为Innovator（L4）和Organization（L5）是更正交或并列的关系。

我当时在群里问了一个问题：当一个大公司CEO和一个科学家，到底哪一个难？

这个不好说，实现路径有区别。所以，不用太纠结谁是第四级，谁是第五级，都很重要。不一定要先实现哪一个才能实现另一个，可以同时去探索。

李广密：这中间有几个关键的问题要突破，比如长期记忆，这是短期可预期突破的吗？

姚顺雨：也许吧。当然也取决于多短期？但我觉得当它足够有价值，它必然会突破——如果你对技术是乐观的。

李广密：长期记忆，你要展开讲一讲吗？

姚顺雨：我不知道我能分享多少，但我的信念是——是Utility（效用）的问题。

为什么我们现在的模型，推理很强，考试很强，玩游戏很强；但它还没创造出足够经济价值？——根本原因是：它没有这些Context（上下文）。

人类社会比较 tricky（复杂微妙）的一点是：当然，我们确实写下了很多东西——我们用文字、Google Doc、Notion，记录了很多东西；但很多Context永远只存在人的大脑，是通过一个分布式的系统来维护。

比如，你老板跟你之间的行为习惯，或者一些很难用语言总结下来的信息。这些Context存在于人的脑海里。人没办法把这些东西全部写下来。

这就导致——人是不可或缺的。

只有人有这样的能力：进入一个环境，获得这个环境里的Context。

如果这个问题解决了，Utility问题就可以在很大程度被解决。这个世界，大多数人并不是乔布斯，也不是爱因斯坦，只是一个普通人。他的数学推理没有o3强，但他能manage Context（管理上下文）。

他去一个公司7天，除了在文件上看到信息外，脑子里也积累了Context。而这些Context是o3没有的。虽然他没有o3聪明，但因为他拥有Context，他做得比o3好。

李广密：有可能我们很快就会看到最强的软件工程师，甚至2027年看到能操作人类电脑、手机上几乎所有任务和指令的通用Agent，你对这一天的想象是怎样的？是过于乐观还是比较合理？

姚顺雨：现在还没有well-defined（明确定义）。现在模型写代码的能力超过世界上几乎所有人，或者说，它的数学和逻辑推理能力，也比大多数人强。但是，当你说它能不能很好使用环境，关键还是看你让它做什么任务，这个任务能不能被合理定义。

很多时候，人类最难的问题不是推理本身，而是获得完整Context（上下文）。

现在模型的bottleneck（瓶颈）不是缺少推理能力，或者写代码、使用前端的能力，而是缺少一个完整的上下文。

我不知道这是Intelligence（智能）问题，是产品问题，还是别的什么问题——但如果想让AI真正发挥价值，这个问题必须解决。

李广密：你刚才提到另一个关键点：模型或Agent要有内生奖励系统。今天是不是还没有这样一个系统？如果我们真的要赋予它内生奖励机制，是不是在它持续自主学习中，就可以改动自己的模型权重，从而更聪明？

我们离这一步还有多远？

姚顺雨：我不知道。我觉得会有这一天，但很难预测时间。

当然，它自我提升的方式，也许是改变自己的权重，也许是拥有一个基于语言的长期记忆，也许是一个基于Embedding（向量表示）的长期记忆，或者其他形式的记忆机制。但我相信，它会自我提升。

李广密：内生奖励，你能讲讲吗？

姚顺雨：就像我刚刚说的，很多创新者之所以能在没有外在激励的情况下坚持，是因为他有内在的价值观或激励机制。

这个问题，AI和神经科学已经研究多年。婴儿是最典型的例子。他们拥有基于好奇心或自我激励的机制。很多婴儿会反复玩一个玩具，用嘴去咬一个东西，或者做一些看似“无意义”的动作。

你说他获得了什么reward吗？他没有升职加薪，没有拿到钱，没有任何外在激励——他只是好奇。他的动机是：“如果我做这个事，我会有什么样的感觉？”如果这个感觉是新的、不同的，他就可以从中学习。

张小珺：他可以获得安全感。

姚顺雨：对，就是说，好奇心、掌控感、安全感，是一些内在动机。正是这些东西驱动了人去做某些事。否则，很难从纯粹理性角度解释：他为什么要做？

但有意思的是，当人长大之后，会发生重要变化。当你是婴儿，你对世界的理解，是基于视觉、触觉，基于物理世界的。你学习的是，怎么把触觉、听觉、视觉，以及对骨骼系统的控制结合起来。

当你长大之后，你对世界的理解方式变了，变成一个基于语言、推理、文字系统的理解。你开始思考：这个世界是怎么运作的？我怎么才能开一个公司？怎么才能升职？怎么才能做成一些事情？

你玩的，不再是一个物理游戏，而是一个文字游戏。

在这个文字游戏里，当然也存在内在激励，但又好像和婴儿时期的好奇驱动不太一样。

这是AI面临的挑战：传统AI，比如玩迷宫、做机器人仿真，它可以定义一些基于世界模型或者模仿婴儿阶段好奇心的内在激励。

但当AI在玩的是一个语言游戏，要怎么定义内在激励？——这个问题就变得不太一样了。

张小珺：你在文章也说，我们忽视了任务评估标准的重要性。应该怎么去评估？——比如，我们怎么去衡量一个Agent？有哪些北极星指标？

姚顺雨：还是要思考怎么去创造更多现实世界的价值。

当然这个事情在不同领域、不同应用下，有非常不同的任务设计、方法和路径。但有一个大趋势是：应该更多去思考实际价值，而不是这些被设计出来、类似考试或游戏的东西。

我们发现，一旦你可以定义一个考试或游戏，离它被解决也不远了。

甚至你可以说，世界之所以难，是因为它不是一个被设计出来的东西。考试和游戏有一个很大特征是：它在被设计的时候，就已经有一个很好的reward或标准答案。

但当你已经有一个非常好的reward或标准答案，再加上现在已经有一个general recipe（通用解法），那这个事情离被解决也不远了。

而真实世界的问题是：它没有标准答案，没有标准的reward function（奖励函数）。很多时候人做事情，也不一定是为了一个理性的reward，但人还是去做了。

张小珺：它是开放的。

姚顺雨：对，现在主要问题是这个。最大问题不在于，我有没有一个well-defined（明确定义）的答案，而是我怎么找到它。

张小珺：我们未来还需要更多地推翻各种各样的基本设定吗？

姚顺雨：我觉得需要。

人类一直在做这件事，不是吗？

## 03 吞噬的边界

### 3.1 双刃剑，“创业公司最大机会是：设计不同的interface”

张小珺：你知道，应用型创业公司很担心，大模型公司的模型能力溢出，会把他们做的Agent吞掉。

长期看，Cursor这样的公司，壁垒是什么？哪些Agent是模型公司必然会做的？哪些有创业公司机会？——边界可能在哪？

姚顺雨：创业公司应该担心的是模型没有溢出能力，这样你就真的什么都做不了了。有溢出能力是个非常好的事情，这几乎意味着你有机会。

创业公司最大机会是：能设计不同的interface（交互方式），或者说人和数字世界交互的方式。

ChatGPT或所有做模型的公司，都在做类似ChatGPT的产品。ChatGPT的本质是：你是在像和人交互一样去进行和数字世界的交互。

你的Chatbot是像人一样的东西——你和他聊天，给他布置任务，让他帮你做Deep Research（深入研究）或者写代码——交互方式是像人，或者像助手一样的交互方式。

如果你能用模型通用能力，创造不同的交互方式，就能创造巨大的机会。

最终，可能模型的能力会产生beyond ChatGPT（超越 ChatGPT）的交互方式，变成Super App（超级应用）。

如果你做旧的interface，你利用这些新的模型，很容易被ChatGPT取代。如果你的交互方式很像ChatGPT，你有什么理由不被ChatGPT取代？如果你做的是新的交互方式，但模型没有继续变好、没有新的溢出能力，也很难做。

对于创业公司，最好的机会是：你做新的交互方式，并且模型不停有新的溢出能力，让你能够赋能这些新的交互方式——两者缺一不可。

张小珺：但是ChatGPT也可以跟进这个新的交互方式。

姚顺雨：对。但拥有一个Super App对于公司是双刃剑。

当你已经有了一个交互方式，你必然形成路径依赖。就像2020年Google有无限多资源和钱，有Transformer，但它最自然的想法是：我怎么用这东西提升搜索引擎？

当你有像ChatGPT这样的Super App，很自然你的研究就会center around（围绕）这个Super App，会center around这个交互方式。

你会探索新的产品，但即使是大厂，即使是谷歌，即使是OpenAI，大部分资源还是会围绕你Super App的交互方式——所以，这是创业公司的机会。

李广密：你刚才提到交互方式，今天还是人跟code交互、人跟text交互，那人跟Agent未来是怎么交互？你感觉Her会是一种正确的交互方式吗？如果这种交互奏效，有没有机会beat（胜过）ChatGPT今天的形态？

姚顺雨：Her是不是还是类似一个Assistant（助手）的形态？只不过它有语音而不是文字？

这是一个显然很有价值的形态，人和人交互已经几千年、几万年、几百万年，这是对人最自然的形态，肯定是最显然的Super App。

但这个生态位，我觉得ChatGPT是站住的。模型公司一开始做的就是这个。

那我觉得不显然的是：我能不能基于不像人的交互方式？

Cursor是很好的例子，创造了一种新的交互。不是像人一样的交互，而是像Copilot（副驾驶）。写代码的时候，它能给你提示或编辑。没有人和人是这样交互的。这是它的价值所在。

Google也是很好的例子。雅虎是一个更像黄页、更让人熟悉的交互。但谷歌是一个让人不熟悉的交互，很奇怪。

Assistant、Her，或者像人一样的交互方式，显然是最重要的交互方式之一，但还是会有足够多的机会，诞生新的交互方式。

张小珺：你脑海里有没有一些新的交互？非ChatGPT在探索的形态，也非传统互联网的交互，在你脑海里有吗？

姚顺雨：Canvas是一个好的尝试，可以基于现在的任务，在线生成最符合情境、个性和任务的前端。这是值得探索的方向，但也很难。

李广密：在你看来，应用公司的数据飞轮，对他们来说重要吗？或者说，在什么环境下才能形成？

我感觉，Chatbot产生的是偏好数据，好像没什么数据飞轮；Code可能有思考过程的数据，这种思考过程的数据代表一类能力，可能是有用的；像Canvas也好，Artifacts也好，可能是有思考过程的数据，这类可能有机会形成很强的数据飞轮效应。

姚顺雨：大多数公司还没有形成数据飞轮；他们依赖模型变好，利用模型变好的溢出能力。

如果你要有数据飞轮，首先你要能自己去训模型，并且能通过交互有很好的reward，使你能把好的数据和不好的数据分开。

比较成功的是Midjourney，有非常清晰的reward——人更喜欢哪张图，这个reward和应用是对齐的，reward做得更好，公司就更成功，模型也更好——一切都对齐。有了这种情况，才能自己训练模型，做数据飞轮。

这个过程必须比较非主线。因为如果很主线，我也可以通过Pre-Training或RL提升能力，靠泛化或其他方式。

总的来说，大部分公司目前还没有形成飞轮。

### 3.2 对Agent创业者的思索：“这世界是相互抄的关系，而不是单向抄的关系”

李广密：在你看来，Agent创业者一定要有研究背景吗？

姚顺雨：不好说，挺看人的。很难把人简单分成research和非research两类，没那么泾渭分明——人与人之间的差异很大。

可能最重要的一点，还是得找到value（价值）。不管你叫它product-market fit（产品与市场契合）、产品的sense，还是别的——找到真正有价值的东西最重要。技术只是手段，目前最重要的是解决问题，需要找到一个好的问题。

如果你有很强research背景，比如自然语言处理，反而可能是坏事——因为你会对技术太执着，拿着锤子到处找钉子。

Cursor创始人是四个本科生。Perplexity创始人是研究员出身。真的挺看人的，跟你是否做过research，没有那么强相关性。

张小珺：好的AI产品经理应该长什么样？

姚顺雨：好的AI产品经理就是一个好的产品经理，可以第一性思考。AI是变化很快的，相对不变的是人、人性、人的需求。这变化得更慢。

你能找到一个好的需求，从第一性原理反推：要把它做成，我需要应用什么样的技术？

张小珺：你怎么看Manus、GensPark这些产品和他们的创始人？

姚顺雨：我试过Manus，还没试过GensPark 。Manus挺有意思，给我一些启发。他们产品sense很好，有打磨产品的基因。

张小珺：这个产品应该是OpenAI主线上的产品对吧？

姚顺雨：Emm……You will see。

基于Manus，我再讲一点。传统大家认为发生的事情是：我大厂先做出来一个东西，创业公司就可以开始抄。比如做出ChatGPT，我可以去抄一下ChatGPT，做一个类似的事情。

但现在，似乎反过来也可以成立。可以先小厂做一个事情，它创造出来一个交互的创新或者产品的创新，做模型的公司也可以去借鉴或者应用。

这点还是挺有意思。很多时候大家会说，模型做得越来越好了，是给创业公司做嫁衣了。因为你创造很好的模型，如果没有自己运用特别好，这些创业公司就用好了。

但也可以反过来，如果你创造一个非常好的交互，但没有能力把模型或底层能力做特别好，大公司也可以借鉴你的交互，再加上它的模型能力，做得也特别好。

这世界是个相互抄的关系，而不是一个单向抄的关系。

李广密：如果你是Manus创始人、CEO，你今天要走向垂直方向吗？

姚顺雨：Manus的一个价值是，它给人非常general（通用）的感觉。但我觉得，有一个非常通用感觉交互方式的Agent，和你有一些Killer App（爆款应用），是不矛盾的。

一个比较理想的情况，你有一个非常通用的交互方式，这个交互方式想象力足够大。比如Cursor，虽然它是IDE（集成开发环境），如果它只做 IDE，想象空间是有上限的，就在IDE里面。但如果你做一个非常general的产品形态，比如Manus，想象空间是很高的。

但并不矛盾的是，你可以有每个阶段的Killer App。比如它做PPT特别好，做Deep Research特别好，或者做其他东西特别好。

iPhone或iPad是非常通用的产品形态，但它一开始，都有一些Killer App支持它有momentum（增长动能）。包括ChatGPT，包括微信，很多伟大产品都这样。

你有一个足够通用、简单，或第一性的交互方式，它有很多想象空间。但你去维护它，或者设计路径的时候，你能有各种各样的应用，使它不停地增长。

张小珺：你听了我和肖宏（Manus创始人）的播客，有什么感觉吗？

姚顺雨：我觉得挺有意思。印象最深刻的是他说，VC是一个非常贵的融资方式，不是在你不好的时候，而是在你好的时候。他有很多挺不一样的思考问题角度。

张小珺：2025年过年DeepSeek全球爆火，这对硅谷的AI研究员带来了哪些叙事变化？

姚顺雨：从OpenAI角度，大家讨论的有几点：

一点是Chain of Thought（思维链）的reveal（展示）。显示出一条长的思维链，似乎很重要，它是产品形态的突破。很多时候，技术积累已经到了，就像洪水已经到达闸口，需要一个时刻“开闸”，让大多数人真正感受这个技术。

我们会说有iPhone moment、ChatGPT Moment，可能有DeepSeek moment。这个moment就是指，一个非常大的交互方式上的冲击，带来了magical（神奇）的体验。

另一点是对开源的重新思考。Sam（OpenAI首席执行官）在他Twitter上讲了很多，说OpenAI过去忽视了这件事，但仔细想一想，它是有价值的，可能应该做。

我们默认认为开源落后于闭源，原因是，它不像 Linux（操作系统），我有1000个人可以每人出一份力，让系统通过分布式变得越来越好。做好一个强模型更像我有20个特别厉害的人，再加上大量资源，就可以做得很好。它需要非常特殊的组织、资源和人才集中。

这种情况下，传统意义上开源的优势没有那么大。比如Facebook在开源上，做得也没有那么好，在美国很多人也习惯性忽视这个路径。

做好开源是一个“很吃亏”的事。你首先要有足够的资源，有很强的人，有很好的组织文化，还要有商业上的justification（正当性）。最好情况是：你是个慈善家，有几百亿美金，你就做这件事造福世界。

这是一个小概率事件，但它发生了，就有这样一个人去做了这样一个事。

DeepSeek在许多方面，组织架构、工程能力、基础设施，确实有值得称道的地方。

张小珺：有一个Agent创业者想问你：Agent如何scale up？现在的主要瓶颈是算力，Agent的token用量非常可怕，单个用户消耗可能是Chatbot的500到1000倍，再叠加几百万个用户，成本非常高。这种情况下，Agent应该怎么扩展？

姚顺雨：最重要的点是——你得先找到一个好的应用。

Cost（成本）本身不是最大问题，问题是你的成本并不能证明你的performance（性能）或value（价值）是合理的。

如果这是一个很有价值的事，我花500美元，但可以赚1000美元——根本不是问题。这不是technical bottleneck（技术瓶颈），而是product-market fit（产品与市场契合度）的问题。

所以，现在最关键的，是要找到真正有价值的应用。模型的cost会下降，能力会提升，这个方向是确定的。但能不能找到那个有value的点，是最本质的问题。

当然，不同的应用，做法可能会很不一样：

如果是一个相对简单的任务，我可以训练一个小模型，让它更快、更便宜、更针对这个任务。

但如果你要做的是更复杂的事，比如投资、Deep Research，就需要更大的模型，在cost和value之间寻找新的平衡。

总的来说，第一步永远是：找到一个真正有价值的场景。

一旦你找到它，cost的问题总是有办法解决。

张小珺：你在OpenAI的一个好处是不是，可以很清楚知道哪些是模型公司的主赛道，哪些领域可能是创业公司的机会？

姚顺雨：每个公司一旦有它的Super App（超级应用），所有事都会围绕Super App。当你有ChatGPT，训练模型的方式、组织架构，都会围绕ChatGPT重构。

如果你做一个和ChatGPT形态很不一样的东西，是会有机会的。

### 3.3 既单极又多元的世界：“这个世界不是单方压倒另一方，双方都有自己的力量”

张小珺：一位AI研究者说，他对Agent的想象很有限，希望你能对未来的Agent畅想一下。你曾经说过，你的终极理想是打造“世界上最强的Agent”，它会是什么样的？

姚顺雨：大多数人对AGI的想象就是一个模型，就像这个世界上最聪明的人，他拥有所有知识、能力，比我们都聪明，是最强智能体。

但我现在的感觉是：不同的交互方式下，有不同“好”的定义，有不同“强”的边界。

最终的智能边界，是由不同的交互方式决定的，而不是由一个single model（单一模型）决定。

想象空间非常大。就像一开始互联网诞生，最早Super App只是把邮件升级成Email，Amazon已经算非常创新的东西了。现在就像那个阶段——我们的想象力仍被以往的交互方式所限制，还有许多尚未诞生的交互方式。

这些全新的交互方式，会改变我们的世界。

张小珺：在你脑海中，最强的Agent应该是什么样？

姚顺雨：对于不同的任务和交互，需要不同的Agent系统去解决。

模型是可以share（共享）的，但如果你讨论的是整个系统，那就不一样了。就像你问，这个世界上最强的互联网网站是什么？最强的互联网公司是什么？很难回答。它是一个multiface（多面向）的系统，有很多不同侧面。

AI可能也会变成这样的结构。OpenAI可能会成为一个类似Google的公司，成为新世界里非常重要的一环——但这并不代表，这个世界就会被这样一个单极系统垄断。

如果真是那样，这个世界就会变得很灰暗。大多数人也就没什么价值了。

张小珺：你对未来Agent生态的构想会是什么样？现在有点像，当年大家都在创业做App的时候，如果再往后推演几年，这个世界会是什么样？

姚顺雨：很难说。但肯定会有很多不同的交互方式，创造出不同的系统。

OpenAI这样的公司，会想继续推进一个中心化的助手系统，有更多环境、更强能力，做更多事情。

也会有不同的生态系统，有不同的交互方式，会训练完全不同的模型。甚至从Pre-Training开始，所需要的能力和很多东西都不同。

比如，另一种交互方式可能是，我想造一个朋友。这个朋友不需要数学、物理特别强，数学太强反而不自然。它记忆不一定特别好，会犯错，有感情，也不是特别rational（理性）。但这也是有价值的——可能有人会做这种事。

这类东西很难和ChatGPT比强弱，它们是不同应用，有不同价值。

也可能出现一个由Agent组成的社会。

为什么这个世界上很多人有价值？不是因为他们的数学或编码能力强，而是因为他们拥有别人没有的信息。

中间商本质是拥有信息差。拥有信息差的人会想维护自己的权利和资源。这样的人会发明出更Multi-Agent（多智能体）或更 Distributed Network（分布式网络）。

在交易世界里，信息很重要，每个人只拥有信息的一小部分，这种情况会出现新的不同形态。可能是Multi-Agent，每个人有自己的Agent，Agent之间可以与百万甚至更多人交换信息，达成交易或某些目的。

根本上，现在非常强的巨头和重要节点，有动力继续推动中心化。但在中心化之外的力量，也有动力做一些非中心化的事情。

这个世界可能不会是单方压倒另一方，双方都会有自己的力量。

而这个世界智能的边界、研究的边界，可能不是由一家机构定义，而是由不同Super App共同定义的。

### 3.4 环境是记忆层级中最外层的部分，“这很哲学”。

李广密：更关键的是，大模型技术没有垄断性。硅谷头3-4家好像都能追到一定的水平。如果OpenAI有垄断性，那是比较可怕的。
姚顺雨：我觉得暂时没有垄断性。但如果你能找到一个产品形态，把研究优势转换成商业优势，就会产生壁垒。

现在对于ChatGPT比较重要的是Memory（记忆）。

这是可能产生壁垒的地方。如果没有Memory，大家拼谁的模型更强。但有了Memory，拼的不仅是谁的模型更强，而是用户用哪个更多、哪个粘性更强。

我积累了更多Context，它能给我更好体验，我就会有粘性——这或许是研究优势转化成商业优势的方式。

张小珺：最近ChatGPT会出现灰色提示词，显示“记忆已更新”，这个更新的是什么？

姚顺雨：我最近没怎么用这个功能，但好像做了一些提升。

我怀疑是它产生或者使用记忆的方式变得更好。包括能更有效从很多用户对话中提炼出来，或者retrieve（检索）出更相关的内容。细节我不特别了解。

李广密：MCP（模型上下文协议）本质也是Memory吗？因为我的很多Context在我的个人软件、企业软件里，MCP本质也是hack（利用）Context的一种方法。

姚顺雨：某种程度上，是的。从Agent角度看，这个世界有一个Memory Hierarchy（记忆层级）。Memory Hierarchy最外层永远是环境。

有点像你考虑电脑，它有个Memory Hierarchy，从CPU缓存到内存再到硬盘，但最外层的Memory永远是外部环境。比如我插一个U盘、拔一个U盘，或者把东西上传到互联网，或者做个音乐变成光盘。

前年冬天，我读到冯诺依曼临终前写的一本书，The Computer and the Brain。最让我印象深刻的一句话是：Essentially, the Environment is always the most outer part of the Memory Hierarchy.（基本上，环境永远是记忆层级中最外层的部分。）

这很哲学。

对于人，你有你的Memory Hierarchy，有Working Memory（工作记忆）、Long-Term Memory（长期记忆）在脑子里，但最外层是你的笔记本、Google Doc、Notion，这些是你最外层Memory Hierarchy的一部分。

《计算机与大脑》（The Computer and the Brain）是20世纪伟大的数学家约翰·冯·诺依曼于1956年完成的未完成著作。这本书源自他为耶鲁大学西里曼讲座准备的讲稿，探讨了计算机与人脑在信息处理的相似性与差异性。尽管书籍篇幅仅96页，但其深刻的洞察力和前瞻性思考，使它成为计算机科学和神经科学领域的重要经典之一。
李广密：Long Context跟Long-Term Memory是什么样的关系？

姚顺雨：Long Context是实现Long-Term Memory的一种方式。

如果你能实现1亿或1千亿或无限长的Context，它是实现Long-Term Memory的一种方式。它是一种和人区别很大的方式，但这是有可能的。当然会有很多不同方式，不好说哪种是最好，或者最合适。

李广密：现在业界实现Long Context有Linear（线性）方式、Sparse（稀疏）方式，或者 Hybrid（混合）方式，你有倾向吗？

姚顺雨：我不想对方法进行评论，但我想对evaluation（评估）和task（任务）进行评论。

起码到去年为止，大家主要还在做所谓Long Range Arena（长距离评估基准），比如needle in the haystack——我有一个很长的输入，我在中间插入一句话，比如"姚顺雨现在在OpenAI"，然后我问你相关问题。

这是一个必要但不充分的任务。你能完成这个任务，是Not Memory Work（非长期记忆任务）中的前置条件，但远不是充分条件。它是必要条件，但现在大家有点陷在这个必要条件，没有创造更难或更有价值的任务，这是个问题。

当没有一个很好的评估方式，很难真正讨论各种方法的好坏。

### 3.5 Chatbot系统会演化成Agent系统：“人和Agent交互的方式是什么样？”

张小珺：对于未来12到24个月，Agent领域有可能发生的事情，你有哪些预测？

姚顺雨：首先，这些模型公司的Chatbot系统会演化成一个很自然的Agent系统，它是一个很自然的过渡。

Grok、ChatGPT或Anthropic Cloud，默认的交互方式会是Agentic（智能体式的）交互方式。Chat可能还会保留或作为一个子集，但Agent会成为一个很显然、更重要的交互方式。

会有新的类似Cursor的产品出现，Cursor是在coding和IDE（集成开发环境）环境下做的Copilot（辅助编程助手），但我觉得会有机会做一些新的环境或更大环境下的Copilot。

这两种大的交互方式是互补的，或者说不一样的正交的。

一边是，我有一个基于模型的，可能是一个remote（远程）的Virtual Machine（虚拟机）或者Environment（环境），我在里面做很多事；另一边是，有很多既有的环境，比如既有的软件，或者既有的场景，我把Agent或AI能力引进去。

大趋势可能是，两方面都会往下发展。

李广密：如果我们想推动Agentic能力变得更强，要在哪里做工作？是在Pre-Training做工作还是在RL做工作？如果我是一个应用创业者，这两个东西是做不了的，最多尝试一些端到端RL的过程，对吧？

姚顺雨：最重要的还是想清楚价值，你应用的价值是什么，痛点是什么，要解决的问题是什么？

虽然你不能做Pre-Training，但更有价值的是：Agent和数字世界的交互环境是什么样的？（是基于MCP还是API，还是别的东西？）人和Agent交互的方式是什么样的？

这两个是你可以去做的，并且它需要很多设计、很多基础设施、很多工程，需要各种各样的东西。现在还远远不够好，有很多进步空间。

还有另一个很重要的是：怎么构建一个生态系统，或者怎么积累用户的Context（上下文）或Intention（意图）？这还有很多可以做的空间。

李广密：你刚才提到Agent Infra（智能体基础设施），如果两年后Agent已经大爆发，巨量的Agents在数字世界运行，需要重新帮Agents设计一套新的数字化系统吗？

Agent需要的虚拟机、电脑、浏览器、搜索的API、身份认证、经济系统等等，这套Infra是为Agent设计的，而不是完全为人设计的？

姚顺雨：我个人感觉两年以内，这个世界还不会变得这么分布式，还是更偏中心化。就是说，会有一些Super App。

当然现在有很多创业公司，但做得好的就是那么几家。两年内还是会有些Super App，这些Super App会有各自的Infra，有各自的Environment或交互方式。

两个事情都可以做到极致，就是一个是基于用户local（本地）的Digital Environment（数字环境），比如我有个手机，有个电脑，有个软件，我已经在这了，我怎么把它去扩充，怎么把它变得更好？

另一个是从头创造新的Environment，比如我做Deep Research或我做Operator（操作者），我实际上创造一个新的Environment。这两个事都还有很多可做的空间。

张小珺：两年后呢？

姚顺雨：这个世界变化很大。有些像科幻的预测、想法或图景。没有人可以预测两年后发生什么。

张小珺：在你看来，大型科技公司是否应该重新开启Pre-Training叙事？（自己从头探索Pre-Training）

姚顺雨：这里面涉及cost和value取舍。现在做的人很少，是因为成本非常大，但带来的additional value（额外价值）没有那么大。

即使你做完Pre-Training，你还需要做Post-Training、RLHF（基于人类反馈的强化学习，Reinforcement Learning with Human Feedback）等一系列工作，才能真正把模型价值释放出来。

但如果有一天，这个世界上存在很多不同的Super App、不同的交互，它们需要不完全相同的模型能力，甚至需要不同的模型，这些差异的价值足够大，能够证明Pre-Training的成本是合理的，那么Pre-Training就是合理的。这最终是value和cost权衡问题。

李广密：Pre-Training和RL未来的关系会是怎样的？会不会更多先验知识被放到Pre-Training里？

姚顺雨：我一个不成熟的想法是：不同应用需要不同形态的Agent，构造方式可能不一样。

如果我只需要下围棋，我直接做AlphaGo就可以了，不需要Pre-Training，也不需要其他。

如果我有一个非常垂直的场景，这个场景价值足够大，我又有很多数据，可以形成闭环，我也许基于一个主要由RL驱动的系统就能work。

像Google的广告系统或TikTok的推荐系统，有点类似这样的系统——我找到了一个足够封闭的环境，做类似RL的事，就可以带来足够多价值，那这个路径是合理的。

但这个世界上还有很多长尾任务，它们需要泛化，需要构建一个更像人的系统。你虽然不是无所不知，但你可以学习，你可以通过在线学习进入一个新的公司、适应环境、完成新的任务。在这些地方，Pre-Training重要性会更高，因为它带来更强的泛化性。

所以不同应用会有不同技术路线。但技术路线毕竟是工具，只要你的value大于cost，技术上的选择是flexible（灵活）的。

没有哪种技术路线一定会胜出。只要它在经济上成立，就有可能性。

## 04 人类的全局

### 4.1 人与系统，Agent要不要像人？“是一个效用问题”

张小珺：在你研究Agent的过程中，对于人，你有更深的认知吗？怎么看人和Agent的同与不同？

姚顺雨：我意识到，人之所以能泛化，是因为人能推理。

这个很有意思。我2018年在MIT Josh Tenenbaum实验室——他是一个认知科学的大佬——我学了很多认知科学的东西。

认知科学，或者计算认知科学，一个核心故事是：我们现在的AI虽然有很多进展，但还有很多问题。我们应该去看看，人有哪些优势，人是怎么做这些事情的，为什么人能把这些事做得更好？比如说，人能够从几个样本中泛化，但机器做不到，为什么？我们要从人身上去寻找这些方法，再把它应用到AI上。

后来我的认知有了变化。我发现，现在真正能奏效的AI系统，跟人还是很不一样。比如Scaling Law、强化学习，还有很多训练策略，它们和人类学习的方法本质是不同的。

我现在觉得，一个更好的方法是：你先去思考人能做什么，而机器现在不能做。这是客观事实。

但你找到差异之后，你可以基于第一性原理去思考，如何解决这个问题。你不一定要依赖“人是怎么解决这个问题的”来解决它。

比如说，人现在能做的事情是什么？我可以进一家公司，在里面工作7天，我能积累公司的Context。即使我不是很聪明，但我依然能完成很多AI做不了的事。这个差异客观存在。那怎么解决？

可能认知科学或神经科学会告诉你：人脑有海马体（Hippocampus），有情节记忆（Episodic Memory），有某种架构或机制。但我觉得，我们不需要完全照搬生物机制。可以从第一性原理出发，设计Long-Term Memory该怎么做。

所以，从人身上可以借鉴的一点：哪些事情是人可以做，而机器目前不能做？这点比较robust（稳固）和客观。但至于“人是怎么做到的”，以及“我们在多大程度上要借鉴这种方式”，这个问题本身更主观、也更 noisy（带噪声）。

神经科学或认知科学也没有100%解答这些问题，只提供了猜想或理论模型。另外，即便被证实，比如人类视觉是目前研究比较透彻的领域之一，人类大脑有六层皮层（cortex），每一层有各种结构和功能。但从这里获得的启发是：我们也许要构建新的神经网络，而不需要照抄那些细节。

张小珺：比方说，设计Agent在什么情况下，需要它越来越像人？什么情况下需要它不像人？

姚顺雨：Again，这是一个Utility Problem（效用问题）。

很多问题上，人的方式并不一定更有价值。比如下围棋、开车。我不知道。大多数人可能开车的方法并不好，也许基于规则有更好的开车方式。但有些事情，人就是做得更好。那你就应该思考，怎么去bridge the gap（弥合这个差距）？

下围棋、打游戏，基于强化学习可以学到和人不一样、甚至更好的方式，就不需要像人。

但如果在一个公司打工，和老板搞好关系，完成各种各样的任务，人就是比AI做得更好，就需要更像人。

张小珺：你怎么思考人和Agents未来的关系？

姚顺雨：这是一个交互方式的问题。

很有可能有很多Agents，长得并不像人，和它交互的方式并不像人——可能是平台、页面、游戏，或者别的东西。你就不会把它拟人化。当然，肯定会有很多拟人化的Agent。

李广密：如果Agent有了长期记忆，它是不是就是你的朋友了？如果它是你的朋友，人和Agent就平等了，是不是我们就要给它发身份证了？

姚顺雨：发身份证的目的是什么？

李广密：它作为独立个体跟我们共存。

姚顺雨：会有可能吧。这些事情最终还是从Utility（效用）出发。

一个事情如果有价值，就会产生。比如，很多人很孤独，他需要一个朋友，技术如果能创造这样的体验，拟人化就是合理存在的未来。

但如果它去做一个平台、一个推荐、一个游戏，这个技术会有很多不同的交互方式，让你感觉它不像一个人，或者你根本感觉不到有区别。你就不会把它看成拟人化。

还是会基于这个事情的经济价值。

李广密：你提到经济价值。你觉得AI Agent跟Crypto（加密技术）未来有结合的地方吗？

比如，Crypto这一套智能合约机制，如果跟Agent结合，在未来有没有可能是这样：一个Agent帮我完成某个任务，这个任务有一个公允价值计量。任务完成之后，就可以按照智能合约的约定去分配经济利益。

这样是有机会探索出一种叫做value-based（基于价值的）商业模式。只是说，现在我们还不太能准确衡量这个任务的客观供给价值是多少。

姚顺雨：我对Crypto了解不多，但可能一个核心问题是：这个技术的演变，会变得更中心化还是去中心化？——两边都有argument（论点）。

中心化论点是：现在这种新的超级公司，OpenAI或Anthropic，它们有可能变成one trillion、ten trillion、hundred trillion（万亿、十万亿、百亿万亿）级别的公司。它们可能会占据绝大多数资源，尤其是算力，也有能力去创造出一个Super App或Super Platform（超级平台），拥有巨大中心化优势。

而去中心化argument（论点）是：每一个个体都可以被赋能。现在人和人之间之所以差距这么大，是因为存在信息差、认知差、智能差。如果智能变得便宜，像电一样，它也可以赋能给大多数人。

这个问题挺有意思的。

我最近的一个思考是这样：我感觉人类社会是一个网络，它有两个重要性质：

一个性质是中心化程度，也可以说是资源分配的集中性。我们发现，原始社会是非常平均的社会，但随着技术发展，它变得越来越中心化。你可以用二八定律、马太效应、或whatever来解释这种趋势。
但还有另一个维度，是你从网络边缘到中心的速度或可能性。
过去几百年发生的事情是这样：网络越来越中心化，贫富差距越来越大，二八定律、马太效应更明显；但与此同时，平民或普通人翻身的机会也变多了。

如果是在古代，门阀制度、九品中正制，或者欧洲贵族制度，农民永远是农民。印度种姓制度也一样，有明显的阶级固化。

看起来，技术发展的趋势是两件事同时加剧——一方面，中心化加剧，因为效率这个因素是根本性的；另一方面，创造新东西的机会，起码到目前为止，是越来越多的。

变得更中心化和变得更diverse（多样化），可能并不矛盾。

但未来是不是一定会持续下去，也不好说。

### 4.2 OpenAI的抉择时刻：“如果你没有different bet，很难超越前面的霸主”

张小珺：我想聊聊OpenAI。我记得你提到OpenAI的几次尝试很有意思。

它最初的计划是构建Gym，一个用于各种游戏的标准强化学习环境。后来是World of Bits和Universe项目，试图把整个互联网或计算机交互编程成一个游戏。一旦能把整个数字世界变成一个环境，用聪明的强化学习算法解决它，就拥有了AGI。

但这套思路并没有奏效。直到GPT-2和GPT-3出现，人们才意识到，之前缺失的是先验知识。你需要一个强大的语言预训练过程，把一般常识和语言知识提炼进模型中。再通过微调，让它成为一个能浏览网页的或能对话的智能体。

你能不能更详细讲讲，OpenAI探索过程背后的思路演化？从Gym到Universe到GPT这一整条路径的尝试中，转折点是怎么发生的？

姚顺雨：这是我自己的总结和揣测。

OpenAI是一个比较bottom-up（自下而上）的公司。在最初7、8年里，它更像是一个research lab（研究实验室），每个人有各种各样的想法，做各种各样的尝试。可能每个人想法都不一样。

但客观看，一开始大家的重点还是聚焦强化学习，当时最火的方向是这个，对吧？

DeepMind大概2010年成立，那时AI领域最受关注的公司是DeepMind，它最成功的成果也是强化学习。GPT出现前，最成功的AI项目是AlphaGo。很自然，OpenAI也做强化学习。

但问题在于，如果你没有一个different bet（不同的下注方向），很难超越前面的霸主。如果OpenAI一直做强化学习，可能很难超过DeepMind。即使你在某些任务上做得比它好，人们提到强化学习，想到的还是DeepMind。

你要想超越之前的霸主，就必须有一个different bet。而GPT是那个不同的赌注——但这个选择在当时是一个非共识的事情。

我可以讲个例子：我导师是GPT‑1第二作者，他在OpenAI待了一年，然后去普林斯顿当教授。他对这件事是有点怀疑的。

他觉得GPT‑1的结果也不是特别好，在排行榜上也不是分数最高，而且训练花了很多算力。当时已经有Scaling Law初步雏形。2017年，Ilya就跟我导师说：”Language is basically solved, and we just need to scale up." 语言模型的问题已经被解决了，现在只需要扩展规模就行了。

但即使你在OpenAI，即使你是GPT作者，你也可能没有形成共识。所以OpenAI当时做的是一个非常反共识的决定。现在已经变成了共识。但接下来，你还需要寻找下一个反共识的方向。

张小珺：当时其他人对你导师的看法是怎样的？

姚顺雨：我说实话，当时OpenAI内部绝大多数人也不认为scale-up（扩大模型规模）是最promising（有前景）的方向，我觉得这是有可能的。

Ilya最大贡献并不是他做了GPT‑1，或者他具体参与了什么技术工作；而是，他是那个号召大家all in（全力投入）这个方向的人。

Dario（Anthropic联合创始人兼 CEO，曾是OpenAI研究副总裁）也是。他最大贡献不是提出某个具体技术，而是：作为一个创始人，我敢赌。我敢赌这个方向，把所有钱砸进去。

李广密：有人愿意去做GPT‑3是特别关键的。像Dario也好，Tom Brown（Anthropic联合创始人）也好，他们敢于把GPT‑3做出来，这件事让人看到了更大希望，也泛化了。

姚顺雨：对，当然好处在于，你并不需要所有人达成共识。只需要有足够多人达成共识，就可以把它做出来。

张小珺：对于OpenAI内部来说，强化学习在什么时候开始变得特别重要？

姚顺雨：强化学习一直很重要。即使我在做GPT的时候，John Schulman（OpenAI联合创始人之一，强化学习领军人物）还是在继续做强化学习。并不是我做了GPT就把强化学习扔掉了。而是公司70%、80%的资源在做强化学习，一些别的东西还在做。

后来证明，ChatGPT成功，强化学习也很关键。没有RLHF，没有Alignment（对齐）技术，它也没办法形成一个产品。

历史并不是说我把强化学习彻底抛弃，转而走另一条路，再返回来走强化学习，而是更soft（柔和）的过程。

李广密：接下来几年，你预计会有更多GPT‑3时刻吗？

姚顺雨：会有新的scaling dimension（扩展维度）出现。如果你有大量的Memory（记忆），你的test-time compute（测试时计算资源）就会有所增加，可以用新的方式scale（扩展）。

如果你有了Multi-Agent（多智能体系统），那你的test-time compute又会出现另一个新维度去扩展。

我觉得会有新的scale dimension出现，但当你有很多scale dimension，怎么去选择？怎么基于某一个应用去分配不同scale维度的比重？——这会是一个很有意思的问题。

### 4.3 假若你是一个CEO：“首先我肯定会学习”

李广密：顺雨，如果你是一个全球超大互联网或科技公司的CEO，今天这个公司还没有自己的模型，没有好的研究文化，甚至没有好的AI战略，你作为CEO会怎么做？

姚顺雨：首先，我肯定会学习，我会想弄清楚这个事情到底是什么。如果你作为CEO不懂这个事情，所有事情会变得很难。

很多时候，一个公司的bottleneck（瓶颈）就在于，CEO 对这个事理解不够。如果你不理解，去招一些很好的人、做一些事情，你很可能被他们忽悠。所以，首先要自己学习。

然后要从创造新的价值来思考问题。毕竟你不是技术专家，而是一个CEO，你有一些场景、一些资源、一些优势。从第一性原理看，一个新的技术产生了，你要思考的是，怎么用这些新技术结合你现在的资源去创造新的价值。

当然，你可以尝试做一个和当前业务完全不一样、但价值非常大的事情，比如ChatGPT，但对大多数公司来说，即使很有钱、很强，也不一定make sense（合理）。

所以，第一是自己要学习技术；第二是要思考怎么创造新的价值。

李广密：如果你成为了伯克希尔的CEO，未来要拿出500亿美金allocate（分配）到AGI行业，你会怎么allocate这笔钱？——既能体现回报，也能体现对人类的贡献。

姚顺雨：这是个很好的问题。取决于你有多少精力，或者有多少资源分配颗粒度。

当然现在OpenAI、Anthropic，这些模型层公司，大概率会有更大价值。

还有一类很有价值的，是能积累User Context（用户上下文），或者能构建特殊Environment（环境）的公司。最终如果AI或AGI是一个系统，它需要有Intelligence（智能），需要有Environment，还需要有User Context，或者对用户的理解。

现在有很多User Data（用户数据）或User Context 的公司，有点像发明车之前的煤炭、煤矿，或者像发明汽车之前的石油公司。

从这个角度，微信或大平台，还是一个易守难攻的好平台，它积攒大量的Context。

如果Intelligence是一个可以逐渐民主化、逐渐变得便宜、逐渐普及，拥有这样的平台，拥有这样的Environment，拥有这样的Context，可能会是一个很强的壁垒。它可能还是一个很好的投资。

李广密：如果你是Cursor的CEO，你会去做Pre-Training的事情吗？

姚顺雨：我肯定会训练模型，或者尝试训练模型，但做不做Pre-Training看情况。

Coding是非常主线的任务，所有大厂都会把模型的coding做好。所有的Pre-Training、Post-Training、RL，都会考虑到这一点。

这个情况下，要不要做可能取决于，首先这些闭源模型做得有多好，其次开源模型做得有多好，中间有多少gap，你能填补多少这样的gap。

但当然，如果你有很多钱，有很多资源，想把这事情做了，也是合理的。

张小珺：今天顺雨当了很多公司的CEO，那我再问一个：如果你是微信的一号位，你会怎么在微信里做Agent？

姚顺雨：我可能会不急，先观望观望。

我好像没有理由要急。我会观察，我会学习 AI，会观察有没有什么新的交互方式很有意思。但我不会急着去做很多事——我有易守难攻的地方，为什么要急着进攻？

比较危险的是一个颠覆性的创新。真正的危险，不是说一个类似于微信的东西打败了微信，而是一个很不一样的东西打败了微信。

就像微信打败了QQ。当时担心的并不是一个类似QQ的东西打败了QQ，而是一个很不一样的产品去打败这个东西。需要对颠覆性创新有所警惕。

但如果是这些incremental（渐进式的）创新，这种小的创新，早做晚做可能区别没有那么大，也不用太担心。

李广密：所有人都说微信卡位好，但今天微信还没有很激进地投入，如果未来Multi-Agents 、Long-Term Memory这些问题解决了，但这个Agent系统不长在微信上，是比较恐怖的。原有网络不一定有价值。

姚顺雨：这取决于人类的网络会变成什么样？你会有更多Agent朋友，还是更多人类朋友？或者你有更多Agent职业上的交互，还是有更多人类职业上的交互？

微信上你既有朋友，也有基于职业的交互——比如我要买个东西，我要咨询律师，对吧？

这取决于人类的网络会变成什么样。但总会有一个这样的网络，基于这个网络，肯定会需要有基础设施，需要有平台。

李广密：怎么保证AGI实现之后的安全问题？微信过去还是一个比较负责任、比较安全的平台，那如果未来power（能力）很强了，很多坏人来做坏事，甚至颠覆人类，安全问题长期怎么解决？要有AI宪法吗？

姚顺雨：安全是很复杂的问题。比如ChatGPT，如果它不安全，产品就失败了，没有商业价值。即使是为了商业价值，它也会重视安全。

但现在的主要分歧是，需不需要产品之外、更意识形态上的安全？这个大家没有定义清楚。

前者容易解决：如果你有一个好的应用，你总会有办法解决安全问题，我相信。至于第二者，会有很大不确定性，我很难评价。

李广密：你个人会担心AGI实现之后的安全问题吗？

姚顺雨：我会担心。但现在最大问题是——AGI还没实现，我们还没创造足够价值。

如果我们还没想清楚，怎么把它变得有价值，就急着把它变得很安全，好像没有意义。

张小珺：你作为AI研究者，博士期间工作已经获得了很多关注，在你眼中，你做对了什么？

姚顺雨：我想做的就两条线：简单通用的方法、有实际价值的任务。这些任务往往是，如何在真实数字世界创造新的价值。这是一个处女地，是一个巨大的宝藏。我恰好挖掘了一些东西。

需要你想得足够大胆或足够通用吧。

另一个很重要的是：要去看很多东西的交界处。ReAct之所以能做出来，因为我们选了一些自然语言处理的任务，也选了一些游戏的任务，需要把自然语言处理和强化学习的边界打通。但很多人会陷入一个学科内部，就更难去做更通用的东西。

张小珺：ReAct在做的过程中有遇到什么坎吗？

姚顺雨：最难的都是找任务。

大多数好的方法提出，是因为它有一个特定任务，这个特定任务恰好激发出一个通用方法。比如PPO（Proximal Policy Optimization，一种强化学习优化算法）一开始是为了解决一个特定问题；Transformer一开始是为了解决一个特定任务；Attention（注意力机制）受翻译这个任务影响很深。

但我的经历比较特殊，很多时候我是脑子里先想到一个东西，我觉得它很通用、很好。但我要去找一些任务，证明它很通用、很好，或者未来有价值。它现在还没有足够多价值，但你需要先找一些简单任务去证明它有价值。这是很难的。

创业需要product market fit，做research需要method-task fit（方法和任务的匹配）——这是最难的。

张小珺：你曾经想到最激进的一个任务是怎样的？

姚顺雨：这个时代再激进也不叫激进——Anything is possible。

毕业前我想得多的是，怎么创造一个爱因斯坦？我那时是比较academia（学院派）的人——你在普林斯顿，你的偶像是冯诺依曼、爱因斯坦——很自然，能想到最有意思的任务是，我能不能发现下一个相对论？这毫无疑问能标志，AGI或ASI（超人工智能）实现了。

后来，我到了硅谷，到了加州，进入公司之后，我发现人类的组织也是一个有意思的事情。如果能创造一家新的公司，创造一个one trillion dollar（一万亿美元）、基于Agent的公司，是很有意思的。

张小珺：为什么是人类的组织也很有意思，而不是人类的产品很有意思？

姚顺雨：产品当然很有意思，但很多组织的方式，就像一个general method（通用方法），能创造很多不一样的伟大的东西。比如股份制、组织架构，它就像非常通用的AI方法一样，创造了很多不一样的伟大的东西。

张小珺：在你的成长路上，你的mindset（思维方式）跟同龄人差不多吗？还是不一样？

姚顺雨：我的路径挺按部就班的，也没有跳级，没有做什么很surprising（让人惊讶）的事情。但我对一个东西的价值，或者taste（品味），有自己的看法。大家往往会倾向于做一个确定性比较高的事情，包括做研究、做公司。

但我觉得恰好是这个时代，你去做上限更高的事情是更好的。

因为现在有一个巨大的机会。如果没有这样一个巨大的机会，最佳路径可能是去做incremental（渐进式）、确定性强的事情，一步一步地积累。但恰好有一个上限非常高的事情。

如果你敢想，或者你胆子特别大，或者你想象力很丰富，就会有好事发生。

张小珺：在你成长路上，对你启发大的是什么？是书、电影、音乐？哪些东西塑造了你的mindset？

姚顺雨：看书挺有帮助，我是一个喜欢看杂书的人。什么书都看，什么电影都看，什么地方都想去。

我从小是一个比较general的人——我想试图变得很通用，试图了解很多不同的学科，做很多不同的事情。

但后来我发现，一个人即使再聪明、再有精力，他能理解的知识或能做的事情，也只是人类社会积累的知识的很小一部分。更好的是，你去创造一个比你更通用、更general的事情。

我好像一直对于通用性，有一种执念或追求。

张小珺：通用性意味着什么呢？——可以足够简洁？

姚顺雨：我不知道，但我从小就是想学习很多不同学科，都很有意思。

我在姚班很多同学，他们是那种很deep（深度的）、很focus（专注的）同学——我去做竞赛，我就把这个事做到极致，不停刷题，做到世界金牌。

但我好像不是那种性格，我是那种——我会看很多数学，也会看很多历史，会看各种各样乱七八糟的东西。

张小珺：你会刷竞赛吗？

姚顺雨：我也搞竞赛，但没有本科同学那么厉害。我是信息学拿了全国银牌。

张小珺：你是清华的说唱社联合创始人，对吧？我昨天去翻了一下你的网易云音乐。

姚顺雨：被你找到了？看来你有Deep Research的能力。

张小珺：你最喜欢的说唱歌手是谁？

姚顺雨：我有很多喜欢的说唱歌手。说唱很有意思，每个人风格都很不一样，这点是很多人喜欢说唱的原因——你有自己的个性、自己的flow（节奏）、自己对生活的思考，你可以创造不一样的东西。它不一定是最好的，但大家是不一样的，这点很吸引人。

张小珺：它跟你做AI有相似之处吗？

姚顺雨：GPT-3刚出来，大家都觉得很厉害嘛，我想到第一个做的就是，看看能不能生成说唱歌词，并且有内容性。似乎今天还是很难。也许说唱歌手是一个被人们低估的工作。

张小珺：填词，这不就是predict next token（预测下一个词元）在做的事情吗？

姚顺雨：一个东西好听、flow好、听上去舒服，是很难被量化的reward。很多时候一个东西，比如flow或style，它出现太多了，就不好了。独特反而是好的。真正伟大的说唱歌手，有很多独特的对生活的思考，而AI还没有生活。

张小珺：有可能有对于智能来说，比语言更本质的存在吗？

姚顺雨：在特定领域，肯定有比语言更好的表示，比如围棋。

但语言的诞生，不是为了处理某个特定任务的效率或交流，它为的是打通所有任务或者打通人的认知能力，形成一个通用的表示。

它并不是为了某个特定任务最优而优化，它在特定任务上有冗余性，但它整体是通用的。

AI当然也可以创造一个新的语言，可能效率更高。但我觉得，最终大概率就是英语。因为人类已经有很强的先验知识，而且人有这样的价值取向或动机，想让机器的语言和人更像。

这样，我们可以更好地理解它、控制它、监控它、改变它、操控它，似乎是个很自然的选择。

张小珺：你内心的驱动力是什么？你的愿景是什么？你10年后想成为谁？

姚顺雨：用一个非常俗的话说，希望你对这个世界创造一些不同——探索新的、根本性的研究，是一种创造不同的方式；创造一种完全不同的新的产品形态，也是一种创造不同的方式。

如果我现在去做一家类似xAI或Thinking Machine的公司，或者做一个类似Chatbot或Assistant的产品，还是可能赚很多钱，商业上很成功；但如果我做了一个形态很不一样的东西，失败了——我起码探索了不一样东西，会更有意思吧？

我导师令我印象最深的是这样一句话。学术圈经常发生这样的事——你有一个想法，然后别人做了，你会很烦。他说：If someone else can do it, then it's okay to let them do it（如果别人能做，那就让他们去做吧）。

从人类全局的角度，如果这个事情很多人能做，别人做可能是不是也没有什么区别？对这个社会，或者对整体来说，似乎没有什么变化。

当然，有人说这个非常假。最终你会发现，这个世上没有什么事情是不可替代的。相对论即使没有被提出，也会有人提出，没有什么事情是你不在，另一个人不能提出了——但是，我觉得这话还是有道理的。

如果你很清楚看到别人就在做这个事，你可以选择去和他卷。如果你要和他卷，你更有效率，或者你能做得更好，也是合理的。或者，你也可以去做一些不一样的探索。

我觉得，最终你要对这个社会产生价值。

但这个时代很幸运的一点：这个技术非常通用，这个技术非常伟大，有足够多探索的空间。

另一点是，我想让生活更有趣，更有意思，更快乐，就去做一些自己喜欢的事情。这很难用语言解释，就是一个taste（品味）或preference（偏好）的问题。

张小珺：你会考虑创业吗？

姚顺雨：OpenAI大多数人都会考虑创业。现在是非常exciting的时候。已经有很多OpenAI的人出去创业了。我需要去做更有挑战的事情，很自然会去创业。

但还是应该找到一个好的事情。我喜欢把事情想得清楚一点再去做。

## 05 快问快答式结尾

张小珺：我们最后还有几个快问快答。

姚顺雨：好。

张小珺：一个全球范围内你喜欢的食物。

姚顺雨：我喜欢椰子。

张小珺：一个全球范围内你喜欢的地点。

姚顺雨：我很喜欢伊斯坦布尔。

张小珺：一个少有人知道但是必须知道的知识点。

姚顺雨：我挺建议大家去看《智能简史》这本书。有很多很有意思的知识点。

为什么大多数动物都是左右两侧对称，并且有一个像嘴一样的食物入口，有一个像肛门一样的食物出口？为什么气体是同一个口，而食物和水是两个口？这个很有意思，有些本质原因。

张小珺：什么本质原因？

姚顺雨：你会发现，如果你要做navigation（导航），在这个世界中移动，左右对称的结构最优。世界上所有交通工具都是左右对称的。因为你可以一个方向前进后退，另一个方向向左转向右转。它和车和飞机都是左右对称，结构是类似的。

至于食物和气体还有别的原因。

张小珺：基于你所有读过的书，推荐两本必读书。

姚顺雨：《智能简史》这本书很有意思，是我去年读的。

我会推荐各种各样的自传。传记很有意思，好像你在体验别人的生活。

张小珺：你心目中影响 AI 进程的几篇论文。

姚顺雨：有很多，我觉得没有最重要——Backprop（反向传播）、Transformer（变换器）、GPT（生成式预训练变换模型）——都是积累的过程，没有一个是最伟大的工作。

李广密：你会对Agent创业者有什么建议吗？

姚顺雨：可能有点老套：想清楚你的价值是什么。技术是工具，理解技术趋势很重要，但创造价值是最重要的——想清楚你为用户带来了什么样的增量价值，这是最主要的。

张小珺：基于你当下的认知，一个最关键的重要的bet是什么？

姚顺雨：就是bet on有different Super App（不同的超级应用）的产品形态，有不同的交互方式。

如果你不相信这一点，世界就变得很灰暗，就是只有OpenAI或者Anthropic有机会。

但如果你相信这一点，就会有很多新的机会。
