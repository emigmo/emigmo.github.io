# Nick Joseph访谈：Anthropic预训练的核心思考与实践

_访谈对象：Nick Joseph（Anthropic预训练团队负责人）_  
_时间：2025年10月10日_  
_来源：Y Combinator深度访谈_

---

## 前言

本文基于Y Combinator于2025年9月30日对Anthropic预训练团队负责人Nick Joseph的深度访谈整理而成。Nick Joseph曾在Vicarious和OpenAI从事AI安全与模型缩放研究，深度参与了多代大语言模型的开发与优化。

**核心观点速览：**

- 预训练的核心是推动损失函数下降，这是我们一直追求的唯一目标
- 对齐问题在于如何让模型分享人类的目标，尤其是在模型比人类更聪明时
- 如果拥有无限计算资源，真正的挑战将是如何有效利用这些资源并解决规模扩展中的工程难题
- 当前AI研究最大的瓶颈之一是计算资源受限，而非算法突破

---

## 目录

- [一、从AI安全初心到Anthropic预训练掌舵人](#一从ai安全初心到anthropic预训练掌舵人)
- [二、预训练基石与扩展定律：自回归建模成为AI发展的核心引擎](#二预训练基石与扩展定律自回归建模成为ai发展的核心引擎)
- [三、工程实战深水区：万卡集群、硬件极限调试与分布式训练挑战](#三工程实战深水区万卡集群硬件极限调试与分布式训练挑战)
- [四、合成数据风险、模型评估指标设计与价值嵌入博弈](#四合成数据风险模型评估指标设计与价值嵌入博弈)
- [五、AGI之路：范式转移焦虑、架构变革与未来展望](#五agi之路范式转移焦虑架构变革与未来展望)

---

## 一、从AI安全初心到Anthropic预训练掌舵人

### 1.1 从Vicarious到OpenAI的早期探索

**Ankit Gupta**：大家好，今天非常高兴邀请到Anthropic的预训练负责人Nick Joseph来和我们对谈。我想先聊一聊你的背景，了解一下你是如何走到今天这一步的。

**Nick Joseph**：我之前在Vicarious工作，后来去了OpenAI，然后才加入Anthropic。Vicarious最初是一家以AGI为目标的实验室，我加入时他们正处于转型阶段，逐渐开始开发一些具体的产品，尤其是机器人相关的项目。我当时负责的主要是为机器人产品训练计算机视觉模型。那是我的第一份工作，所以我在那段时间主要学会了如何构建机器学习模型，以及如何搭建机器学习的基础设施。

### 1.2 选择实践而非学术的职业路线

**Ankit Gupta**：那时候你有没有考虑过走学术路线？

**Nick Joseph**：其实我对这件事的想法有点不同，这很大程度上源于我之前在一个叫GiveWell的非营利机构实习的经历。GiveWell主要负责评估慈善项目的效果，当时那里有一些人提出——也许未来我们会拥有AGI，而它可能带来风险，需要提前关注这种潜在的威胁。

那时我对这种说法并不是特别信服，我更倾向于直接去做一些可以帮助贫困人口的事情。后来因为种种原因，这条路没有走通，于是我决定至少去做AI相关的工作。这样无论未来AI安全是不是一个重大问题，我都能有所贡献：如果它真的重要，我就投身其中；如果不是，我也能用AI创造出一些能切实帮助贫困人群的东西。

> **所以我并不是以学术研究为出发点来进入这个领域。实际上，我之所以选择这条路，其中一个吸引我的地方是——我可以立即投入到AI实践中去。**

### 1.3 AI安全的早期状态与哲学探讨

**Ankit Gupta**：那么，当时AI安全这方面的研究处于什么状态呢？

**Nick Joseph**：在我看来，当时关于AI安全的大部分讨论都还停留在理论层面。那时候的模型其实并没有多强，也不存在真正的威胁。因此，当时的讨论更多是哲学性的——比如，假设未来我们会拥有比人类更聪明的AI，那我们是否应该提前重视这种潜在风险？

### 1.4 加入OpenAI与代码能力的惊人突破

**Ankit Gupta**：接下来你去了OpenAI。当时的OpenAI是什么样的？

**Nick Joseph**：我当时加入的是一个安全研究团队，同时也在参与代码模型的相关研究。刚到那儿时，我看到的第一个项目就是他们对GPT-3进行微调，让它能够写代码。而且效果相当好。

这让我突然意识到——如果人们担心AI会变得极其强大，甚至能自我编写、改进自己的代码，那么这种能力确实看起来有可能导致自我提升。于是我开始做很多评估和研究，分析哪些因素会促成这种能力。

大概八个月后，我所在团队的几位AI安全负责人相继离开，而我之所以加入OpenAI，本身就是因为我非常关心AI安全问题，希望能和他们共事。后来他们中的一些人去了Anthropic，我也几乎在Anthropic成立初期就跟随加入了。

---

## 二、预训练基石与扩展定律：自回归建模成为AI发展的核心引擎

### 2.1 什么是预训练？从互联网数据到下一个词预测

**Ankit Gupta**：既然提到了Anthropic，我们就来聊聊你现在的工作吧。如今你是Anthropic的预训练团队负责人。你能先谈谈什么是"预训练"吗？

**Nick Joseph**：我们知道让AI模型变得更强的关键要素之一就是"规模"。你需要投入大量的算力。而如果退一步思考，要想让模型获得尽可能多的算力，我们就需要一个拥有海量数据的训练目标。

一个显而易见的想法是——互联网本身就是庞大的数据源，可能是人类历史上最大的数据集合。而且这些数据是无标签的，你不可能指望有人去把整个互联网内容都看一遍、逐条标注。因此，我们希望能从数据本身提取出"标签"。

> **于是，就有了这样的思路：我们可以让模型预测下一个词。这种方式的好处是，你能得到非常密集的学习信号——每一个词都是一个新的训练样本。**

从GPT-1到GPT-2的研究发现，只要你持续增加算力、使用更多数据、训练更大的模型，模型的能力就会不断增强，表现也会更智能。可以说，这就是整个预训练理念的核心假设。

### 2.2 扩展定律与正反馈循环

这里还涉及"扩展定律"的概念，也就是说，我们可以用相当精确的方式去量化：当你增加算力、数据量或模型参数时，模型在预测下一个词时的损失会以可预期的方式降低，性能也会相应提升。

真正出乎意料的是，这一机制形成了一个**"正反馈循环"**：

1. 你训练出一个模型
2. 它能被用来创造有用的产品
3. 这些产品带来收入
4. 你再把收入投入更多算力
5. 从而训练出更好的模型

过去五年左右，我们其实反复地在运行这一循环。

### 2.3 为什么自回归建模胜出？

**Ankit Gupta**："下一个词预测"这种自回归方式似乎已经成为主流的预训练方法。但如果回到2017年至2020年，当时其实存在各种各样不同的预训练目标。例如BERT、BART这些模型采用的是"掩码语言建模"。你对那个阶段有什么回顾或感想吗？

**Nick Joseph**：我认为答案主要是经验驱动的——换句话说，我们是通过大量实验得出的。我的观点是，这类问题最终要靠实证：都试一试，看哪种更有效。

**自回归建模的巨大优势：**

- 你可以直接从模型中采样生成文本，这个过程非常自然
- 损失函数本身就能直接反映出你真正关心的目标
- 如果你能把这个任务做到完美，模型自然就能像人一样写作
- 容易转化为产品应用

相比之下，其他一些方法并不具备这种天然的生成特性。

### 2.4 算力才是王道

**Nick Joseph**：没错，这种方式赋予了模型最具开放性的潜力。一般的流程是：你先训练一个基础模型，然后针对不同任务进行微调。

不过我总体的直觉是：**真正起决定作用的是算力**。只要你投入足够的算力，无论采用哪种预训练目标，最终都能训练出表现不错的模型，然后再通过微调适配各种用途。

> **令人意外的是，很多我们以为关键的细节，其实远不如"增加算力"来得重要。**

---

## 三、工程实战深水区：万卡集群、硬件极限调试与分布式训练挑战

Ankit Gupta：确实如此。而且当我们谈到“增加算力”时，这本身也有很多维度。比如，对于一个固定的模型架构，你可以给它输入更多数据；或者让模型更深，增加层数或参数量；再或者通过神经架构搜索去尝试不同结构的组合。我想如今大家已经比较明确哪种架构更有效，但在早期阶段，这方面的探索应该更具不确定性。能否谈谈当时你们是怎么思考这些问题的？你们的基础设施又是怎样支持这些探索和决策的？

Nick Joseph：我想，简短的答案是——这真的很难。你要做的事情其实是训练一个庞大而昂贵的模型，而你面对的是一个包含上百个超参数的空间。比如你要决定层数、宽度等等，而你希望所有这些超参数都能最优。你需要在“它们到底重要到什么程度”之间取得平衡——也就是说，你能不能凭直觉选一个差不多的配置，然后只要增加算力就能得到不错的效果？

Ankit Gupta：也就是说，没关系你怎么调都行？

Nick Joseph：对，只要别太离谱。但有趣的是，实际上这并没有那么重要。我记得这在早期的扩展定律论文里也提到过——你可以调整这些参数获得一些小的提升，但只要你持续增加计算资源，模型的性能一般都会稳定地提升。当然，如果你调得太离谱，效果会停止提升，但你又不会知道到底出了什么问题。这其实是最难的部分之一。

Ankit Gupta：因为你不知道反事实是什么，对吧？你没有足够长时间去跑出结果。

Nick Joseph：对。我们有这些“扩展定律”，你可以预期，当你增加计算量时，损失值会按幂律下降——实际上是“幂律+常数”。所以最终你会看到这条曲线开始偏离幂律，这就意味着出问题了。而这时问题可能是根本性的，也可能只是你应该微调一下学习率。如何判断是哪一种，就是一大挑战。通常的做法是先在小规模下测试，再在大规模上运行。

Ankit Gupta：这里的小规模是指数据规模，还是别的？

Nick Joseph：是所有方面。你希望比例缩放，比如如果你要把计算量增加十倍，你就要有一套理论去指导：这十倍的算力该怎么分配——多少给层数、多少给数据、多少给注意力机制。然后在小规模下按比例缩放去验证。

Ankit Gupta：那在Anthropic早期，你们团队可能十来个人的时候，作为一个小型但有资金的创业公司，你们当时能用到什么样的大规模基础设施？

Nick Joseph：这其实是件挺疯狂的事。你永远不知道别人到底在做什么，但我们感觉自己就在最前沿。那时关心这件事的人其实很少。我当时的心态是：我们在做AGI，这是人类历史上最重要的技术。但我环顾四周，发现好像全世界只有30个人在认真干这件事。虽然我只是个初级工程师，但我惊讶于事情的“简单”——比如当时公开估算GPT-3的训练成本是500万美元。听起来多，但从公司角度看其实不算高。所以我们完全能买到足够的算力来训练类似规模的模型。

Ankit Gupta：你们是用云计算，还是自建机房？

Nick Joseph：我们用的是云服务，但其实差别不大。让我意外的是，我们真的得理解硬件的物理布局。有次同事甚至跑了聚类算法来推测芯片分布在哪些机房，因为我们怀疑不同机房间的网络延迟导致训练瓶颈。结果真能“反推”出来两个聚类，连接状况不同。这种极限优化在当时很重要——我们资金比别人少，只能靠算力效率取胜。

Ankit Gupta：那你们具体做了什么来提升算力利用率？这听起来像早期Google那种“便宜硬件+极致优化”的故事。

Nick Joseph：我们主要是把分布式框架调到极致。因为训练要跨大量GPU并行进行。分布式有多种模式：数据并行、流水线并行、模型并行等，要把这些都整合好。

Ankit Gupta：那时候还没有现成的开源框架能直接用，对吧？

Nick Joseph：对，有一些，但很有限。比如我们当时用的数据并行方法需要自己写all-reduce通信，不能完全依赖现有包。因为像PyTorch Distributed虽然有工具，但我们要扩展到比Facebook还大的规模，就得自己写，以便后续能灵活修改。

Ankit Gupta：你刚说“我们要比Facebook更大规模”，这挺颠覆的。毕竟那时Facebook AI Research是最顶尖实验室之一。你们不觉得冒险吗？

Nick Joseph：确实有点意外。但也许我有点自信过头吧。当时我觉得他们都忽略了重点。扩展定律已经很清楚了，而反对的论点听起来没道理。那篇原始论文横跨11个数量级，学界却争论它能不能再延伸一个数量级——我觉得，这种怀疑没什么根据。

Ankit Gupta：毕竟都已经延展11个数量级了。

Nick Joseph：对，所以我觉得继续下去的概率相当高。而且这类规律很多时候真的就是“直接有效”。但我能理解，从外部看就没那么显然——论文太多，每篇都说自己很重要。尤其像FAIR那种地方，研究者更独立，重视发表，而不是去协调一整个庞大的工程项目。训练一个大型语言模型需要几十人协作搭建复杂的基础设施，而那种成果不会是一篇论文，所以那类文化对这事其实并不重视。

Ankit Gupta：你提到你们有时要修改PyTorch底层实现，那你们是停留在高层API还是直接写CUDA？

Nick Joseph：看情况。大多数操作我停留在torch.matmul这种层面，不去管矩阵乘法内部实现。但像注意力机制这种复杂操作，我们会深入优化底层，因为它在GPU上效率很难调。

Ankit Gupta：那你们会先纸上推导理论效率，再实现？

Nick Joseph：对。你其实可以用纸笔算出理论上能达到的最大效率（MFU）。效率差的原因通常就是显存带宽瓶颈、CPU传输瓶颈等等。这些约束项不多，也就六七个。建模清楚后再实现。实现后用profiler分析每步耗时，对比预期，再不断优化。

Ankit Gupta：那时有现成的分析工具吗？

Nick Joseph：单GPU的Python profiler已经不错，但成百上千GPU的分布式Profiling几乎没人做过，我们得自己改写分析器来聚合所有追踪数据。

Ankit Gupta：那你这些都是怎么学的？

Nick Joseph：我入职Anthropic时，内部资料还少，我第一天就把整个内部Wiki都读完了。然后主要靠结对编程学，像Tom Brown、Sam McCandlish这些人之前都做过。我跟他们大量结对，看他们怎么调Profiler、怎么排错。比如我以前从没用过调试器，只靠print调试，后来才发现PDB调试器的效率高太多。

Ankit Gupta：后来你们的预训练规模越来越大，算力暴涨——那策略上发生了什么变化？

Nick Joseph：其实变化不大，真的令人惊讶。到今天我还在盯着和当年一样的指标——损失函数。可以把五年前第一个模型的loss曲线和现在的放在一起看，核心目标一直没变。

Ankit Gupta：所以你们的OKR就是“loss越低越好”？

Nick Joseph：对，当然现在的变化是团队更专业化。早期我会看每个PR，知道所有模块；后来各模块分工更细，有人专攻attention，有人专攻并行策略。但这样带来一个挑战：要平衡“专家”和“通才”。如果全是通才，大家懂点皮毛没人精通；全是专家，又容易出现系统割裂，需要管理者去把体系连起来。我个人更喜欢保持平衡。

Ankit Gupta：那在算力规模暴涨后，有没有遇到一些意想不到的挑战？

Nick Joseph：有，比如最典型的——“连接问题”。当你要并联越来越多的GPU时，最小的硬件故障都可能导致整个训练崩溃。比如一台GPU坏了，整个模型就会挂掉。你可以想象，如果模型每层在不同GPU上，那掉一个“第七层”的GPU就意味着整个网络失效。

还有一点，整个技术栈都太新，从芯片架构到数据中心布局都在快速演进。以前我写代码出错会想“肯定是我写错了”，但现在有时真的是“电脑错了”。比如有次GPU真的坏了，换了之后一切正常。如今你得考虑的变量太多：GPU可能有瑕疵、供电可能波动、数据中心线路可能老化等等，对一个Python程序员来说，这些都是意料之外的复杂性。

Ankit Gupta：那早期你们单次训练大概用多少GPU？

Nick Joseph：上千个，能塞进一个机房。现在则是一整栋楼、甚至园区。那时我们还在研究：这些GPU是不是得放在同一个房间？带宽要多少？供电能不能扛得住？有时候只是一个电容不足，都可能导致整个训练任务瞬间断电崩溃。

Ankit Gupta：那么，你们是否需要考虑不同类型芯片之间的差异？我的意思是，你们和各种不同的云服务提供商都有合作。从你的角度来看，这些芯片只是计算资源吗？还是说如果你们使用TPU和GPU，它们就像谷歌的CPU和英伟达的GPU那样？作为工程师，你在使用这两者进行训练时，是否需要采取不同的思路？

Nick Joseph：是的，从根本上来说，它们做的都是同样的事情，对吧？都是在进行计算。进行同一类操作、矩阵乘法等等。它们实现的方式相当不同，编程方式也很不同。而且实际规格也差别很大。有些芯片可能拥有很多浮点运算能力，但内存不多；有些可能内存带宽大，但内存容量有限。所以拥有多种芯片在某些方面很有优势，这意味着你可以根据任务特点选择最合适的芯片来执行。

Ankit Gupta：比如，有些任务更适合在TPU集群上执行，而另一些更适合在NBHP集群上？你们会怎么选择？你可以谈谈这个。

Nick Joseph：有趣，我举个例子，推理任务通常需要更多的HBM带宽，因为你在一次时间步中需要加载每个token的所有权重，这意味着需要很高的HBM带宽。而预训练通常更依赖于浮点运算，因为批量更大。所以你可以针对不同用途选择不同芯片。但拥有多种芯片的缺点是，你可能需要为每种芯片重复编写代码。理论上可以通过抽象层来统一，但实际差异太大，很难做到。因此如果你处理所有工作负载和所有芯片，工作量会按芯片数量成倍增加。

Ankit Gupta：关于你提到计算机有时会出故障，我记得你之前做过类似事情。我当时公司在使用Google TPU时遇到一些奇怪的segfault错误，你告诉我一个有效的方法，如果六个月前用上它们，就能解决一半的问题。我可以想象，你们在使用这些非常新的芯片时，会遇到很多问题，需要和提供商紧密合作来解决。

Nick Joseph：是的，项目组在解决问题上很积极。有趣的是如何找到合适的合作方式。他们有强烈动机去修复问题，因为希望芯片能正常工作，从而未来能卖更多芯片。我们当然也有很强动机让芯片工作，因为我们提前大量采购这些芯片。所有工作都建立在让集群正常运行的基础上，但我们不一定能共享所有信息。有些信息不能完全共享。因此一种策略是制作小规模复现环境，当遇到问题时，你可以在单芯片或单文件上复现，然后发给对方进行调试。

Ankit Gupta：你们是在共享的Slack上交流，互相发送问题和数据吗？还是说你们和大厂的人员实际上在同一个办公室？

Nick Joseph：主要是共享Slack，有时候见面更有效，但Slack是常用方式。

合成数据风险、模型评估指标设计与价值嵌入博弈
Ankit Gupta：那我们谈谈近期预训练的现状。这几年，各家公司对预训练的关注似乎有所分散，除了预训练，还有后训练的强化学习、微调和安全性调整等。从外部看，预训练似乎相对不那么受关注，而推理类模型的进展主要依赖后训练。你怎么看？这种理解是否合理？在推理和新型后训练方法的时代，有没有预训练层面仍然重要的因素，对实现优秀模型有影响？

Nick Joseph：以前预训练意味着做一个大规模训练，但其实已经有变化，比如直接进行大量自由训练，把大部分计算资源用于训练。现在人们发现强化学习也能带来很大收益，你可以把更多算力投入强化学习，从而得到更好的模型。因此如何平衡预训练和后训练、各自作用叠加还是互补，这些问题仍在早期阶段，还没有定论。

Ankit Gupta：你认为这些更多是经验性问题吗？像我们之前讨论的，你会尝试多种方法看效果，还是有一些基于原理的方式来判断？

Nick Joseph：最终还是经验为主。可以提出理论，但实践中大多数理论都需要验证，多半是错的。所以最可靠的办法是收集数据再做判断。解决问题的经验方法对于做出正确决策至关重要，但在组织内很难实现。关键是不要因为你管理预训练，就坚持预训练必须胜出。

Ankit Gupta：团队间是否存在某种竞争？还是视为同一个整体？

Nick Joseph：我们这边合作性很强，基本上是共同产出一个模型。但据我了解，有些公司团队间存在摩擦，这是一个有趣的组织设计问题：如何设置团队，避免科学问题被个人团队观念绑架。

Ankit Gupta：关于预训练，你如何看待高质量数据的可用性？你们已经训练了几乎所有互联网文本。外界常说数据已经枯竭，是否如此？尤其当大量数据由AI生成时，是否存在模式崩塌风险，模型会过拟合AI生成数据？

Nick Joseph：我对数据问题看到很多自信的说法。有人说互联网数据已经用尽，我不确定实际情况。数据的质量与数量总是有权衡。基本事实是数据量巨大，增长速度慢于算力增长。

Ankit Gupta：也就是说，虽然新数据在增加，但算力也在增加，不容易判断哪个增长更快。

Nick Joseph：我需要强调，我并不完全确定。一方面互联网似乎无限，你可以不断生成文本。但“有用的互联网”规模无人知晓。

Ankit Gupta：我脑子里简单想法是，用PageRank过滤出有价值的网页，不是可行吗？

Nick Joseph：我认为不完全可行。人眼认为有价值的内容与模型学习所需的有用信息不完全相同。有些内容虽然链接少，但对模型可能很重要，尤其是处理难题的尾部知识。

Ankit Gupta：这是原始Google的链接算法。

Nick Joseph：对，它是质量指标，但不一定是最优指标。

Ankit Gupta：AI的任务可能不同。

Nick Joseph：尾部数据可能更有价值，可以通过蒸馏或智能模型生成数据训练新模型，接近原始模型的智能水平。

Ankit Gupta：开放源代码模型中小模型蒸馏大模型的例子很多。

Nick Joseph：完全可行。问题是，如果用现有模型生成数据训练更好模型，可能无法超越原模型，因为你只是学到了原模型的分布。如果分布有误，新模型也不会学到真实知识。

Ankit Gupta：这是因为下一个token预测的损失对原模型生成内容很低，对吧？

Nick Joseph：没错。模型只会学到原分布，如果原分布错误，就学不到真理。例如模型认为5+5=11，新模型也会学到11。这是一个研究难点，因为小规模研究难以直接扩展到大规模训练。还有一层问题是互联网上大量内容由LLM生成，其影响不易量化。1%的LLM内容，会浪费1%的算力，还是破坏5%-10%的模型表现？难以判断。

Ankit Gupta：这不一定坏吧？如果训练目标是从当前分布向真实分布靠近，互联网上流通的数据本身可能有助于校正。

Nick Joseph：你说的是通过互联网自然过滤，可能有效，但垃圾内容和恶意内容仍存在，这会影响模型。

Ankit Gupta：你之前提到评估指标，除了模型本身，还有数据质量等指标。有没有可以量化的数据和模型质量指标，除了损失函数？

Nick Joseph：损失函数其实很有效。理想的评估应关注实际关心的目标，而不是代理指标；评估需低噪声、快速易用。三点标准缺一不可，但第一点最难：明确你关心的到底是什么。

Ankit Gupta：即使微小差异在评估中也要能体现出来，以便优化方向。

Nick Joseph：对，比如GPT-4的MLA分数86.4%，下一代Gemini90%，差异明显，可用于判断优劣。评估需快速执行且易于操作。

Ankit Gupta：例如AI医生任务，考试题可能轻松通过，但实际长时间与患者交流、提取关键信息更难，这类评估难度高。

Nick Joseph：确实。创业公司可以做这些评估，而大实验室往往只优化标准化指标。医生场景中，我认为可用真实医生与患者的对话记录训练，预测对话文本，低噪声，模型可用于辅助诊疗。

Ankit Gupta：这可以作为创业项目。外部讨论排列时，你能定义排列吗？它在预训练中如何体现？

Nick Joseph：我们目标是AGI，即能完成大部分人类能做的事情。Sci-fi常被误导，人类一般想象一个机器人，而实际上应是大量智能体复制。关键问题是AI的目标是什么，目前下一token预测是目标，但人类目标不同。排列就是让模型目标与人类一致。

Ankit Gupta：这与人类目标不同。

Nick Joseph：是的。排列可从理论或经验角度解决，现有模型往往不符合期望。另一层是实际控制模型行为，比如通过宪法式规则或系统提示来约束模型交互方式。

Ankit Gupta：系统提示类似提词，而非训练时调整。

Nick Joseph：可以训练时加入，也可通过系统提示，取决于所需鲁棒性。

Ankit Gupta：如何选择模型体现哪些价值？

Nick Joseph：这是个难题。比喻为装方向盘，先获取控制权，再考虑驾驶者是谁。最好有民主化的价值设定，而非单人价值，以避免走向极权。

Ankit Gupta：现阶段，你们如何实现排列？是通过后训练调整模型人格吗？

Nick Joseph：大体正确。后训练迭代快，效果反馈快，适合调整模型。预训练用于基础科学探索，小模型无法有效模拟复杂行为。

Ankit Gupta：必须在足够智能的模型上进行。

Nick Joseph：对，但某些排列可以融入预训练，增强鲁棒性和智能性，例如让模型在学习智能的过程中融入人类反馈。

Ankit Gupta：这如何在预训练中体现？

Nick Joseph：可参考论文《Pretraining on Human Feedback》，将人类反馈特性融入预训练，观察效果。但缺点是灵活性下降，无法在后训练中快速调整。

Ankit Gupta：你提到迭代速度关键，三个月与一天的差距巨大，可在短时间内尝试多种后训练策略并行执行。

Nick Joseph：完全正确，自由训练本质困难，因为一次训练周期长且不可中断。

---

## 五、AGI之路：范式转移焦虑、架构变革与未来展望

### 5.1 未来最大的挑战：范式转变与隐藏的Bug

**Ankit Gupta**：所以我现在在想，未来几年你们打算做的事情，作为团队，你们是如何考虑的呢？比如，你们会遇到哪些已知的问题，必须去应对？

**Nick Joseph**：我觉得我最关注的可能是**范式的转变**。我认为向强化学习的转变就是一个领域内的范式变化。我认为未来可能还会出现更多变化。很多人会争论，比如说，现有的范式是否足够让我们达到通用智能。我不知道，也许够，但我几乎可以肯定还会有新的范式。

如果结果只是简单地扩大规模，没有任何意外，那会非常令人惊讶。但我真正感到最紧张的，其实是一些**非常难以解决的bug**。

Ankit Gupta：这很有意思。

Nick Joseph：是的，可能有点出乎我意料，但一个bug就可能让你搁置几个月。因为模型训练需要几个月的时间，一个小小的代码错误可能导致整代模型失效，而且难以被发现。机器学习本身就很难找到bug，但在大规模系统中，这类问题更难解决。

Ankit Gupta：是啊，有时候甚至不知道从哪里下手。比如你写一个网络架构，漏掉了单元测试，或者没办法写测试去覆盖这种架构，你该怎么验证呢？

Nick Joseph：你可以发送一个数据包，确认它正常传输，或者用小模型训练试一下，但即使是小模型，也不明显。

Ankit Gupta：如果早期做ML的人遇到过经典bug，比如网络有10层，层7连到了层9，而不是8到9，模型依然可以训练，权重也在更新，但架构实际上是错的。这类bug非常隐蔽，很难发现。你说的这些随机bug，是这个意思吗？

Nick Joseph：对，就是这种情况。随着系统复杂度上升，你可能在某个底层内核里用了错误的精度，这会让大规模模型崩掉。

Ankit Gupta：训练一个月后才发现，甚至可能永远都发现不了。

Nick Joseph：对，有时候甚至永远不会发现。代码量成万行，你怎么排查？所以最让我担心的就是这种微妙而棘手的bug。有时候你会看到模型崩溃或者训练速度突然下降，这类问题也非常难调试。我记得Nelson Nelha曾遇到过一个“诅咒”的bug，我遇到它时就觉得很棘手，于是把它交给别人，一个月后才松了一口气，自己可能永远解决不了。

我觉得一个非常有价值的能力是可以深入到任何层面去分析问题，但这是一项罕见技能。我在Torch底层工作过，如果不了解CUDA，Torch底层出问题，我就无法自己解决。通讯方面也是一样，我可以发送数据，但如果底层网络协议有问题，我必须学整个领域才能理解数据包和TCP等细节。能从ML学习动态到字节级别全栈掌握的人非常少。

Ankit Gupta：完全理解。那你们团队成员的背景是怎样分布的？外界可能认为你们全是写论文的博士研究员，但我觉得事实可能并非如此。

Nick Joseph：是的，团队混合多种背景。我们最需要的几乎一直都是工程师。在这个领域的历史中，往往只要算力足够，模型就能工作，真正的挑战是如何正确实现。

Ankit Gupta：行动力是关键，对吧？

Nick Joseph：没错。实现正确并非ML问题，架构其实很简单，你甚至不必完全理解数学，只要实现正确即可。然后你面临的工程问题是如何在大规模上并行化、验证正确性。这种工程技能是关键，但与快速迭代网站等技能不同，更偏向于解决极难工程问题。

Ankit Gupta：你们找的工程师是有类似Anthropic经验的人，还是学术出身的人？

Nick Joseph：目前我们倾向于直接聘请有相关经验的人。早期我们会从各种背景挑选，但现在领域足够大，有经验的人可以直接上手。也有一些非常聪明、勤奋的人可以快速学习，比如一些理论物理学家，他们通过实习掌握编程，也能做出出色工作。

Ankit Gupta：我想换个话题，谈谈未来你对AI领域其他方向的看法。你怎么看非下一步预测的方向？比如不是用Transformer架构，或者非自回归训练，有没有值得关注的？

Nick Joseph：我觉得这些很有趣，但我不认为自回归是唯一路径。不过，自回归可能足够达到EGI。主要驱动还是规模和对基础科学的精细研究，而不是完全新颖的架构。确实存在更优新颖方法，但规模更容易、可靠性更高，而且仍有很大提升空间。

Ankit Gupta：你花很多时间研究开源论文，看到一些中国实验室对架构做优化，比如缓存或高效注意力机制。你觉得这些优化是“加算力就行”的小改进，还是可能像Transformer一样带来革命性变化，必要时才能实现AGI？

Nick Joseph：我认为是两者结合。我的猜测是，你会不断调整，随着算力增加，实验价值也会提升。同时还有推理优化，比如服务大量用户时做出更低成本、高效率的推理，这涉及推理堆栈和芯片细节。

Ankit Gupta：预训练团队是否需要考虑推理问题，还是只管降低loss，然后交给别人处理？

Nick Joseph：不，我们非常关注推理问题。我们提供的模型必须快速运行，否则用户体验会受影响。

Ankit Gupta：能举例说明吗？

Nick Joseph：最简单的例子是把模型做得过大，推理会变慢，或者增加不必要的通信步骤，使推理复杂化。这并非技术难题，而是架构设计问题。

Ankit Gupta：明白。

Nick Joseph：推理团队是我最常合作的团队，因为我们共同设计模型，使其既聪明又经济高效。有限算力下，优化推理是服务更多用户的关键。

Ankit Gupta：如果算力无限呢？

Nick Joseph：即便算力无限，挑战也在于如何利用它。大规模系统会遇到芯片故障等工程问题。无限算力能大幅提升速度和实验频率，但仍需解决复杂工程问题。现在的AI研究非常受制于算力，算力限制了模型训练和迭代速度。

Ankit Gupta：你怎么看Diffusion方法，比如Gemini diffusion模型？在蛋白质设计等领域应用广泛，你觉得有发展潜力吗？

Nick Joseph：坦白说，我们没做过图像生成，主要是扩散模型用途。我自己理解不够深入，但团队中有人可以更好回答。我觉得这类方法属于“不是范式性转变，但可能带来运行效率提升”的类别。

Ankit Gupta：那么在近期，如果Anthropic每年持续改进模型，你认为创业公司有哪些机会？

Nick Joseph：任何能受益于模型智能提升的方向都有机会。虽然大公司算力和资源更强，但他们做的是通用系统，我们的目标是让创业公司用这些模型完成具体工作。关键是利用当前模型基础上稍加努力就能落地的应用，但要注意不要过度依赖搭建复杂脚手架，因为下一代模型可能不需要。

Ankit Gupta：在训练堆栈中，有没有问题是如果有公司解决，我们会直接购买产品的？

Nick Joseph：有很多。比如芯片计算错误，如果有初创公司能检测每块芯片的精确性并报告故障，非常有用。还有组织管理方面，如果有服务能帮快速扩展团队，管理人员招聘和组织问题，也很有价值。未来更值得关注的是AGI普及后的社会影响，如何让其积极造福世界，而不仅仅是经济增长。

Ankit Gupta：非常认同。最后一个问题，如果回到10年前，你还是学生，正从经济学转向AI，你会给学生什么建议，尤其是想进入你现在这种岗位的学生？

Nick Joseph：时间点不同，策略也不同。如果回到10年前，我会专注AI，特别是工程技能，而不仅仅是数学或理论ML文献。同时会关注AGI相关应用，这是我认为最重要的两件事。

---

## 结语

Nick Joseph的访谈为我们提供了一个难得的窗口，深入了解Anthropic预训练团队的核心思考与实践。从AI安全初心到技术路线选择，从万卡集群的工程挑战到对齐问题的深层思考，这次对话展现了一个预训练团队负责人的完整视角。

**核心要点总结：**

1. **算力是王道**：预训练的核心就是推动损失函数下降，扩展定律已被证明在11个数量级上有效
2. **工程能力关键**：正确实现比理论创新更重要，全栈能力是稀缺资源
3. **预训练与后训练的平衡**：如何平衡两者仍在早期探索阶段，强化学习带来了新的范式转变
4. **推理与训练并重**：预训练团队必须考虑推理问题，模型要既聪明又经济高效
5. **未来挑战**：范式转变和隐藏的Bug是最大威胁，AGI的社会影响值得关注

正如Nick所说，现在AI研究最大的瓶颈是计算资源受限，而非算法突破。在这个领域，工程实践的价值远远超过纸面上的理论创新。对于想要进入这个领域的年轻人来说，培养扎实的工程技能，保持对AGI应用的关注，将是最重要的两件事。

---

**原视频：** [Anthropic Head of Pretraining on Scaling Laws, Compute, and the Future of AI](https://www.youtube.com/watch?v=YFeb3yAxtjE)

**编译：** Zhenning Du | **来源：** Z Potentials
