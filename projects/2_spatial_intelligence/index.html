<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 李飞飞:空间智能是AI的下一个前沿 | Chao Yang </title> <meta name="author" content="Chao Yang"> <meta name="description" content="从文字到世界,World Labs开启空间智能新时代"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emigmo.github.io/projects/2_spatial_intelligence/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chao</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/chinese/">中文简介 </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">李飞飞:空间智能是AI的下一个前沿</h1> <p class="post-description">从文字到世界,World Labs开启空间智能新时代</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/feifei-li-stanford-480.webp 480w,/assets/img/feifei-li-stanford-800.webp 800w,/assets/img/feifei-li-stanford-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/feifei-li-stanford.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="李飞飞教授" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 李飞飞教授 - 斯坦福大学教授、ImageNet创始人、World Labs联合创始人 </div> <h2 id="前言">前言</h2> <blockquote> <p><strong>“空间智能将改变我们创造和交互现实与虚拟世界的方式——彻底革新叙事、创造力、机器人学、科学发现,以及更多领域。这,正是AI的下一个前沿。”</strong></p> </blockquote> <p>1950年,当计算机还只是自动化算术和简单逻辑时,艾伦·图灵提出了一个至今仍回荡的问题:<strong>机器能思考吗?</strong></p> <p>今天,以大语言模型(LLM)为代表的前沿AI技术,已经开始改变人类获取与处理抽象知识的方式。然而,它们仍然是”黑暗中的文字匠”:能言善辩,却无经验;知识丰富,却缺乏根基。</p> <div class="row"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial-ai-concept-480.webp 480w,/assets/img/spatial-ai-concept-800.webp 800w,/assets/img/spatial-ai-concept-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/spatial-ai-concept.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="空间智能概念图" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-labs-logo-480.webp 480w,/assets/img/world-labs-logo-800.webp 800w,/assets/img/world-labs-logo-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/world-labs-logo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="World Labs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 左:空间智能让AI理解三维世界 | 右:World Labs致力于构建具备空间智能的AI系统 </div> <hr> <h2 id="一引言从图灵之问到空间智能">一、引言:从图灵之问到空间智能</h2> <h3 id="图灵的愿景与ai的征程">图灵的愿景与AI的征程</h3> <p>1950年,艾伦·图灵的问题催生了一场持续至今的科学征程——人工智能(AI)。在我投身AI研究的25年中,图灵的愿景依然不断启发着我。</p> <h3 id="当前ai的成就与局限">当前AI的成就与局限</h3> <p>今天,以大语言模型(LLM)为代表的前沿AI技术展示了曾被认为不可能的能力:生成连贯的文本、成山的代码、逼真的图像,甚至短视频。</p> <p><strong>然而,它们仍然是”黑暗中的文字匠”</strong>:</p> <ul> <li>能言善辩,却无经验</li> <li>知识丰富,却缺乏根基</li> <li>无法真正理解物理世界</li> </ul> <h3 id="为什么空间智能是下一个前沿">为什么空间智能是下一个前沿</h3> <p>空间智能(spatial intelligence)将改变我们创造和交互现实与虚拟世界的方式——彻底革新叙事、创造力、机器人学、科学发现,以及更多领域。</p> <p>自我进入这一领域以来,对视觉与空间智能的探索就一直是我的”北极星”:</p> <ul> <li>构建了 <strong>ImageNet</strong>——首个大规模视觉学习与评测数据集</li> <li>在斯坦福实验室将计算机视觉与机器人学习相结合</li> <li>与联合创始人创建 <strong>World Labs</strong>,第一次真正实现这一可能性</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/imagenet-480.webp 480w,/assets/img/imagenet-800.webp 800w,/assets/img/imagenet-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/imagenet.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ImageNet数据集" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ai-evolution-480.webp 480w,/assets/img/ai-evolution-800.webp 800w,/assets/img/ai-evolution-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ai-evolution.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="AI发展历程" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model-480.webp 480w,/assets/img/world-model-800.webp 800w,/assets/img/world-model-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/world-model.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="世界模型" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ImageNet奠定了现代AI基础 | AI从语言走向空间理解 | 世界模型是下一代AI架构 </div> <hr> <h2 id="二空间智能人类认知的脚手架">二、空间智能:人类认知的脚手架</h2> <h3 id="进化的基石感知-行动循环">进化的基石:感知-行动循环</h3> <p>视觉长期以来是人类智能的基石,但它的力量源自更为根本的东西。早在动物能筑巢、抚育后代、用语言交流或建立文明之前,那看似简单的”感知行为”就已经悄然点燃了通向智能的进化旅程。</p> <p><strong>从感知到智能的桥梁:</strong></p> <p>这种从外部世界汲取信息的能力,在感知与生存之间搭建起一座桥梁。神经元层层叠加,形成能解释世界、协调生物与环境互动的神经系统。因此,许多科学家认为,<strong>“感知—行动”循环成为了智能进化的核心机制</strong>。</p> <h3 id="日常生活中的空间智能">日常生活中的空间智能</h3> <p>空间智能在我们与物理世界的互动中扮演着基础性的角色。每天,我们都在依赖它完成最平常的动作:</p> <p><strong>日常场景示例:</strong></p> <ul> <li>停车时想象车尾与路缘的距离</li> <li>接住被抛来的钥匙</li> <li>在人群中穿行而不碰撞</li> <li>半睡半醒间准确地将咖啡倒进杯中</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial-perception-480.webp 480w,/assets/img/spatial-perception-800.webp 800w,/assets/img/spatial-perception-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/spatial-perception.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="人类空间感知" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 人类依赖空间智能完成日常任务 </div> <h3 id="人类创造力的根基">人类创造力的根基</h3> <p>空间智能同样是我们想象力与创造力的基石。讲故事的人在脑中构建出丰富的世界,并用各种视觉媒介将之传达给他人:</p> <ul> <li>原始洞穴壁画</li> <li>现代电影</li> <li>沉浸式电子游戏</li> </ul> <p>无论是孩子在沙滩上筑城堡,还是在电脑上玩《我的世界》,这种以空间为根基的想象构成了人与虚拟世界交互体验的基础。</p> <h3 id="推动文明进步的力量">推动文明进步的力量</h3> <p>历史上那些塑造文明的关键时刻中,空间智能往往扮演着核心角色。</p> <p><strong>埃拉托色尼测量地球周长(古希腊):</strong></p> <ul> <li>通过对阴影的几何化思考</li> <li>在亚历山大测得太阳影子形成的7度角</li> <li>与赛恩(Syene)”正午无影”的现象进行对比</li> <li>从而计算出了地球的周长</li> </ul> <p><strong>沃森与克里克揭示DNA结构:</strong></p> <ul> <li>依赖于他们亲手搭建的三维分子模型</li> <li>用金属板与铁丝不断调整、拼接</li> <li>直到碱基对的空间排布完美契合</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/earth-measurement-480.webp 480w,/assets/img/earth-measurement-800.webp 800w,/assets/img/earth-measurement-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/earth-measurement.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="埃拉托色尼测量地球" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dna-structure-480.webp 480w,/assets/img/dna-structure-800.webp 800w,/assets/img/dna-structure-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dna-structure.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DNA双螺旋结构" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 空间智能推动了科学发现:从测量地球到揭示DNA结构 </div> <h3 id="当前ai的空间智能缺陷">当前AI的空间智能缺陷</h3> <p>虽然过去几年确实取得了巨大进步,多模态大语言模型(MLLMs)初步具备了空间感知能力,但坦率地说,<strong>AI的空间能力依然远未接近人类水平</strong>。</p> <p><strong>基本空间理解的失败:</strong></p> <ul> <li>最先进的MLLM在估计距离、方向、大小等任务上表现往往不比随机猜测好多少</li> <li>无法”心智旋转”物体——即从新角度再现同一对象的形状</li> <li>不会在迷宫中导航、识别捷径,或预测基本的物理规律</li> </ul> <p><strong>缺乏空间智能的后果:</strong></p> <p>缺乏它,AI将:</p> <ul> <li>无法真正安全地驾驶汽车</li> <li>无法在家庭与医院中引导机器人</li> <li>无法创造全新的沉浸式学习与娱乐体验</li> <li>也无法加速材料科学与医学的发现</li> </ul> <p>哲学家维特根斯坦曾写道:<strong>“语言的边界就是我世界的边界”</strong>。我不是哲学家,但我知道,<strong>对AI而言,世界不止于语言</strong>。空间智能代表着超越语言的前沿。</p> <hr> <h2 id="三构建世界模型ai的下一个十年">三、构建世界模型:AI的下一个十年</h2> <h3 id="什么是世界模型">什么是世界模型</h3> <p>那么,我们该如何打造拥有空间智能的AI?</p> <p><strong>答案:世界模型(World Models)</strong></p> <p>要实现这样的AI,我们需要比LLM更具雄心的体系:世界模型。这是一种全新的生成式模型,其在理解、推理、生成与交互方面的能力,将超越当今LLM所能触及的极限。它能够在语义、物理、几何与动态层面上,理解并生成复杂的虚拟或真实世界。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llm-vs-world-model-480.webp 480w,/assets/img/llm-vs-world-model-800.webp 800w,/assets/img/llm-vs-world-model-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/llm-vs-world-model.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="LLM vs 世界模型" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 从大语言模型到世界模型:AI理解世界的范式转变 </div> <h3 id="世界模型的三大核心能力">世界模型的三大核心能力</h3> <p>在这个新兴领域中,最重要的是确立指导发展方向的核心原则。对于空间智能而言,我将”世界模型”定义为具备以下三项核心能力的系统:</p> <h4 id="能力1生成性generative">能力1:生成性(Generative)</h4> <p><strong>世界模型能够生成具有感知、几何与物理一致性的世界</strong></p> <p>要实现空间理解与推理,世界模型必须能够生成自身的模拟世界。核心要求:</p> <ul> <li>能在语义或感知指令的引导下,生成无限多样、变化丰富的虚拟世界</li> <li>同时保持几何、物理与动态上的一致性</li> <li>无论这些世界是现实的还是虚拟的</li> </ul> <p><strong>时间连贯性:</strong></p> <p>尤其重要的是,模型对当下世界的理解必须与其过去的状态保持连贯一致——<strong>理解当前,就是理解它是如何演化而来的。</strong></p> <h4 id="能力2多模态multimodal">能力2:多模态(Multimodal)</h4> <p><strong>世界模型在设计上就是多模态的</strong></p> <p>正如人类与动物一样,世界模型应能处理多种形式的输入。面对不完整的信息——无论是:</p> <ul> <li>图像</li> <li>视频</li> <li>深度图</li> <li>文本指令</li> <li>手势</li> <li>还是动作</li> </ul> <p>世界模型都应能预测或生成尽可能完整的世界状态。</p> <h4 id="能力3交互性interactive">能力3:交互性(Interactive)</h4> <p><strong>世界模型能根据输入动作输出下一个状态</strong></p> <p>当动作(actions)和/或目标(goals)作为输入提示的一部分时,世界模型的输出必须包含世界的下一个状态。随着空间智能世界模型在推理与生成能力上不断增强,未来模型不仅能预测世界的下一个状态,还将能够基于该状态预测下一步行动。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/generative-capability-480.webp 480w,/assets/img/generative-capability-800.webp 800w,/assets/img/generative-capability-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/generative-capability.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="生成性" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/multimodal-capability-480.webp 480w,/assets/img/multimodal-capability-800.webp 800w,/assets/img/multimodal-capability-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/multimodal-capability.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="多模态" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/interactive-capability-480.webp 480w,/assets/img/interactive-capability-800.webp 800w,/assets/img/interactive-capability-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/interactive-capability.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="交互性" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 世界模型的三大核心能力:生成性、多模态、交互性 </div> <h3 id="技术挑战与研究方向">技术挑战与研究方向</h3> <p>这一挑战的规模,超越了AI以往所面临的一切。语言是人类认知中纯粹生成的现象,而”世界”遵循的规则则复杂得多。</p> <p>在World Labs,我们的研究团队正致力于这一目标的基础性突破。以下是我们当前研究的几个方向:</p> <p><strong>方向1:新的通用训练任务函数</strong></p> <p>在世界模型研究中,一个长期目标是定义一种像LLM中”下一个token预测”一样简洁优雅的通用任务函数。这一目标函数及其对应表征必须符合几何与物理规律。</p> <p><strong>方向2:大规模训练数据</strong></p> <p>训练世界模型所需的数据远比文本复杂。关键在于构建能够在相似规模上有效利用视觉数据的架构,并结合高质量的合成数据和额外模态(如深度、触觉)。</p> <p><strong>方向3:新的模型架构与表征学习</strong></p> <p>世界模型研究将不可避免地推动模型架构与学习算法的革新。在World Labs,我们最近开发了一种基于帧的实时生成模型——<strong>RTFM(Real-Time Generative Frame-based Model)</strong>,以空间为基础的帧作为空间记忆形式。</p> <h3 id="world-labs的研究进展">World Labs的研究进展</h3> <h4 id="marble首个世界模型产品">Marble:首个世界模型产品</h4> <p>我们最近向部分用户展示了 <strong>Marble</strong> 的早期版本:</p> <p><strong>Marble的特点:</strong></p> <ul> <li>全球首个可通过多模态输入生成并保持一致性3D环境的世界模型</li> <li>让用户与创作者能够探索、交互并在其中继续构建他们的创意世界</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/marble-demo-480.webp 480w,/assets/img/marble-demo-800.webp 800w,/assets/img/marble-demo-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/marble-demo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Marble演示" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Marble - World Labs的首个世界模型产品,能够生成一致性3D环境 </div> <hr> <h2 id="四用世界模型构建更美好的世界">四、用世界模型构建更美好的世界</h2> <h3 id="人工智能的发展动机">人工智能的发展动机</h3> <p>作为推动现代AI时代到来的科学家之一,我的动机始终十分明确:<strong>AI应当增强人类的能力,而非取而代之。</strong></p> <p><strong>务实的立场:</strong></p> <p>当下关于”技术乌托邦”与”世界末日”的极端叙事比比皆是,但我依然持一种更务实的立场:</p> <ul> <li>AI是由人开发、被人使用、并由人治理的</li> <li>它必须始终尊重人的自主性与尊严</li> <li>它的”魔力”在于拓展我们的能力,让我们变得更具创造力、更紧密相连、更高效并更有成就感</li> </ul> <blockquote> <p><strong>这一信念,正是我将空间智能视为AI下一个伟大前沿领域的根本原因。</strong></p> </blockquote> <h3 id="创造力为叙事注入超能力">创造力:为叙事注入超能力</h3> <blockquote> <p><strong>“创意,是智慧的乐趣。”</strong> ——爱因斯坦</p> </blockquote> <h4 id="故事的力量">故事的力量</h4> <p>在人类发明文字之前,我们就会讲故事。今天,空间智能有潜力彻底变革我们创作与体验叙事的方式,从娱乐到教育,从设计到建造。</p> <p><strong>Marble平台的能力:</strong></p> <p>World Labs的Marble平台将前所未有的空间表达能力与编辑控制权交到创作者手中:</p> <ul> <li>电影人</li> <li>游戏设计师</li> <li>建筑师</li> <li>及各类讲述者</li> </ul> <div class="row"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/filmmaking-ai-480.webp 480w,/assets/img/filmmaking-ai-800.webp 800w,/assets/img/filmmaking-ai-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/filmmaking-ai.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="AI辅助电影制作" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/game-design-ai-480.webp 480w,/assets/img/game-design-ai-800.webp 800w,/assets/img/game-design-ai-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/game-design-ai.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="AI辅助游戏设计" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 空间智能赋能创意产业:从电影制作到游戏设计 </div> <h4 id="三大应用方向">三大应用方向</h4> <p><strong>1. 多维叙事体验</strong></p> <p>电影人和游戏设计师可以利用 Marble 构建整个世界,不受预算或地理限制。随着媒介与娱乐的界限模糊化,我们正接近一种全新的互动体验形态。</p> <p><strong>2. 以设计讲述空间故事</strong></p> <p>几乎所有被制造的物品或建造的空间,都必须在物理实现之前经过虚拟3D设计。空间智能能够:</p> <ul> <li>让建筑师在数分钟内可视化并漫游尚不存在的建筑</li> <li>让设计师即时将想象转化为形态</li> </ul> <p><strong>3. 全新的沉浸与互动体验</strong></p> <p>如今,空间智能结合VR、XR(扩展现实)头显与沉浸式显示设备,将体验提升到前所未有的高度。人们”走进”多维世界将如同打开一本书般自然。</p> <h3 id="机器人具身智能的实践">机器人:具身智能的实践</h3> <p>从昆虫到人类,动物都依赖空间智能来理解、导航并与世界交互。机器人也不会例外。</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/robotics-spatial-ai-480.webp 480w,/assets/img/robotics-spatial-ai-800.webp 800w,/assets/img/robotics-spatial-ai-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/robotics-spatial-ai.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="机器人与空间智能" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 空间智能是实现通用机器人的关键 </div> <h4 id="三大应用方向-1">三大应用方向</h4> <p><strong>1. 通过世界模型扩展机器人学习</strong></p> <p>机器人的学习进步取决于可扩展的训练数据方案。世界模型将在此发挥决定性作用。随着其感知精度与计算效率的提高:</p> <ul> <li>世界模型生成的输出将迅速缩小模拟与现实之间的差距</li> <li>让机器人能在数不清的状态、互动与环境中学习</li> </ul> <p><strong>2. 人机协作伙伴</strong></p> <p>机器人可以扩展劳动力并提升社会生产力。但要做到这一点,机器人必须具备空间智能:</p> <ul> <li>能感知、推理、规划、行动</li> <li>保持对人类目标与行为的同理一致</li> </ul> <p>应用示例:</p> <ul> <li> <strong>实验室机器人</strong>:替代科学家完成仪器操作</li> <li> <strong>家庭助理机器人</strong>:帮助老人做饭,而不剥夺他们的乐趣与自主性</li> </ul> <p><strong>3. 扩展的具身形态</strong></p> <p>将来自更加多样的设计:</p> <ul> <li>输送药物的纳米机器人</li> <li>穿行狭窄空间的软体机器人</li> <li>以及为深海或外太空而造的机器</li> </ul> <p>世界模型将为仿真数据、训练环境和评测任务提供支持。</p> <h3 id="更长远的地平线科学医疗与教育">更长远的地平线:科学、医疗与教育</h3> <p>除了创造性与机器人应用外,”空间智能”的深远影响还将延伸至更多能够增强人类能力、拯救生命、加速发现的领域。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/scientific-research-ai-480.webp 480w,/assets/img/scientific-research-ai-800.webp 800w,/assets/img/scientific-research-ai-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/scientific-research-ai.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="科学研究" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/healthcare-ai-480.webp 480w,/assets/img/healthcare-ai-800.webp 800w,/assets/img/healthcare-ai-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/healthcare-ai.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="医疗应用" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/education-ai-480.webp 480w,/assets/img/education-ai-800.webp 800w,/assets/img/education-ai-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/education-ai.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="教育创新" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 空间智能在科学、医疗和教育领域的应用前景 </div> <h4 id="1-科学研究模拟实验与假设验证">1. 科学研究:模拟实验与假设验证</h4> <p>具备空间智能的系统可以:</p> <ul> <li>模拟实验</li> <li>并行验证假设</li> <li>探索人类无法亲临的环境——从深海到遥远的行星</li> </ul> <p>这项技术有望彻底变革气候科学、材料研究等领域的计算建模方式。</p> <h4 id="2-医疗领域从实验室到病床">2. 医疗领域:从实验室到病床</h4> <p>在医疗领域,空间智能将重塑从实验室到病床的全过程:</p> <ul> <li> <strong>药物研发</strong>:通过多维建模加速药物研发</li> <li> <strong>诊断辅助</strong>:辅助放射科医生识别影像中的模式</li> <li> <strong>环境感知式监护</strong>:为患者与护理人员提供持续支持</li> </ul> <h4 id="3-教育领域沉浸式学习体验">3. 教育领域:沉浸式学习体验</h4> <p>空间智能能够实现沉浸式学习:</p> <ul> <li> <strong>学生</strong>:以多维方式探索细胞机器或”亲历”历史事件</li> <li> <strong>教师</strong>:借助互动环境进行个性化教学</li> <li> <strong>专业人士</strong>:在高度逼真的仿真环境中安全地练习复杂技能</li> </ul> <h4 id="统一的目标">统一的目标</h4> <p>跨越这些领域,目标始终如一:</p> <blockquote> <p><strong>让AI成为增强人类专长、加速人类发现、放大人类关怀的力量——而不是取代那份属于人的判断力、创造力与共情力。</strong></p> </blockquote> <hr> <h2 id="五结语构建与世界契合的智能">五、结语:构建与世界契合的智能</h2> <h3 id="历史性的时刻">历史性的时刻</h3> <p>人类历史上第一次,我们正站在这样一个时刻:<strong>有望构建出与物理世界高度契合的机器,让它们成为我们应对重大挑战的真正伙伴。</strong></p> <p>无论是:</p> <ul> <li>加速疾病研究</li> <li>革新故事叙述方式</li> <li>还是在病痛、受伤或衰老的脆弱时刻给予支持</li> </ul> <p>我们都正处于一场技术变革的门槛上,它将提升我们最珍视的生命价值。</p> <h3 id="关于生活的愿景">关于生活的愿景</h3> <blockquote> <p><strong>这是一个关于更深刻、更丰富、更有力量的生活的愿景。</strong></p> </blockquote> <p>距自然在原始动物中首次显现空间智能的曙光已近五亿年,而我们有幸成为这一代技术创造者:</p> <ul> <li>可能即将赋予机器同样能力的人类</li> <li>也有幸能将此能力用于全人类的福祉</li> </ul> <h3 id="我的北极星">我的”北极星”</h3> <p>若没有空间智能,我们关于”真正智能机器”的梦想将永远不完整。</p> <blockquote> <p><strong>这场探索,是我的”北极星”。邀请你一同追寻它。</strong></p> </blockquote> <hr> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/future-spatial-ai-480.webp 480w,/assets/img/future-spatial-ai-800.webp 800w,/assets/img/future-spatial-ai-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/future-spatial-ai.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="空间智能的未来" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 空间智能:AI的下一个伟大前沿 </div> <hr> <p><strong>作者简介:</strong> 李飞飞(Fei-Fei Li)是斯坦福大学教授、ImageNet创始人、World Labs联合创始人。她是计算机视觉和人工智能领域的先驱,致力于构建具备空间智能的AI系统。</p> <p><strong>来源:</strong> World Labs 官方博客 <strong>发布时间:</strong> 2025年11月10日</p> <p><strong>相关链接:</strong></p> <ul> <li><a href="https://www.worldlabs.ai/" rel="external nofollow noopener" target="_blank">World Labs官网</a></li> <li><a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">李飞飞斯坦福主页</a></li> </ul> <hr> <p><strong>注:</strong> 本文图片需要替换为实际图片。建议使用的图片包括:</p> <ul> <li>李飞飞教授照片</li> <li>World Labs品牌图片</li> <li>空间智能概念图</li> <li>机器人与AI应用场景</li> <li>科学、医疗、教育领域的AI应用图片</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Chao Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-\u4e2d\u6587\u7b80\u4ecb",title:"\u4e2d\u6587\u7b80\u4ecb",description:"",section:"Navigation",handler:()=>{window.location.href="/chinese/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-",title:"",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/2025-01-15-xiaohanding-paper-suggestion/"}},{id:"post-\u674e\u98de\u98de-\u4ece\u6587\u5b57\u5230\u4e16\u754c-\u7a7a\u95f4\u667a\u80fd\u662fai\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf",title:"\u674e\u98de\u98de:\u4ece\u6587\u5b57\u5230\u4e16\u754c,\u7a7a\u95f4\u667a\u80fd\u662fAI\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/drfeifei-spatial-intelligence/"}},{id:"post-anthropic-\u7814\u7a76\u5458\u8be6\u89e3-\u6784\u5efa\u9ad8\u6548-claude-\u667a\u80fd\u4f53\u7684\u5b8c\u6574\u65b9\u6cd5\u8bba",title:"Anthropic \u7814\u7a76\u5458\u8be6\u89e3\uff1a\u6784\u5efa\u9ad8\u6548 Claude \u667a\u80fd\u4f53\u7684\u5b8c\u6574\u65b9\u6cd5\u8bba",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/anthropic-claude-agent-methodology/"}},{id:"post-claude-code\u81ea\u5b9a\u4e49\u547d\u4ee4\u5728\u77e5\u8bc6\u7ba1\u7406\u4e0e\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u7cfb\u7edf\u5316\u5e94\u7528\u7814\u7a76",title:"Claude Code\u81ea\u5b9a\u4e49\u547d\u4ee4\u5728\u77e5\u8bc6\u7ba1\u7406\u4e0e\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u7cfb\u7edf\u5316\u5e94\u7528\u7814\u7a76",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/claude-code-knowledge-management/"}},{id:"post-18\u4e2a\u6539\u53d8\u4eba\u751f\u7684\u4e60\u60ef-\u79d1\u5b66\u8bc1\u636e\u652f\u6301\u7684\u957f\u671f\u4e3b\u4e49\u6307\u5357",title:"18\u4e2a\u6539\u53d8\u4eba\u751f\u7684\u4e60\u60ef\uff1a\u79d1\u5b66\u8bc1\u636e\u652f\u6301\u7684\u957f\u671f\u4e3b\u4e49\u6307\u5357",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/habit/"}},{id:"post-openai\u53cc\u5de8\u5934\u9996\u6b21\u8be6\u89e3gpt-5-\u4e0d\u662f\u4e0b\u4e00\u4ee3gpt-\u7ec8\u6781\u5f62\u6001\u662fai\u7814\u7a76\u5458",title:"OpenAI\u53cc\u5de8\u5934\u9996\u6b21\u8be6\u89e3GPT-5\uff1a\u4e0d\u662f\u4e0b\u4e00\u4ee3GPT\uff0c\u7ec8\u6781\u5f62\u6001\u662fAI\u7814\u7a76\u5458",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/openai-gpt5-researcher-vision/"}},{id:"post-kimi\u521b\u59cb\u4eba\u6768\u690d\u9e9f\u6df1\u5ea6\u8bbf\u8c08-\u6500\u767b\u65e0\u9650\u4e4b\u5c71",title:"KIMI\u521b\u59cb\u4eba\u6768\u690d\u9e9f\u6df1\u5ea6\u8bbf\u8c08\uff1a\u6500\u767b\u65e0\u9650\u4e4b\u5c71",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/kimi-yang-dialogue/"}},{id:"post-jason-wei-\u7406\u89e32025\u5e74ai\u8fdb\u5c55\u7684\u4e09\u79cd\u5173\u952e\u601d\u8def",title:"Jason Wei\uff1a\u7406\u89e32025\u5e74AI\u8fdb\u5c55\u7684\u4e09\u79cd\u5173\u952e\u601d\u8def",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/jason-wei-ai-insights/"}},{id:"post-nick-joseph\u8bbf\u8c08-anthropic\u9884\u8bad\u7ec3\u7684\u6838\u5fc3\u601d\u8003\u4e0e\u5b9e\u8df5",title:"Nick Joseph\u8bbf\u8c08\uff1aAnthropic\u9884\u8bad\u7ec3\u7684\u6838\u5fc3\u601d\u8003\u4e0e\u5b9e\u8df5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/anthropic-pretraining-nick-joseph/"}},{id:"post-\u5f90\u626c\u751f\u9662\u58eb-\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\u7684\u6559\u80b2",title:"\u5f90\u626c\u751f\u9662\u58eb\uff1a\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\u7684\u6559\u80b2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/xuyangsheng-CUHK-SZ/"}},{id:"post-andrej-karpathy\u6df1\u5ea6\u5bf9\u8bdd-agent\u7684\u5341\u5e74\u5f81\u7a0b\u4e0eai\u7684\u5e7d\u7075\u672c\u8d28",title:"Andrej Karpathy\u6df1\u5ea6\u5bf9\u8bdd\uff1aAgent\u7684\u5341\u5e74\u5f81\u7a0b\u4e0eAI\u7684\u5e7d\u7075\u672c\u8d28",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/karpathy-agent-ten-years/"}},{id:"post-sutton-\u5927\u8bed\u8a00\u6a21\u578b\u8d70\u9519\u4e86\u8def-\u4e0d\u7b26\u5408-\u82e6\u6da9\u6559\u8bad-\u7cbe\u795e",title:"Sutton\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u8d70\u9519\u4e86\u8def\uff0c\u4e0d\u7b26\u5408\u300c\u82e6\u6da9\u6559\u8bad\u300d\u7cbe\u795e",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/sutton-llm-is-wrong-way/"}},{id:"post-patrick-hsu-evo2\u534e\u4eba\u79d1\u5b66\u5bb6-\u865a\u62df\u7ec6\u80de\u8fc8\u5411gpt-2\u9636\u6bb5-\u5408\u6210\u751f\u7269\u5b66\u5c06\u6df1\u523b\u6539\u53d8\u4e16\u754c",title:"Patrick Hsu-Evo2\u534e\u4eba\u79d1\u5b66\u5bb6\uff1a\u865a\u62df\u7ec6\u80de\u8fc8\u5411GPT-2\u9636\u6bb5\uff0c\u5408\u6210\u751f\u7269\u5b66\u5c06\u6df1\u523b\u6539\u53d8\u4e16\u754c",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/patrick-hsu-evo2/"}},{id:"post-sam-altman-\u4e30\u6c9b\u667a\u80fd-\u6df1\u5ea6\u89e3\u6790-ai\u57fa\u7840\u8bbe\u65bd\u7684\u5b8f\u5927\u613f\u666f\u4e0e\u6218\u7565\u8def\u5f84",title:"Sam Altman\u300a\u4e30\u6c9b\u667a\u80fd\u300b\u6df1\u5ea6\u89e3\u6790:AI\u57fa\u7840\u8bbe\u65bd\u7684\u5b8f\u5927\u613f\u666f\u4e0e\u6218\u7565\u8def\u5f84",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/sam-altman-abundant-intelligence/"}},{id:"post-anthropic\u5185\u90e8ai\u4ee3\u7801\u9769\u547d-\u73b0\u5b9e\u8fd8\u662f\u7092\u4f5c-\u5f00\u53d1\u8005\u793e\u533a\u7684\u6df1\u5ea6\u8d28\u7591",title:"Anthropic\u5185\u90e8AI\u4ee3\u7801\u9769\u547d\uff1a\u73b0\u5b9e\u8fd8\u662f\u7092\u4f5c\uff1f\u5f00\u53d1\u8005\u793e\u533a\u7684\u6df1\u5ea6\u8d28\u7591",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/claude-code-sofaware/"}},{id:"post-\u59da\u987a\u96e8-ai\u4e0eagent\u7814\u7a76\u89c2\u70b9\u96c6",title:"\u59da\u987a\u96e8\uff1aAI\u4e0eAgent\u7814\u7a76\u89c2\u70b9\u96c6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yao-shunyu-agent-research/"}},{id:"post-\u59da\u987a\u96e8ai\u4e0eagent\u7814\u7a76\u89c2\u70b9\u96c6",title:"\u59da\u987a\u96e8AI\u4e0eAgent\u7814\u7a76\u89c2\u70b9\u96c6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yao-shunyu-ai-agent-insights/"}},{id:"post-\u6768\u690d\u9e9f\u89c2\u70b9\u7cbe\u534e-ai\u65f6\u4ee3\u7684\u6280\u672f\u54f2\u5b66\u4e0e\u5b9e\u8df5",title:"\u6768\u690d\u9e9f\u89c2\u70b9\u7cbe\u534e\uff1aAI\u65f6\u4ee3\u7684\u6280\u672f\u54f2\u5b66\u4e0e\u5b9e\u8df5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/yang-zhilin-ai-philosophy/"}},{id:"post-openai\u59da\u987a\u96e8\u6df1\u5ea6\u8bbf\u8c08-ai\u4e0b\u534a\u573a\u7684agent\u9769\u547d",title:"OpenAI\u59da\u987a\u96e8\u6df1\u5ea6\u8bbf\u8c08\uff1aAI\u4e0b\u534a\u573a\u7684Agent\u9769\u547d",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/openai-yao-dialogue/"}},{id:"post-deepmind\u79d1\u5b66\u8d1f\u8d23\u4eba\u6df1\u5ea6\u8bbf\u8c08-\u5982\u4f55\u7b5b\u9009\u5e76\u653b\u514b\u53d8\u9769\u6027\u6311\u6218",title:"DeepMind\u79d1\u5b66\u8d1f\u8d23\u4eba\u6df1\u5ea6\u8bbf\u8c08\uff1a\u5982\u4f55\u7b5b\u9009\u5e76\u653b\u514b\u53d8\u9769\u6027\u6311\u6218",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/deepmind-kohli-dialogue/"}},{id:"post-church-nature-ai\u9a71\u52a8\u86cb\u767d\u8d28\u8bbe\u8ba1-\u9769\u547d\u6027\u8303\u5f0f\u7684\u5168\u6d41\u7a0b\u89e3\u6790",title:"Church @ Nature\uff1aAI\u9a71\u52a8\u86cb\u767d\u8d28\u8bbe\u8ba1\uff0c\u9769\u547d\u6027\u8303\u5f0f\u7684\u5168\u6d41\u7a0b\u89e3\u6790",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/nature-ai-driven-protein-design/"}},{id:"news-aaai2024-one-offline-rl-paper-critic-guided-decision-transformer-for-offline-reinforcement-learning-accepted-by-aaai-2024-sparkles-sparkles",title:"[AAAI2024] One Offline RL Paper Critic-Guided Decision Transformer for Offline Reinforcement Learning accepted...",description:"",section:"News"},{id:"news-cvpr2024-two-papers-llama-excitor-videodistill-are-accepted-by-cvpr-2024-sparkles-sparkles",title:'[CVPR2024] Two Papers(LLaMA-Excitor, VideoDistill) are accepted by CVPR 2024. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"><img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-naacl2024-one-llm-safety-survey-paper-attacks-defenses-and-evaluations-for-llm-conversation-safety-a-survey-accepted-by-naacl-2024-sparkles-smile",title:"[NAACL2024] One LLM safety survey paper Attacks, Defenses and Evaluations for LLM Conversation...",description:"",section:"News"},{id:"news-ijcai2024-one-paper-attacks-defenses-and-evaluations-for-llm-conversation-safety-a-survey-accepted-by-ijcai-2024-survey-track-sparkles-sparkles",title:"[IJCAI2024] One Paper Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey...",description:"",section:"News"},{id:"news-icml2024-robocodex-multimodal-code-generation-for-robotic-behavior-synthesis-is-accepted-by-icml-2024-sparkles-sparkles",title:"[ICML2024] RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis is accepted by ICML...",description:"",section:"News"},{id:"news-acl2024-three-papers-emulated-disalignment-seer-structured-reasoning-multi-objective-dpo-are-accepted-by-acl-2024-sparkles-sparkles",title:"[ACL2024] Three Papers (Emulated Disalignment, SEER: Structured Reasoning, Multi-Objective DPO) are accepted by...",description:"",section:"News"},{id:"news-eccv2024-mm-safetybench-a-benchmark-for-safety-evaluation-of-multimodal-large-language-models-is-accepted-by-eccv-2024-sparkles-sparkles",title:"[ECCV2024] MM-SafetyBench (A Benchmark for Safety Evaluation of Multimodal Large Language Models) is...",description:"",section:"News"},{id:"news-emnlp2024-inference-time-language-model-alignment-via-integrated-value-guidance-is-accepted-by-emnlp-2024-arixv-link-sparkles-sparkles",title:"[EMNLP2024] Inference-Time Language Model Alignment via Integrated Value Guidance is accepted by EMNLP...",description:"",section:"News"},{id:"news-neurips2024-weak-to-strong-search-align-large-language-models-via-searching-over-small-language-models-is-accepted-by-neurips-2024-neurips-link-sparkles-sparkles",title:"[NeurIPS2024] Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models...",description:"",section:"News"},{id:"news-we-proposal-a-new-law-ai-45-law-toward-trustworthy-agi-arxiv-link-sparkles",title:'We proposal a new law, AI 45\xb0-Law toward trustworthy AGI! Arxiv Link <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">...',description:"",section:"News"},{id:"news-icml2025-emergent-response-planning-in-llm-and-c-3po-compact-plug-and-play-proxy-optimization-to-achieve-human-like-retrieval-augmented-generation-are-accepted-by-icml2025-sparkles",title:"[ICML2025] Emergent Response Planning in LLM and C-3PO: Compact Plug-and-Play Proxy Optimization to...",description:"",section:"News"},{id:"news-acl2025-our-paper-adversarial-preference-learning-for-robust-llm-alignment-is-accepted-by-acl2025-arxiv-link-sparkles",title:"[ACL2025] Our paper Adversarial Preference Learning for Robust LLM Alignment is accepted by...",description:"",section:"News"},{id:"news-neurips2025-we-find-patches-from-harmful-content-enabling-them-to-bypass-data-moderation-and-generate-dangerous-responses-when-encountering-the-full-image-or-related-text-vlms-can-aggregate-scattered-training-patches-is-accepted-by-neurips2025-sparkles",title:"[NeurIPS2025] We find patches from harmful content, enabling them to bypass data moderation...",description:"",section:"News"},{id:"news-big-project-release-we-introduce-safework-r1-a-cutting-edge-multimodal-reasoning-model-that-demonstrates-the-coevolution-of-capabilities-and-safety-safework-r1-rocket-sparkles",title:"\ud83c\udf89 Big Project Release! We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that...",description:"",section:"News"},{id:"news-aaai2026-shadow-dynamic-aware-credit-assignment-for-efficient-long-horizon-agent-training-is-accepted-by-aaai2026-sparkles",title:"[AAAI2026] SHADOW: Dynamic-Aware Credit Assignment for Efficient Long-Horizon Agent Training is accepted by...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-\u674e\u98de\u98de-\u7a7a\u95f4\u667a\u80fd\u662fai\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf",title:"\u674e\u98de\u98de:\u7a7a\u95f4\u667a\u80fd\u662fAI\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf",description:"\u4ece\u6587\u5b57\u5230\u4e16\u754c,World Labs\u5f00\u542f\u7a7a\u95f4\u667a\u80fd\u65b0\u65f6\u4ee3",section:"Projects",handler:()=>{window.location.href="/projects/2_spatial_intelligence/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%61%6E%67%63%68%61%6F [%41%54] %70%6A%6C%61%62 [%44%4F%54] %6F%72%67 [%44%4F%54] %63%6E","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=https://scholar.google.com/citations?hl=en&user=5KRbHPMAAAAJ&view_op=list_works&sortby=pubdate","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emigmo","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/00/5867-26.html","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@chaoyang4587","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>